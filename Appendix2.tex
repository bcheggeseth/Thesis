\chapter{Ascending property of the CEM algorithm}
\label{append:2}

\begin{reptheorem}{Thm:CEM}
The classification likelihood $L(\BS\theta,\BS\eta)$ defined in Section \ref{sec:multi} is non-decreasing after each update of $\BS\eta$ and $\BS\theta$ by the CEM algorithm. That is, $L(\BS\theta^{(t+1)},\BS\eta^{(t+1)}) \geq L(\BS\theta^{(t)},\BS\eta^{(t)})$ for all $t$.
\end{reptheorem}

\begin{proof}
We now prove the ascending property of the CEM algorithm used to estimate parameters in the multilayer mixture model, which is stated in Theorem 1 in Chapter \ref{chap:methods}. We need to show that
    \begin{align}\label{ap1}L(\BS\theta^{(t+1)},\BS\eta^{(t+1)})\geq L(\BS\theta^{(t)},\BS\eta^{(t)}).\end{align}
    The CEM algorithm alternatives between optimizing $\BS\eta$ and $\BS\theta$, keeping the other fixed. After the classification step, the parameter vector is updated from $(\BS\theta^{(t)},\BS\eta^{(t)})$ to $(\BS\theta^{(t)},\BS\eta^{(t+1)})$. Then the maximization step updates $(\BS\theta^{(t)},\BS\eta^{(t+1)})$ to $(\BS\theta^{(t+1)},\BS\eta^{(t+1)})$. To prove \ref{ap1}, it suffices to show that
    \begin{align*}
    L(\BS\theta^{(t)},\BS\eta^{(t+1)})&\geq L(\BS\theta^{(t)},\BS\eta^{(t)})\\
    L(\BS\theta^{(t+1)},\BS\eta^{(t+1)})&\geq L(\BS\theta^{(t)},\BS\eta^{(t+1)}).
    \end{align*}
    Note that 
    \begin{align*}
 L(\BS\theta^{(t)},\BS\eta)&=\sum^{n}_{i=1}\log\left[\pi_{\eta_{i}}(\B w_{i},\BS \gamma^{(t)})\sum_{j:c(j)=\eta_{i}}\pi_{j|c(j)}^{(t)}f(\B y_{i}|\lambda_{j}^{(t)}\B 1+\B x_{i}\BS\beta^{(t)}_{\eta_{i}},(\sigma^{(t)}_{j})^{2}\B I)\right]\\
 &\leq \sum^{n}_{i=1}\max_{\eta_{i}}\log\left[\pi_{\eta_{i}}(\B w_{i},\BS \gamma^{(t)})\sum_{j:c(j)=\eta_{i}}\pi_{j|c(j)}^{(t)}f(\B y_{i}|\lambda_{j}^{(t)}\B 1+\B x_{i}\BS\beta^{(t)}_{\eta_{i}},(\sigma^{(t)}_{j})^{2}\B I)\right]\\
  &= \sum^{n}_{i=1}\log\left[\pi_{\eta^{(t+1)}_{i}}(\B w_{i},\BS \gamma^{(t)})\sum_{j:c(j)=\eta^{(t+1)}_{i}}\pi_{j|c(j)}^{(t)}f(\B y_{i}|\lambda_{j}^{(t)}\B 1+\B x_{i}\BS\beta^{(t)}_{\eta^{(t+1)}_{i}},(\sigma^{(t)}_{j})^{2}\B I)\right]\\
  &= L(\BS\theta^{(t)},\BS\eta^{(t+1)})
    \end{align*}
for all $\BS\eta$. This is true because in the algorithm, $$\eta^{(t+1)}_{i} = \arg \max_{\eta_{i}} \pi_{\eta_{i}}(\B w_{i},\BS \gamma^{(t)})\sum_{j:c(j)=\eta_{i}}\pi_{j|c(j)}^{(t)}f(\B y_{i}\lambda_{j}^{(t)}\B 1+\B x_{i}\BS\beta^{(t)}_{\eta_{i}},(\sigma^{(t)}_{j})^{2}\B I).$$
   Thus, we have $    L(\BS\theta^{(t)},\BS\eta^{(t+1)})\geq L(\BS\theta^{(t)},\BS\eta^{(t)})$. To prove the other part of the inequality, note that
       \begin{align*}
 L(\BS\theta,\BS\eta^{(t+1)})  &= \sum^{n}_{i=1}\log\left[\pi_{\eta^{(t+1)}_{i}}(\B w_{i},\BS \gamma)\sum_{j:c(j)=\eta^{(t+1)}_{i}}\pi_{j|c(j)}f(\B y_{i}|\lambda_{j}\B 1+\B x_{i}\BS\beta_{\eta^{(t+1)}_{i}},\sigma_{j}^{2}\B I)\right]\\
 &= \sum^{K}_{k=1}\sum^{n}_{i=1}I(\eta^{(t+1)}_{i} = k)\log\left[\pi_{k}(\B w_{i},\BS \gamma)\sum_{j:c(j)=k}\pi_{j|c(j)}f(\B y_{i}|\lambda_{j}\B 1+\B x_{i}\BS\beta_{k},\sigma_{j}^{2}\B I)\right]\\
  &= \sum^{K}_{k=1}\sum^{n}_{i=1}I(\eta^{(t+1)}_{i} = k)\log\pi_{k}(\B w_{i},\BS \gamma)\\
  &\quad+\sum^{K}_{k=1}\sum^{n}_{i=1}I(\eta^{(t+1)}_{i} = k)\log \sum_{j:c(j)=k}\pi_{j|c(j)}f(\B y_{i}|\lambda_{j}\B 1+\B x_{i}\BS\beta_{k},\sigma_{j}^{2}\B I)\\
    &\leq\max_{\BS\gamma} \sum^{K}_{k=1}\sum^{n}_{i=1}I(\eta^{(t+1)}_{i} = k)\log\pi_{k}(\B w_{i},\BS \gamma)\\
  &\quad+\sum^{K}_{k=1}\max_{\pi_{j|c(j)},\BS\beta_{k},\lambda_{j},\sigma_{j}\text{ for }j:c(j)=k}\sum^{n}_{i=1}I(\eta^{(t+1)}_{i} = k)\log \sum_{j:c(j)=k}\pi_{j|c(j)}f(\B y_{i}|\lambda_{j}\B 1+\B x_{i}\BS\beta_{k},\sigma^{2}_{j}\B I)\\
  & = L(\BS\theta^{(t+1)},\BS\eta^{(t+1)})
    \end{align*}
    for all $\BS\theta$. This suggests that $L(\BS\theta,\BS\eta^{(t+1)})$ can be maximized by first using numerical optimization to find $\BS\gamma$ that maximizes the first term, and then the rest of the parameters can be optimized separately for the mixture model within each cluster using the samples assigned to the cluster by $\BS\eta^{(t+1)}$. The maximization step in the CEM computes $\BS\theta^{(t+1)}$ exactly in this fashion. Therefore, we have $L(\BS\theta^{(t)},\BS\eta^{(t+1)})\leq L(\BS\theta^{(t+1)},\BS\eta^{(t+1)})$. 
    \end{proof}