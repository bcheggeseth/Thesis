\chapter{Impact of covariance misspecification in Gaussian mixture models}
\section{Introduction}
Multivariate Gaussian mixtures are a class of models that provide a flexible parametric approach for the representation of heterogeneous multivariate outcomes potentially originating from distinct subgroups in the population. An overview of finite mixture models is available in many texts \cite{everitt1981,titterington1985,mclachlan1988,mclachlan2000,fruhwirth2006}. We can estimate covariate effects on the outcome as well as group membership probabilities by extending mixture models to include a regression structure for both the mean and mixing proportions. See De la Cruz-Mes\'{\i}a et al. \cite{cruzmesia2008} for a review of finite mixture models with a regression mean structure and Wedel \cite{wedel2002} for a history of concomitant variable models that use baseline variables to explain variation in subgroups. These extensions are used in several medical applications  \cite{pranab2011} including epidemiology, genomics, and pharmacology in addition to other fields including astronomy, biology, economics, and speech recognition. When the multivariate outcome is a vector of repeated measures taken over time, these methods are identified as group-based trajectory modeling \cite{nagin1999,nagin2005} or latent-class growth analysis \cite{muthen2000, muthen2001}. See Pickles and Croudace \cite{pickles2010} and references within for a review of mixture methods applied to longitudinal data. The use of mixture models for multivariate data is increasing due to computational advances that have made maximum likelihood (ML) parameter estimation possible, via the Expectation Maximization (EM) algorithm, through model-specific packages such as Proc Traj in SAS \cite{jones2001}, flexmix  \cite{leisch2004} and mclust \cite{fraley1999} in R, and software such as Mplus \cite{muthen2010} and Latent Gold \cite{vermunt2005}.

 Despite the increased use of these models, the sensitivity of estimated regression coefficients to model assumptions has only been explored to a limited degree. In a multivariate mixture model, one must specify the component distribution, the form of the mean, the structure of the covariance matrix, and the number of components; therefore, there are many ways to misspecify the model. For example, in practice, the number of components is unknown and model selection procedures based on the Bayesian information criterion are often employed. However, if the specified covariance structure is too restrictive relative to the truth, the estimated number of components will typically be greater than the true number because more components are needed to model the extra variability. The literature in estimating the number of components is  vast \cite{oliveira2005} and continues to debate this unresolved issue. Owing to the potential complexity of mixture models, simplifying assumptions are made to reduce the dimension of the parameter space, to make estimation possible, and for computational convenience. In particular, many researchers assume Gaussian components and/or restrict the components to have equal variance, both of which are known to result in asymptotic bias if the assumptions are not met \cite{gray1994, lo2011}. In this chapter, we assume that the number of components, mean structure, and distribution are known and focus on other indeterminacies such as the covariance matrix.
 
In terms of the covariance matrix, eigenvalue and Cholesky decompositions \cite{banfield1993,mcnicholas2010}, as well as mixed effects structures \cite{muthen1999}, are used to impose structure and parsimony. Additionally, one common assumption is conditional independence---given the mixture component label, the outcomes for a subject are assumed independent \cite{ostbye2011,muthen2008}. Of the available software that estimate regression effects for the mean and mixing probabilities, most of them make this simplifying assumption. This restriction is convenient when the data are unbalanced or if the sample size is small to make estimation of the covariance parameters more stable. Despite the wealth of proposed covariance models, there has been little work done in the area of mixture models with misspecified covariance structures, and the conditional independence assumption is unlikely to hold in many multivariate data settings, specifically in longitudinal applications. If the mixture consists of one component, the work carried out by Liang, Zeger \cite{liang1986} suggests that regression estimates are asymptotically unbiased. However, these properties do not hold with additional components since estimation includes mixing proportions as well as component parameters. 

Here, we investigate the impact of covariance misspecification on ML estimation of parameters and standard errors in multivariate Gaussian mixture models. In particular, our focus is on the assumption of conditional independence for the covariance structure; therefore, we assume the number of components, the distribution, and the mean structure is known. This chapter is organized as follows. Section 2 presents the model specification. Section 3 describes the estimation procedure, issues, and asymptotic properties of the parameter estimators based on the seminal results of White \cite{white1982}.  In Section 4, we present a series of simulations of a simple misspecified example to compare asymptotic and finite-sample bias of parameter and standard error estimates under varying levels of dependence and separation between components. In Section 5, we apply these ideas to body mass index data from a national longitudinal study to demonstrate the effects of misspecification on potential inferences made in practice. 

\section{Model specification}
In a finite multivariate mixture, the density of a random vector $\B y$ takes the form
$$f(\B y)=\pi_{1}f_{1}(\B y)+\cdots+\pi_{K}f_{K}(\B y)$$
where $\pi_{k}>0$ for $k=1,...,K$ and $\sum^{K}_{k}\pi_{k}=1$. The parameters $\pi_{k}$ are mixing proportions, and the functions $f_{1},...,f_{K}$ are component densities, assumed multivariate Gaussian here. 

We extend the general model to allow other factors to affect the mean as well as the mixing proportions. Let $\B y\in\mathbb{R}^{m}$ be a random vector whose distribution, conditional on regression covariates, $\B x$, and concomitant variables, $\B w$, is a mixture of $K$ Gaussian densities with mixing proportions: $\pi_{1}(\B w,\BS \gamma),....,\pi_{K}(\B w,\BS \gamma)$. That is, the conditional mixture density for $\B y$ is defined by
\begin{align}
f(\B y|\B x,\B w,\BS \theta)=\sum^{K}_{k=1}\pi_{k}(\B w,\BS\gamma)f_{k}(\B y|\B x,\BS\theta_{k}) \label{mixdens}
\end{align}
where $f_{k}(\B y|\B x,\BS\theta_{k})$ denotes the $m$-variate Gaussian probability density function with mean $\B x\BS \beta_{k}$ and covariance matrix $\BS\Sigma_{k}$ $(k=1,...,K)$, $\BS\theta_{k}$ includes both $\BS\beta_{k}$ and $\BS\Sigma_{k}$, $\B x$ is a $m\times p$ matrix, and $\B w$ is a vector of length $q$. The regression covariates include measures that affect the mean, whereas the concomitant variables influence the mixing proportions. This general structure allows the possibility that some baseline variables could be in both $\B x$ and $\B w$. 

We parameterize the mixing proportions using the multinomial/generalized logit model with the form
$$\pi_{k}(\B w,\BS\gamma)=\frac{\exp(\B w^{T}\BS\gamma_{k})}{\sum_{j=1}^{K}\exp(\B w^{T}\BS\gamma_{j})}$$ 
for $k=1,...,K$ where $\BS \gamma_{k}\in\mathbb{R}^{q}$, $\BS\gamma = (\BS\gamma_1^T,...,\BS\gamma_K^T)$ where $\BS\gamma_{K}=\B 0$.

Throughout this chapter, we generally assume conditional independence with constant variance within a component where $\B\Sigma_{k}=\sigma^{2}_{k}I_{m}$ as the {\em proposed estimation model}, but it is straightforward to extend the covariance model to include other correlation structures such as exchangeable or exponential. Therefore, the vector of all unknown parameters, $\BS\theta$, consists of the mixing proportion parameters, $\BS\gamma_{k}$, and the component regression and variance parameters, $\BS\theta^{T}_{k}=(\BS\beta^{T}_{k},\sigma^{2}_{k})$, for $k=1,...,K$ and could include correlation parameters.  

\section{Parameter estimation}
\subsection{EM Algorithm}
Under the assumption that $\B y_{1},...,\B y_{n}$ are independent realizations from the mixture distribution, $f(\B y | \B x, \B w, \BS\theta)$, defined in \ref{mixdens}, the log likelihood function for the parameter vector, $\BS \theta$ is given by
$$\log L(\BS\theta)=\sum^{n}_{i=1}\log f(\B y_{i}|\B x_{i},\B w_{i},\BS \theta).$$
The ML estimate of $\BS\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\BS\theta)/\partial \BS\theta=\B 0.$
Solutions of this equation corresponding to local maxima can be found iteratively through the EM algorithm \cite{dempster1977}, which is thoroughly described in Section \ref{subsec:em}. 

\subsection{Asymptotic properties of estimates}
If the true underlying data-generating distribution is a member of the specified model class, then ML estimation via the EM algorithm gives parameter estimates that are consistent  \cite{wald1949, lecam1953}. However, if the specified model does not contain the true underlying mixture, then the ML estimators potentially have asymptotic bias \cite{gray1994,lo2011}. Here, we are interested in the impact of misspecifying the covariance matrix structure on parameter estimation and inference.

General theoretical results for ML estimators are given by White \cite{white1982}. Our investigation is a special case where the covariance matrices of mixture components are incorrectly specified but the mean structure and distribution are known. Let $f(\B y|\BS \theta)$ be the assumed estimation model, $g(\B y)$ be the true density, and $C$ be a compact subset of the parameter space. It follows that the ML estimator, $\hat{\BS\theta}_{n}$, is consistent for the parameter vector, $\BS\theta^{*}$, that minimizes the Kullback-Leibler divergence, $\int \log[g(\B y)/f(\B y|\BS\theta)]g(\B y)d\B y=\int \log[g(\B y)]g(\B y)d\B y-\int \log[f(\B y|\BS\theta)]g(\B y)d\B y$, under some regularity conditions \cite{white1982}, which is equivalent to maximizing $\int \log[f(\B y|\BS\theta)]g(\B y)d\B y$ with respect to $\BS\theta$.  

In the case of mixture densities, this integral is mathematically intractable. Lo \cite{lo2011} used a modified EM algorithm for univariate data that maximized $\int \log[f(\B y|\BS\theta)]g(\B y)d\B y$  with respect to $\BS \theta$ in order to estimate $\BS\theta^{*}$. This procedure could be adapted to bivariate data, but for outcome vectors of larger dimension, this procedure is not as useful.  We know that for outcome vectors, $\{\B y_i\}_{i=1,...,n}$, generated from the true density, under suitable regularity conditions \cite{jennrich1969},
$$\sup_{\BS \theta\in C}\frac{1}{n}\sum_{i=1}^{n} \log f(\B y_{i}|\BS \theta)\overset{a.s.}{\longrightarrow} \sup_{\BS \theta\in C} \int^{\infty}_{-\infty} \log f(\B y|\BS \theta)g(\B y)d\B y \quad \text{ as } n\rightarrow \infty$$
Therefore, to investigate asymptotic bias under a misspecified covariance structure when $g(\B y)$ is known, we numerically approximate $\BS\theta^{*}$ using the EM algorithm on a large sample from $g(\B y)$ of size $n=100,000$. 

In addition to consistency, White \cite{white1982} also showed that $\sqrt{n}(\hat{\BS\theta}_{n}-\BS\theta^{*})\rightarrow N(0, C(\BS\theta^{*})),$ where the asymptotic covariance matrix is
$C(\BS\theta^{*})=A(\BS\theta^{*})^{-1}B(\BS\theta^{*})A(\BS\theta^{*})^{-1}$, with
$$A(\BS\theta^{*})=\left\{E\left(\frac{\partial^{2}\log f(\B y_{i}|\BS\theta^{*})}{\partial \theta_{j}\partial\theta_{l}}\right)\right\},\quad B(\BS\theta^{*})=\left\{E\left(\frac{\partial\log f(\B y_{i}|\BS\theta^{*})}{\partial \theta_{j}}\cdot \frac{\partial\log f(\B y_{i}|\BS\theta^{*})}{\partial \theta_{l}}\right)\right\}.$$
Moreover, $C_{n}(\hat{\BS\theta}_{n})=A_{n}(\hat{\BS\theta}_{n})^{-1}B_{n}(\hat{\BS\theta}_{n})A_{n}(\hat{\BS\theta}_{n})^{-1}\stackrel{a.s.}{\rightarrow} C(\BS\theta^{*})$, with
$$A_{n}(\hat{\BS\theta}_{n})=\left\{\frac{1}{n}\sum^{n}_{i=1}\frac{\partial^{2}\log f(\B y_{i}|\hat{\BS\theta}_{n})}{\partial \theta_{j}\partial\theta_{l}}\right\},\quad B_{n}(\hat{\BS\theta}_{n})=\left\{\frac{1}{n}\sum^{n}_{i=1}\frac{\partial\log f(\B y_{i}|\hat{\BS\theta}_{n})}{\partial \theta_{j}}\cdot \frac{\partial\log f(\B y_{i}|\hat{\BS\theta}_{n})}{\partial \theta_{l}}\right\}.$$
 Following a similar procedure as Boldea and Magnus \cite{boldea2009}, we derive the score vector and Hessian needed to calculate $A_{n}$ and $B_{n}$ for a multivariate Gaussian mixture model as specified in this chapter. Derivations are found in the appendix.
 
If the model is correctly specified, then both $-A_{n}(\hat{\BS\theta}_{n})^{-1}$ and $B_{n}(\hat{\BS\theta}_{n})^{-1}$ are consistent estimators of $C(\BS\theta^{*})$ \cite{white1982}, and two possible variance-covariance estimates for the parameter estimator are
$$\widehat{\text{Cov}}_{1}(\hat{\BS\theta}_{n}) = \widehat{\B W}_{1}=-(nA_{n}(\hat{\BS\theta}_{n}))^{-1}$$
\begin{center}and\end{center}
$$\widehat{\text{Cov}}_{2}(\hat{\BS\theta}_{n})= \widehat{\B W}_{2}=(nB_{n}(\hat{\BS\theta}_{n}))^{-1}$$
On the other hand, $A_{n}(\hat{\BS\theta}_{n})^{-1}B_{n}(\hat{\BS\theta}_{n})A_{n}(\hat{\BS\theta}_{n})^{-1}$ provides a consistent estimator of $C(\BS\theta^{*})$ {\em despite any misspecification}. Therefore, a third and {\em robust} variance estimate of the parameter estimator is given by
$$\widehat{\text{Cov}}_{3}(\hat{\BS\theta}_{n})= \widehat{\B W}_{3}=n^{-1}A_{n}(\hat{\BS\theta}_{n})^{-1}B_{n}(\hat{\BS\theta}_{n})A_{n}(\hat{\BS\theta}_{n})^{-1}.$$
 We refer to calculated standard error estimates corresponding to these three indexed variance-covariance estimates throughout the rest of the chapter.
 
 \section{Simulation study}
We carry out two series of simulations to examine the behavior of the ML estimators in terms of bias under misspecification of the covariance structure for finite samples from a multivariate mixture. Specifically, we are mainly interested in the impact of dependence in the true error structure on bias in parameter and standard error estimates when the conditional independence is assumed incorrectly and how this is affected by the (i) level of dependence and (ii) the separation between mixture components. Secondly, we investigate the behavior of the bias when the estimation structure gets closer to the true correlation structure by comparing the bias under three correlation structures.

In all of the simulations, the data with sample size $n$ are generated from an $m$-variate Gaussian mixture model with parameters $(\BS\gamma_{k},\beta_{k},\sigma^{2}_{k},\B R_k)$ for $k=1,...,K$ where $\B R_k$ is the true correlation structure as follows:
\begin{itemize}
\item Fix $K$. 
\item For each subject, $i=1,...,n$, 
\begin{itemize}
\item Fix $\B x_{i} = \B 1_{m}$ and $\B w_{i}=1$.
\item Construct matrices $\B A_k$ such that $\B A_k\B A_k^{T}=\B R_k$ for $k=1,..,K$ using the Cholesky decomposition. 
\item Randomly assign group membership, $h_{i}$, by drawing a value from the categorical distribution defined by $P(h=k)=\pi_{k}(\B w_{i},\BS\gamma)$ for $k=1,...,K$. 
\item Draw $m$ standard normal random values $\B e_{i}$ and let
$$\B y_{i}=\B x_{i}\beta_{h_{i}}+\sigma_{h_{i}}\B A_{h_{i}}\B e_{i}$$
\end{itemize}
\end{itemize}
Thus, $\B y_{i}\sim N(\B x_{i}\beta_{h_{i}},\sigma_{h_{i}}^{2}\B R_{h_{i}})$. We then estimate the parameters and standard errors, $\widehat{SE}_1$, $\widehat{SE}_2$, $\widehat{SE}_3$, using constrained maximum likelihood via the EM algorithm \cite{hathaway1985} doing five random initializations, on the basis of a multivariate mixture model with a specified correlation structure and known design matrix.

 For simplicity, we focus on a example of two Gaussian components ($K = 2$) with constant mean vectors (i.e. no relationship between covariates and $\B y$), one component with independent errors, the other with some level of dependency in the errors. For the first series, the latter dependence is based on an exchangeable correlation structure where all outcomes in an observational unit are equally correlated, which is mathematically equivalent to a random intercept model if the correlation is positive.
 
To investigate the influence of the level of dependence, we set the vector length to $m=5$, equal mixing proportions ($\gamma_{1}=0$; baseline variables have no effect), mean of the components to $\beta_{1}=1$ and $\beta_{2}=3$, and the variance of the components to $\sigma^{2}_{1}=0.25$ and $\sigma^{2}_{2}=1$. The errors are independent ($\B R_{1}=I_{m}$) in component one and we let the level of dependence vary with $\rho=0,0.5,0.99$ within the exchangeable structure ($\B R_2 = \rho(J_m-I_m)+I_m$ where $J_m$ is a $m\times m$ matrix of 1's) for component two. We present the bias of parameter estimates and the three standard error estimates under these conditions.

Then, we consider the separation between two component distributions using the concept of c-separation \cite{dasgupta1999}. Two Gaussian distributions, $N(\BS\mu_{1},\BS\Sigma_{1})$ and $N(\BS\mu_{2},\BS\Sigma_{2})$, in $\mathbb{R}^{m}$ are c-separated if $||\BS\mu_{1}-\BS\mu_{2}||_{2}\geq c\sqrt{m\cdot\max(\lambda_{max}(\BS\Sigma_{1}),\lambda_{max}(\BS\Sigma_{2}))}$
where $\lambda_{max}(\BS\Sigma)$ is the largest eigenvalue of $\BS\Sigma$.  Dasgupta \cite{dasgupta1999} notes that two Gaussians are almost completely non-overlapping when $c=2$. This inequality can be rearranged to establish a measure of separation,
$$S = ||\BS\mu_{1}-\BS\mu_{2}||_{2}/\sqrt{m\cdot\max(\lambda_{max}(\BS\Sigma_{1}),\lambda_{max}(\BS\Sigma_{2}))},$$
which is a standardized Euclidean distance between mean vectors. In this simulation, we calculate the value of S for data-generating component densities as a measure of the separation between the two components and if $S>2$, the components do not overlap and are well separated. For this series of simulations, we again use a strong level of dependence ($\rho=0.99$) in the exchangeable structure, a vector length of $m=5$, but vary the mean and variance of the second component ($\beta_{2}=3,5$ and $\sigma_{2}^{2}=0.25,1,4$) to invoke different degrees of separation between components.

We perform 1000 replications of each simulation for sample sizes $n=100, 500, 1000$. We approximate the true standard error with the standard deviation of the replicates. To estimate the asymptotic bias of the model parameters ($n=\infty$), we complete one replication with $n=100,000$.

The two prong simulation described above focuses on the impact of using a conditional independence estimation model under different levels of dependence and separation in the data-generating components. In practice, we can choose correlation structures other than conditional independence. To explore the bias under difference covariance assumptions, we run a short simulation adjusting the data-generating model from above to use an exponential correlation structure, such that the dependence decreases as the time lag increases, rather than the constant dependence from the exchangeable structure. Therefore, for component two, the correlation between two measurements within a subject that are observed $d$ time units apart is $\exp(-d/r)$ where $r$, the range parameter, determines how quickly the correlation decays to zero. This structure is general enough so that if $r$ is close to zero, the correlation matrix is close to conditional independence and if $r$ is very large, the structure is close to exchangeable correlation with strong dependence. 

For this simulation, we continue using the two Gaussian components ($K = 2$) with constant mean vectors ($\beta_{1}=1$ and $\beta_{2}=3$) of length $m=5$ with observations at times $t=1,2,3,4$ and $5$, one component with independent errors, and the second component with a moderate level of dependence that decays exponentially ($r=3$). We estimate the mixing proportion, mean, and variance parameters assuming different correlation structures: conditional independence, exchangeable, and exponential correlation. We estimate and compare the finite-sample bias of parameters and standard errors by letting $n=500$ with 1000 replications. Additionally, we compare the conventional estimate $\widehat{\B W}_{1}$ of the covariance matrix of $\widehat{\BS \theta}$ and the robust estimate $\widehat{\B W}_{3}$. If the estimation model is close to the true structure, the matrices should be similar and $\B Q = \widehat{\B W}_{1}^{-1}\widehat{\B W}_{3}$ should be close to the identity matrix. We calculate $RJ=tr(\B Q)/\nu$ where $\nu$ is the length of $\widehat{\BS\theta}$, which has been termed the RJ criteria and should be close to 1 if the estimation model is close to the truth \cite{shults2009,rotnitzky1990}.


\subsection{Results}
Table \ref{tab:dep1} lists bias estimates for the dependence-varying simulation study. The estimates range from close to zero when $\rho=0$ to magnitudes of upwards of 0.3 when $\rho = 0.99$. It is clear from this table that stronger dependence in the errors results in greater finite-sample and asymptotic bias when estimating under the conditional independence assumption. Additionally, the magnitude of bias seems to reach the level of asymptopia at sample sizes of $n=500$, but it is important to note that the estimates for the asymptotic bias, based on one replication with $n=100,000$, are only numerically accurate to two decimal places for $\gamma_{1},\sigma^{2}_{1},$ and $\sigma^{2}_{2}$. We see this numerical inaccuracy when $\rho=0$ since the asymptotic bias should be zero when the conditional independence assumption is met.  In terms of standard error estimates, the bias increases with increased dependence with values ranging from 0.001 when $\rho=0$ to 0.111 when $\rho = 0.99$. We see a divergence between the three variance estimators with $\widehat{SE}_3$, the robust estimator, consistently having the least bias (Table \ref{tab:dep2}).  When the model is correctly specified with $\rho=0$, the three estimators are similar as supported by asymptotic theory.  
% latex table generated in R 2.15.1 by xtable 1.7-0 package
% Wed Oct 10 10:18:23 2012
\begin{table}[ht]
\begin{center}
\begin{tabular}{llccccc}
   \hline \multicolumn{7}{c}{Bias Estimates}\\ $\rho$ & $n$ & $\widehat{\gamma}_1$ & $\widehat{\beta}_1$ & $\widehat{\sigma}^2_1$ & $\widehat{\beta}_2$ & $\widehat{\sigma}^2_2$ \\ 
   \hline$0.00$ & $100$ & -0.004 (0.006) & 0.000 (0.001) & -0.000 (0.001) & -0.002 (0.002) & -0.006 (0.003) \\ 
    & $500$ & 0.005 (0.003) & -0.000 (0.000) & -0.000 (0.000) & 0.001 (0.001) & -0.000 (0.001) \\ 
    & $1000$ & 0.004 (0.002) & -0.000 (0.000) & -0.000 (0.000) & 0.001 (0.001) & -0.000 (0.001) \\ 
    & $\infty$ & -0.001 & 0.000 & 0.001 & -0.000 & 0.002 \\ 
  $0.50$ & $100$ & 0.125 (0.006) & 0.031 (0.001) & 0.024 (0.001) & 0.106 (0.003) & -0.136 (0.003) \\ 
    & $500$ & 0.125 (0.003) & 0.028 (0.001) & 0.024 (0.000) & 0.101 (0.002) & -0.124 (0.001) \\ 
    & $1000$ & 0.125 (0.002) & 0.028 (0.000) & 0.024 (0.000) & 0.098 (0.001) & -0.125 (0.001) \\ 
    & $\infty$ & 0.115 & 0.027 & 0.024 & 0.095 & -0.125 \\ 
  $0.99$ & $100$ & 0.370 (0.007) & 0.087 (0.002) & 0.033 (0.001) & 0.327 (0.005) & -0.410 (0.005) \\ 
    & $500$ & 0.346 (0.003) & 0.078 (0.001) & 0.030 (0.001) & 0.310 (0.002) & -0.388 (0.002) \\ 
    & $1000$ & 0.350 (0.002) & 0.079 (0.001) & 0.030 (0.000) & 0.309 (0.001) & -0.385 (0.001) \\ 
    & $\infty$ & 0.353 & 0.082 & 0.031 & 0.315 & -0.383 \\ 
   \hline\end{tabular}
\caption{Bias estimates (SE) of maximum likelihood parameter estimators when the covariance structure of a two-component Gaussian mixture is assumed to be conditionally independent based on 1000 replications under each mixture distribution with $m=5$, $\gamma_1=\gamma_2=0$, $\beta_{1}=1$, $V_1=I_{m}$, $\sigma_1^{2}=0.25$, $\beta_2=3$, $R_2=R(\rho)$ and $\sigma_2^{2}=1$ where $R(\rho)$ is the exchangeable correlation matrix. Asymptotic estimates ($n=\infty$) are based on one replication with $n=100,000$. Values equal to zero represent values less than 0.001.}
\label{tab:dep1}
\end{center}
\end{table}
\begin{landscape}
% latex table generated in R 2.15.1 by xtable 1.7-0 package
% Wed Oct 10 10:18:23 2012
\begin{table}[ht]
\begin{center}
\begin{tabular}{llccccccccc}
   \hline \multicolumn{11}{c}{Bias Estimates}\\ $\rho$ & $n$ & $\widehat{SE}_1(\widehat{\gamma}_1)$ & $\widehat{SE}_2(\widehat{\gamma}_1)$ & $\widehat{SE}_3(\widehat{\gamma}_1)$ & $\widehat{SE}_1(\widehat{\beta}_1)$ & $\widehat{SE}_2(\widehat{\beta}_1)$ & $\widehat{SE}_3(\widehat{\beta}_1)$ & $\widehat{SE}_1(\widehat{\sigma}^2_1)$ & $\widehat{SE}_2(\widehat{\sigma}^2_1)$ & $\widehat{SE}_3(\widehat{\sigma}^2_1)$ \\ 
   \hline$0.00$ & $100$ & 0.007 & 0.007 & 0.007 & 0.000 & 0.001 & -0.000 & -0.001 & 0.001 & -0.001 \\ 
    & $500$ & 0.002 & 0.002 & 0.002 & -0.000 & 0.000 & -0.000 & 0.000 & 0.000 & 0.000 \\ 
    & $1000$ & 0.001 & 0.001 & 0.001 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\ 
  $0.50$ & $100$ & 0.004 & 0.004 & 0.004 & -0.006 & -0.008 & -0.002 & -0.007 & -0.009 & -0.004 \\ 
    & $500$ & 0.002 & 0.002 & 0.002 & -0.003 & -0.004 & -0.001 & -0.002 & -0.004 & -0.001 \\ 
    & $1000$ & -0.001 & -0.001 & -0.001 & -0.002 & -0.002 & -0.000 & -0.001 & -0.002 & -0.000 \\ 
  $0.99$ & $100$ & -0.003 & -0.003 & -0.002 & -0.026 & -0.035 & -0.010 & -0.013 & -0.016 & -0.008 \\ 
    & $500$ & -0.005 & -0.005 & -0.004 & -0.011 & -0.015 & -0.003 & -0.005 & -0.007 & -0.002 \\ 
    & $1000$ & -0.003 & -0.003 & -0.002 & -0.007 & -0.010 & -0.001 & -0.003 & -0.005 & -0.002 \\ 
   \hline$\rho$ & $n$ &   &   &   & $\widehat{SE}_1(\widehat{\beta}_2)$ & $\widehat{SE}_2(\widehat{\beta}_2)$ & $\widehat{SE}_3(\widehat{\beta}_2)$ & $\widehat{SE}_1(\widehat{\sigma}^2_2)$ & $\widehat{SE}_2(\widehat{\sigma}^2_2)$ & $\widehat{SE}_3(\widehat{\sigma}^2_2)$ \\ 
   \hline$0.00$ & $100$ &   &   &   & 0.001 & 0.003 & 0.001 & -0.008 & -0.004 & -0.010 \\ 
    & $500$ &   &   &   & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.000 \\ 
    & $1000$ &   &   &   & 0.000 & 0.000 & 0.000 & 0.001 & 0.001 & 0.001 \\ 
  $0.50$ & $100$ &   &   &   & -0.046 & -0.070 & -0.005 & -0.019 & -0.028 & -0.004 \\ 
    & $500$ &   &   &   & -0.021 & -0.032 & -0.002 & -0.010 & -0.016 & -0.001 \\ 
    & $1000$ &   &   &   & -0.014 & -0.021 & -0.000 & -0.009 & -0.013 & -0.002 \\ 
  $0.99$ & $100$ &   &   &   & -0.092 & -0.122 & -0.017 & -0.085 & -0.111 & -0.022 \\ 
    & $500$ &   &   &   & -0.040 & -0.054 & -0.006 & -0.035 & -0.049 & -0.003 \\ 
    & $1000$ &   &   &   & -0.027 & -0.037 & -0.002 & -0.026 & -0.036 & -0.004 \\ 
   \hline\end{tabular}
\caption{Bias estimates of the three standard error estimators ($SE_1$, $SE_2$, $SE_3$) when the covariance structure of a two-component Gaussian mixture is assumed to be conditionally independent based on 1000 replications under each mixture distribution with $m=5$, $\gamma_1=\gamma_2=0$, $\beta_{1}=1$, $R_1=I_{m}$, $\sigma_1^{2}=0.25$, $\beta_2=3$, $R_2=R(\rho)$ and $\sigma_2^{2}=1$ where $R(\rho)$ is the exchangeable correlation matrix. Approximate standard error is based on the estimated standard deviation of the simulation distribution. Values equal to zero represent values less than 0.001.}
\label{tab:dep2}
\end{center}
\end{table}
\end{landscape}
Figure \ref{fig:2-1} shows that the relationship between the level of component separation and the magnitude of bias is complex. As in the previous simulation, sample sizes of $n=500$ and larger produce similar bias estimates so we only present the asymptotic results. When the level of separation is high, $S>2$, then the magnitude of the bias is small, but when there is some overlap, $S<2$, there is not a clear, consistent relationship between the value of $S$ and the magnitude of the estimated bias for all parameters. That is, for two sets of parameter values, such as $(\beta_{2}=3,\sigma^{2}_{2}=0.25)$ and $(\beta_{2}=5,\sigma^{2}_{2}=1)$, that have the same level of separation, $S = 2.836$, the magnitude of the bias for all parameter estimates is drastically different for the two settings. However, in general, the bias decreases as the level of separation increases for a fixed mean parameter. The only exception is that the estimator for the first component mean ($\widehat{\beta}_{1}$) has increased bias when $S=1.418$ as compared to $S=0.709$, but the bias then decreases when $S=2.836$. It appears that when there is high overlap between two components, there is a point at which the bias peaks and then starts to decrease as $\sigma_2$ increases even though the amount of overlap continues to increase. Lastly, similar to the parameter estimates, the greater amount of separation results in less bias in the standard errors with biases as large as 1.0 unit in the situation with the most overlap and as little as less than 0.001 when components are well separated. Again, the robust estimator again has the lowest bias. 
\begin{figure}
\begin{center}
\includegraphics[height=3.3in]{Chp2Figure1}
\end{center}
\caption{Asymptotic bias estimates of maximum likelihood parameter estimators when the covariance structure of a two-component Gaussian mixture is assumed to be conditionally independent based on one replication with $n=100,000$ under each mixture distribution with $m=5$, $\gamma_1=\gamma_2=0$, $\beta_{1}=1$, $R_1=I_{m}$, $\sigma_1^{2}=0.25$,  $R_2=R(\rho)$, and $\rho=0.99$ where $V(\rho)$ is the exchangeable correlation matrix. The level of separation ($S$) is calculated using the true mixture distribution. For $\beta_{2} = 3$, variance parameters, $\sigma_{2}^{2}=0.25,1,4$ result in $S=2.836,1.418, 0.709$, respectively. For $\beta_{2} = 5$, variance parameters, $\sigma_{2}^{2}=0.25,1,4$ result in $S=5.671,2.836,1.418$, respectively. Values of $S\geq 2$ indicate almost completely separated components.}
\label{fig:2-1}
\end{figure}

The simulations based on dependence and separation demonstrate the finite-sample and asymptotic bias in the ML estimators when the covariance structure is misspecified as conditional independence and the mixture components overlap. However, if two components are well separated, the misspecification of the dependence in the errors does not result in large biases and thus any finite-sample bias could be removed potentially conventional techniques such as bootstrapping with careful tracking of component labels \cite{grun2004}. Additionally, when there is no covariance misspecification or when components are well separated, all of the standard error estimates are similar and have little bias. However, when there is misspecification in the dependence structure, the estimates basely solely on the Hessian matrix or the score vector understate the true variability while the robust estimate has little bias. In cases where the true level of dependence is high, the bias in the Hessian estimator, $\widehat{SE}_{1}$, and the robust version, $\widehat{SE}_{3}$, can differ by as much as a relative factor of 2. In simulations not shown, using unequal mixing proportions result in similar conclusions. When the component proportions are unbalanced, the magnitude of bias increases when a majority of observations units originate from the misspecified component (here component 2). 

Figure \ref{fig:2-2} shows the absolute bias of parameter estimates under the three different covariance assumptions when the data was generating with the exponential correlation structure for component 2. As expected, when the model is correctly specified, there is very little bias. We note that assuming the exchangeable structure, while incorrect, results in less bias than assuming conditional independence. As the RJ criteria gets closer to 1 from 1.97 to 1.02 to 0.99 using independence, exchangeable, and then exponential, the model structure gets closer to the true structure resulting in little bias in the parameter estimates.
\begin{figure}
\begin{center}
\includegraphics[height=3in]{Chp2Figure2}
\end{center}
\caption{Bias estimates of maximum likelihood parameter estimators when the covariance structure of a two-component Gaussian mixture is assumed to be conditionally independent, exchangeable, and exponential structure based on 1000 replications under each mixture distribution with $n=500$, $m=5$, $\gamma_1=\gamma_2=0$, $\beta_{1}=1$, $V_1=I_{m}$, $\sigma_1^{2}=0.25$, $\beta_2=3$, $R_2=R(r)$ and $\sigma_2^{2}=2$ where $V(r)$ is the exponential correlation matrix and $r=3$. Mean values of the RJ criteria are $RJ = 1.97, 1.02, 0.99$ for the three covariance assumptions, respectively.}
\label{fig:2-2}
\end{figure}

\section{Body mass index data example}
To look at the behavior of the parameter and standard error estimates in practice, we use data from the 1979 National Longitudinal Survey of Youth (NLSY79). The NLSY79 is a nationally representative sample of 12,686 young American men and women aged 14-22 years in 1979. The cohort, interviewed annually from 1979 to 1994 and biennially thereafter, has provided health and economic data for a total of 23 interviews (until 2008).  In particular, the available body weight data for the 1979 cohort span a twenty-five year period \cite{ostbye2011}. We study body mass index (BMI) over time as it is an important longitudinal measure for public health and elucidating obesity development.  Self-reported weight was collected in 17 interviews and height in five of those. BMI [weight ($kg$)/height ($m^{2}$)] was calculated for each interview based on the weight and the average height.\\

For the purposes of this chapter, the complex sampling structure is ignored and we randomly sample 500 subjects who were at least 18 years of age in 1981 and reported all 17 weight measurements. Of this sample, 51\% are female, 54\% are non-Hispanic/non-Black, 29.2\% Black and 16.8\% Hispanic. To model the BMI outcomes, we allow a quadratic relationship between mean BMI and age and include sex as a baseline concomitant variable. Therefore, for $i=1,...,500$, we assume that the observed data were generated according to
$$BMI=\beta_{k0}+\beta_{k1}\cdot (AGE-18)+\beta_{k2}\cdot (AGE-18)^{2}+\epsilon_{k}$$
with probability
$$\pi_{k}(\B w,\BS\gamma)=\frac{e^{\gamma_{k0}+I(male)\gamma_{k1}}}{\sum_{j}e^{\gamma_{j0}+I(male)\gamma_{j1}}}$$
where $\epsilon_{k}\sim N(0,\sigma_{k}^{2}\B R_k)$
for $k=1,...,4$. The choice of four groups is based on previous research \cite{ostbye2011}. Using the EM algorithm with five random initializations, we estimate parameters and standard errors and present the estimates that produced the highest log likelihood. For the sake of comparison, we complete the estimation assuming conditional independence ($\B R_k  = I_m$), and under an exchangeable ($\B R_k = \rho_k(J_m - I_m)+I_m$) and exponential ($\B V_k = \exp(-D/r_k)$ where $D$ is the Euclidean distance matrix of the ages at interviews for a subject) correlation model.
%%%%%%%
\subsection{Results}
Parameter and standard errors are estimated for a four-component multivariate Gaussian mixture model assuming conditional independence, exchangeable, and exponential correlation (Table \ref{tab:dat}). The regression parameter estimates are used to calculate the mean curves for the four groups under all three covariance assumptions, and we see that the mean curves differ between the models mainly in terms of the innermost curves (Figure \ref{fig:2-3}).  Under exchangeable correlation, one of the middle curves represents little BMI increase over time in contrast to the other groups. Under the exponential correlation assumption, the two lowest groups have a similar pattern over time, but the dependence differs between these groups with the range parameters estimated as $r_1=2.973$ and $r_2=23.579$ indicating that component 2 has more long range dependence between the BMI outcomes than component 1. Our simulation results suggest the magnitude of bias in the parameter estimator depends on how close the estimation correlation structure is to the truth and the overlap between components. We note there are no well-separated components and we see bias in the mean estimates by comparing the three covariance assumptions. 
% latex table generated in R 2.15.1 by xtable 1.7-0 package
% Wed Oct 10 10:18:23 2012
\begin{table}[ht]
\begin{center}
\begin{tabular}{lccccccccc}
  \hline  &\multicolumn{3}{c}{Conditional Independence}&\multicolumn{3}{c}{Exchangeable}&\multicolumn{3}{c}{Exponential}\\ &Estimate&$\widehat{SE}_1$&$\widehat{SE}_3$&Estimate&$\widehat{SE}_1$&$\widehat{SE}_3$&Estimate&$\widehat{SE}_1$&$\widehat{SE}_3$\\\hline  $\gamma_{10}$ & 0.827 & 0.201 & 0.461 & 0.122 & 0.182 & 0.261 & 0.420 & 0.235 & 0.442 \\ 
 $\gamma_{11}$ & -1.800 & 0.380 & 0.570 & 0.966 & 0.300 & 0.430 & -0.888 & 0.734 & 2.275 \\ 
 $\beta_{10}$ & 19.315 & 0.140 & 0.178 & 20.994 & 0.208 & 0.225 & 19.984 & 0.358 & 0.467 \\ 
 $\beta_{11}$ & 0.104 & 0.020 & 0.027 & 0.155 & 0.010 & 0.017 & 0.217 & 0.049 & 0.056 \\ 
$\beta_{12}$ & 0.001 & 0.001 & 0.001 & -0.001 & 0.000 & 0.001 & -0.002 & 0.001 & 0.001 \\ 
 $\sigma_1^2$ & 2.969 & 0.110 & 0.277 & 6.331 & 0.721 & 1.002 & 5.718 & 0.427 & 0.976 \\ 
 $\rho_1$ & - & - & - & 0.855 & 0.017 & 0.023 & - & - & - \\ 
 $r_1$ & - & - & - & - & - & - & 2.973 & 0.207 & 0.280 \\ 
 $\gamma_{20}$ & 0.599 & 0.206 & 0.435 & -0.450 & 0.216 & 0.275 & 0.716 & 0.221 & 0.523 \\ 
     $\gamma_{21}$ & 0.435 & 0.280 & 0.385 & 0.851 & 0.344 & 0.464 & 1.424 & 0.511 & 1.687 \\ 
     $\beta_{20}$ & 20.979 & 0.120 & 0.312 & 24.041 & 0.520 & 0.689 & 20.910 & 0.241 & 0.389 \\ 
     $\beta_{21}$ & 0.238 & 0.016 & 0.023 & 0.198 & 0.032 & 0.070 & 0.187 & 0.024 & 0.029 \\ 
     $\beta_{22}$ & -0.002 & 0.000 & 0.001 & -0.005 & 0.001 & 0.002 & -0.001 & 0.001 & 0.001 \\ 
     $\sigma_2^2$ & 3.150 & 0.104 & 0.365 & 21.113 & 2.926 & 4.716 & 8.147 & 0.788 & 2.458 \\ 
     $\rho_2$ & - & - & - & 0.805 & 0.027 & 0.022 & - & - & - \\ 
     $r_2$ & - & - & - & - & - & - & 23.579 & 2.016 & 4.213 \\ 
 $\gamma_{30}$ & 0.279 & 0.221 & 0.482 & 0.129 & 0.186 & 0.282 & 0.174 & 0.247 & 0.526 \\ 
     $\gamma_{31}$ & 0.534 & 0.296 & 0.402 & 1.066 & 0.304 & 0.424 & 1.739 & 0.514 & 1.575 \\ 
     $\beta_{30}$ & 22.818 & 0.207 & 0.721 & 21.165 & 0.275 & 0.372 & 23.516 & 0.385 & 0.563 \\ 
     $\beta_{31}$ & 0.449 & 0.027 & 0.051 & 0.366 & 0.016 & 0.031 & 0.464 & 0.049 & 0.085 \\ 
     $\beta_{32}$ & -0.006 & 0.001 & 0.002 & -0.002 & 0.000 & 0.001 & -0.006 & 0.001 & 0.002 \\ 
     $\sigma_3^2$ & 6.416 & 0.260 & 1.162 & 11.241 & 1.225 & 2.230 & 13.049 & 1.004 & 2.851 \\ 
     $\rho_3$ & - & - & - & 0.822 & 0.018 & 0.019 & - & - & - \\ 
     $r_3$ & - & - & - & - & - & - & 10.215 & 0.675 & 0.826 \\ 
 $\gamma_{41}$ & 0 & - & - & 0 & - & - & 0 & - & - \\ 
     $\gamma_{40}$ & 0 & - & - & 0 & - & - & 0 & - & - \\ 
     $\beta_{40}$ & 25.713 & 0.480 & 0.672 & 22.473 & 0.573 & 0.480 & 24.356 & 1.054 & 0.833 \\ 
     $\beta_{41}$ & 0.679 & 0.069 & 0.108 & 0.725 & 0.042 & 0.080 & 0.636 & 0.138 & 0.099 \\ 
     $\beta_{42}$ & -0.008 & 0.002 & 0.003 & -0.008 & 0.001 & 0.003 & -0.006 & 0.004 & 0.004 \\ 
     $\sigma_4^2$ & 26.864 & 1.154 & 7.891 & 32.982 & 3.484 & 6.928 & 44.788 & 4.093 & 13.757 \\ 
     $\rho_4$ & - & - & - & 0.681 & 0.034 & 0.044 & - & - & - \\ 
     $r_4$ & - & - & - & - & - & - & 8.058 & 0.654 & 0.850 \\ 
   \hline\end{tabular}
\caption{Parameter and standard error estimates ($\widehat{SE}_{1},\widehat{SE}_{3}$) for a random sample of 500 from NLSY79 assuming a four-component mixture model with quadratic mean and the following correlation structures: conditional independence, exchangeable, and exponential correlation. Values equal to zero represent values less than 0.001. Additionally, the RJ criteria was calculated each covariance assumption: $RJ=7.34, 3.02, 2.22$ under conditional independence, exchangeable, and exponential, respectively.}
\label{tab:dat}
\end{center}
\end{table}

\begin{figure}
\begin{center}
\includegraphics[height=3.4in]{Chp2Figure3}
\end{center}
\caption{Random sample of 500 body mass index (BMI) trajectories from NLSY79 and mean curves for the four components estimated using a Gaussian mixture model specified with a quadratic mean under the covariance assumptions: conditional independence, exchangeable, and exponential correlation. The labeled are consistent with the tables in the text: component 1 (solid), component 2 (dashed), component 3 (dotted), and component 4 (dashed-dot). Additionally, the RJ criteria was calculated for each covariance assumption: $RJ =7.34,3.02,2.22$ under conditional independence, exchangeable, and exponential, respectively.}
\label{fig:2-3}
\end{figure}

Given that the repeated outcome is BMI, we expect some dependence in the error structure within individuals.  We consider the level of dependence in errors by plotting the estimated autocorrelation function by calculating the empirical variogram of the residuals from the conditional independence model \cite{diggle2002} for each estimated component by randomly assigned each individual to a component using posterior probabilities \cite{wang2005}. The estimated autocorrelation function of the residuals shows strong dependence between residuals within 5 to 10 years and the correlation decreases with increasing time lags (Figure \ref{fig:2-4}). This correlation structure is therefore neither consistent with conditional independence nor exchangeable correlation, but rather decreases to zero which is more consistent with the exponential correlation structure. We see that the robust standard estimators are almost twice those of estimates using the standard estimators under conditional independence, and the RJ criteria, which compares the conventional and robust estimates of the covariance matrix the parameters, suggests that the exponential correlation structure is the one closest to the truth. 

 In this data example, we see the influence of the covariance structure on the estimates, especially in terms of the regression parameters. With the simulation results and the RJ criteria, we expect the exponential correlation model fits the data the best out of the three structures. However, we note that we fixed the number of components to be four for the sake of consistency and this may not be the optimal number of components. In practice, this value is estimated from the data as mentioned earlier. This data application highlights the impact of covariance misspecification as well as the fact that the mean structure may not be the only aspect differing between individuals; the level of dependence and variability also distinguish groups of individuals. 
\begin{figure}
\begin{center}
\includegraphics[width=6in]{Chp2Figure4}
\end{center}
\caption{Smoothed sample autocorrelation of component residuals of estimated Gaussian mixture model specified with a quadratic mean and conditional independence with a random sample of 500 body mass index trajectories from NLSY79 randomly assigned to components based on estimated posterior probabilities. }
\label{fig:2-4}
\end{figure}
\section{Discussion}
We have shown that covariance misspecification of a two-component Gaussian mixture may produce very little bias in regression and mixing probability parameter estimates when the components are well separated. This is well aligned with Lo's univariate findings \cite{lo2011}. However, when there is some overlap in the component distributions, assuming the wrong correlation structure can produce asymptotically biased parameter estimates, the magnitude dependent on the level of separation and how close the structure is to the truth. With misspecified mixture models, the potential for biased mixing and regression parameters estimates differs from the one-component models for which general estimating equations \cite{liang1986} produce unbiased estimates despite dependence present in the errors. Depending on the context and precision of the estimates, the bias may or may not have practical significance, but it is important to note that the ML estimators are inconsistent under covariance misspecification and there may be substantial bias when the components are not well separated.

In addition to potential biases in the parameter estimates, the simulations provide evidence that conventional standard errors estimates that are based solely on the score equation, or the Hessian, can be extremely biased and underestimate the true variability of the estimates when the covariance structure is misspecified. Therefore, standard errors should be robustly estimated using White's estimator that sandwiches the two conventional estimators. We use the exact formula for this estimator since the numerical approximations to the Hessian matrix and score vector are not by-products of the EM algorithm. To the authors' knowledge, very few software programs automatically use a robust standard error estimator, but it should be implemented in every mixture model software as the default variance estimator and presented along with standard estimators to allow for comparisons, calculation of the RJ criteria, and the detection of misspecification bias. 

Given our results, we recommend three things when estimating parameters in a mixture model. First, count the number of subjects whose maximum posterior probability is less than 0.95. If this count is non-zero, this indirectly indicates that the component distribution are not well separated, suggesting that specifying the correct correlation structure is important. Second, if the components are not well separated, fit the mixture model using several correlation structures such as conditional independence, exchangeable, and exponential correlation. For each model, calculate the RJ criteria based on the conventional and robust estimated variance-covariance matrix.  Compare the parameter estimates to see if they change under the different assumptions and assess the RJ criteria values to see which structure results in a value closest to one. Choose the most parsimonious model that has an RJ criteria value close to one.  Third, if none of these three structure fulfills this requirement, consider a more complex, potentially non-stationary covariance matrix as well as other sources of misspecification such as an incorrect number of components, assumed distribution, or an inflexible mean structure.

Our simulation study is limited, but the results likely apply to more complex mean structures and a larger number of components. In future studies, the impact of bias should be explored for more than two components with all components potentially having a misspecified covariance structure and for nonstationary covariance structures. Additionally, mixture models as specified in this chapter group individuals with similar trajectories over time; in the next chapter, we investigate methods that distinguish between the shape of the trajectory and the vertical level of the curve when grouping individuals together.




