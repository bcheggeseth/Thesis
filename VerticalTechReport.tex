\documentclass[12pt]{article}
\usepackage{fullpage,amsmath,amsfonts,graphicx,amsthm}
\usepackage{setspace}
\title{Vertically Shifting Mixture Models \\ Clustering longitudinal data by shape}
\author{Brianna C. Heggeseth}

\newtheorem{theorem}{Theorem}
  \newcommand{\B}[0]{\mathbf}
    \newcommand{\bs}[0]{\boldsymbol}

\begin{document}
\doublespace
\maketitle

A key advantage of a longitudinal study is its ability to measure individual change over time and to distinguish between ``aging'' affects and cohort variability. Typical longitudinal data analysis often involves modeling the conditional mean outcome as a function of time and explanatory variables while taking the inherent dependence into account. This type of analysis provides a useful summary of relationships present within observed groups defined by categorical variables such as race/ethnicity or sex; however, if there are unmeasured subgroups in which the relationships differ, averaging over all the data can mask interesting and useful patterns. In these circumstances, it may be more informative to partition the subjects into data-driven groups prior to estimating the associations.\\

Cluster analysis methods have been adapted and developed specifically for longitudinal data such as growth trajectories and gene expression levels \cite{schneiderman1993,genolini2010, jones2001, muthen2010, mcnicholas2010}. Most of these methods involve fitting a finite mixture model with variations in the distribution, mean, and covariance structure. The others cluster subjects with a partition method such as K-means \cite{macqueen1967,hartigan1979,} or partitioning around mediods (PAM) \cite{kaufman1990} using on a dissimilarity measure developed specifically to take the time-ordered structure of longitudinal data into account. These methods are widely used to cluster subjects based on a time-ordered vector of outcome measures. For example, researchers are working to discover distinct childhood growth pattern by clustering body mass index measured over time and determine what early life factors may contribute to following that pattern \cite{pryor2011,carter2012}. However, none of the popular methods explicitly group subjects based on the pattern or shape over time. Applied researchers have either been deceived or simply misunderstand this fact and interpret their clustering results as if the data were grouped by shape \cite{pryor2011,carter2012,nagin1999}. \\

There has been little work and discussion on the best methods to utilize when shape is the feature of interest. The few proposals are methods such that the dissimilarity measure is either based on the derivative function or correlation coefficient. Since we do not directly observe the derivative function, researchers such as M\"{o}ller-Levet et. al. \cite{moller2003} and D'Urso \cite{d2000} suggest estimating the derivative function through quotient differencing, calculating the slope of a linear interpolation between points, and then calculating the Euclidean distance between the estimates from two trajectories. This leads to calculating the dissimilarity between individuals based only on their estimated derivative curve ignoring the original level of the curve. However, many issues arise with this procedure. This method also requires balanced, evenly sampled data to compare derivative estimates from two vectors. Also, if we assume that the observed data is a realization of the model
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma_{i}^{2})$ then the quotient difference estimator of the derivative equals $\hat{f}_{i}^{'}(\tau) = (y_i(t_{j+1})-y_i(t_j))/(t_{j+1}-t_j)$ for $\tau= t_{j}$ or$ t_{j+1}$. This estimator is unbiased for the quotient difference of $f_{i}$ but not for $f_{i}^{'}(\tau)$ and is highly inefficient if the variance of the random noise is substantial,
$$\text{Var}(\hat{f}_{i}^{'}(\tau)) =  2\sigma^{2}_{i}/ (t_{j+1}-t_j)^{2},$$
which could cause significant problems when comparing derivative estimates. Lastly, there is no way to borrow strength between individuals to estimate the derivative even if we hypothesize subjects have common shape.  \\ 

The correlation coefficient has been generally used to calculate dissimilarities based on the shape of the data without much discussion about how well it does to discriminate between shapes \cite{chouakria2007,  eisen1998, chiou2008}. In the context of functional clustering, Chiou and Li \cite{chiou2008} suggested using the functional correlation as a similarity measure to cluster similar functions. However, their approach involves estimating parameters of a mixture of stochastic process in an iterative procedure that seeks to maximizing the functional correlation among all clusters. This clustering approach requires densely collected longitudinal data and therefore, it is not applicable in our circumstance. In the multivariate setting, the Pearson correlation coefficient between two vectors can be the basis of a dissimilarity measure such as $d_{cor}(x,y) = (1-cor(x,y))/2$ where $$cor(x,y) = \frac{\sum^{m}_{j=1}(x_{j}-\bar{x})(y_{j}-\bar{y})}{\sqrt{\sum^{m}_{j=1}(x_{j}-\bar{x})^{2}}\sqrt{\sum^{m}_{j=1}(y_{j}-\bar{y})^{2}}}.$$
If the signal dominates the noise, the correlation should be close to one for two vectors with the same scaled functional shapes. Therefore, a noise-free vector with an underlying functional shape of $f(x)=x$ is perfectly correlated with a noise-free vector with $f(x)=a\cdot x$ where $a\geq0$. This results in a distance of zero and the two vectors being placed in the same cluster. However, if two vectors are constant in that their functional shape is a horizontal line, the correlation coefficient will always be close to zero and thus the distance equals to 1/2. In other words, the Pearson correlation coefficient cannot detect two horizontal curves as having the same shape. Thus, this methods fails in many cases where the longitudinal values may be stable over time. This characteristic does not get discussed in the literature, but it has huge ramifications for clustering typical longitudinal data.\\

Both of these proposals are not conducive to irregularly sampled longitudinal data in which the observation times may not be consistent between subjects. Additionally, the dissimilarity-based clustering does not provide a probability framework in which to estimate the impact of baseline factors on group membership while taking into account membership uncertainty.  In this paper, we introduce a method that provides a probability framework and works with irregularly sampled longitudinal data. In the first section, we discuss related work and then we present the model specification and provide details for the mean and covariance structure in Section 2. The method implementation including parameter estimation is discussed in Section 3. A simulation study demonstrates the merits of the proposed method in comparison to standard clustering methods and those that explicitly attempt to group based on shape. Lastly, we discuss potential issues.

\section{Related Work}
A finite mixture model is a standard method for clustering multivariate data \cite{everitt2009} and has been used for longitudinal applications \cite{muthen2010, jones2001}. However, for longitudinal data, the models are commonly used for the observed data without much regard to the goal of clustering by shape. We suggest vertically shifting each subject's data by subtracting out the mean outcome level before modeling. Subtracting the level is not a novel idea in statistics or even cluster analysis. In most multivariate clustering applications, it is recommended that each variable is standardized by subtracting the mean of the variable measures and dividing by the standard deviation so that each standardized variable is in comparable units and equally contributes to the clustering process. This is not recommended for the longitudinal setting where each ``variable'' is a repeated measurement at a different time point. We want the variables to stay in the same units since the change in the original units is the aspect of interest. Any transformation performed on longitudinal data should only be additive in nature to maintain the original shape of the data over time. Rather than completing variable-specific normalization, we suggest subtracting the subject-specific level. This seems like an obvious pre-processing step; however, it is rarely used in practice and there may be unintended consequences when modeling.\\

A partial version of this idea has been implemented in the functional data analysis literature. For processes in a Hilbert space of square integrable functions with respect to the Lebesgue measure, $dt$, on the interval $\mathcal{T}=[0,T]$, Chiou and Li \cite{chiou2008} propose using a mixture model and the Karhunen-Lo{\`e}ve expansion for centered stochastic processes within their correlation-based clustering algorithm. To center the processes, they subtract the integral of the random function over interval $\mathcal{T}$ divided by $T$, the length of the interval; the resulting process integrates to zero. The integral of the process is the functional analogue to a mean vector in vector space; similarly, the resulting vector after subtraction has mean zero.\\

 Although centering a process and a shifting a vector stems from the same idea, there are distinct consequences of subtracting the estimated level of a noisy curve observed at a finite number of points that don't arise when centering a smooth function. The term centering is used in the stochastic processes literature, but we use the term vertically shifting to refer to the procedure of subtracting the mean since it graphically describes the transformation of the noisy longitudinal data.\\

\section{Model Specification}
 Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote an outcome vector of repeated observations for individual $i$, $i=1,...,n$. As longitudinal data is collected over a period of time, the vector of corresponding times of observation for individual $i$ is denoted as $\B t_{i}=(t_{i1},...,t_{im_{i}})$ and $\B z_{i}$ is a $q$-length design vector based on time-fixed factors that are typically collected at or before time $t_{i1}$. We assume that there are $K$ mean shape function, $\mu_{k}(t)$, in the population such that the outcome vector for individual $i$ in shape group $k$ is
 $$\B y_{i} = \lambda_{i}+\bs\mu_{ik}+\bs\epsilon_{i},\quad \lambda_{i}\sim F, \quad \bs\epsilon_{i}\sim(0,\bs\Sigma_{k})$$
 where $F$ is a probability distribution, $\bs\mu_{ik} = \{\mu_{k}(t_{ij})\}_{i=1}^{m_{i}}$ is a $m_{i}$-length vector of mean values evaluated at the observation times, $\B t_{i}$. Let $\bar{y}_{i}= m_{i}^{-1}\sum^{m_{i}}_{j=1} y_{ij} = \lambda_{i}+\bar{\mu}_{ik}+\bar{\epsilon}_{i}$ be the mean of the observed outcomes for individual $i$. This is one measure of the vertical level of the data vector that we remove by applying a linear transform, $\B A_{i} = B I_{m_{i}} - m_{i}^{-1}\B 1_{m_{i}}\B 1_{m_{i}}^{T}$, to the vector of observations and 
\begin{align*}
\B y^{*}_{i} &= \B A_{i}\B y_{i}\\
&=\B A_{i}(\lambda_{i}+\bs\mu_{k}+\bs\epsilon_{i})\\
&=\bs\mu_{ik}-\bar{\mu}_{ik}+\bs\epsilon_{i}-\bar{\epsilon}_{i}
\end{align*}
where $\B 1_{m_{i}}$ is a m-length vector of 1's and $\B I_{m_{i}}$ is an $m_{i}\times m_{i}$ identity matrix. Multiplying the symmetric matrix $\B A_{i}$ to the vector $\B y_{i}$ subtracts the individual mean, $\bar{y}_{i}$, from each element $\B y_{i}$, resulting in the removal of the intercept and a mean function shifted by a constant. Hence, we do not have to worry about the distribution of the random intercept, $\lambda_{i}$.\\

Therefore, we assume that the vertically shifted data, $\B y_{i}^{*}$, originated from a mixture of $K$ groups with mean shape functions and potentially dependent random noise. We assume that, conditional on $\B t$ and $\B z$, $\B y^{*}$ is a realization from a finite multivariate Gaussian mixture model with density
\begin{align*}
 f(\B y^{*}|\B t,\B z,\bs\theta) =  \sum^{K}_{k=1}\pi_{k}(\B z, \bs\gamma)f_{k}( \B y^{*}|\B t,\bs\theta_{k})\label{mixmodel}
\end{align*}
where $\pi_{k}(\B z,\gamma)>0$ for $k=1,...,K$ and $\sum^{K}_{k=1}\pi_{k}(\B z,\gamma)=1$ and $\bs\theta = (\bs\gamma,\bs\theta_{1},...,\bs\theta_{K})$. To allow baseline covariates to affect the probability of having a certain shape pattern over time, we parameterize the mixing proportions using the generalized logit function with the form
$$\pi_{k}(\B z,\bs\gamma)=\frac{\exp(\B z^{T}\bs\gamma_{k})}{\sum_{j=1}^{K}\exp(\B z^{T}\bs\gamma_{j})}$$ 
for $k=1,...,K$ where $\bs \gamma_{k}\in\mathbb{R}^{q}$, $\bs\gamma = (\bs\gamma_{1},...,\bs\gamma_{K})$, and we fix $\bs\gamma_{K}=\B 0$. We assume the component densities $f_{k}(\B y^{*}|\B t,\bs\theta_{k})$ are multivariate Gaussian with mean $\bs\mu_{k}(\B t)$ and covariance matrix $\Sigma_{k}(\B t)$.

\subsection{Mean Structure}
The mean shape is modeled as a smooth function of time represented using a chosen functional basis. If the shape is periodic in nature, a Fourier basis is appropriate. Another common basis is a lower order polynomial basis such as $\{1, t, t^{2}\}$. However, this basis cannot capture complex shapes with drastic changes. To allow local changes, we can break the time interval, $[a,b]$, up into smaller interval using $L$ knots, $a<\tau_{1}<\cdots<\tau_{L}<b$, and fit polynomials of order $m$ in each subinterval. This piecewise polynomial can be expressed as a linear combination of truncated power functions and polynomials of order $p$. In other words,
$\{1,t,t^{2},...,t^{p-1},(t-\tau_{1})_{+}^{p-1},...,(t-\tau_{L})_{+}^{p-1}\}$
is a basis for a piecewise polynomial with knots at $\tau_{1},...,\tau_{L}$. However, the normal equations associated with the truncated power basis are highly ill-conditioned. A better conditioned basis for the same function space is the B-spline basis \cite{deboor1978, schumaker1981,curry1966, de1976}. The B-spline function of order $m$ with $L$ internal knots, $\tau_{1},...,\tau_{L}$, is defined by a linear combination of coefficients and B-spline basis functions
$$\mu(t) = \sum^{L+p}_{j=1} \beta_j B_{j,p}(t)$$
where the basis functions, $B_{j,m}(t)$, are defined iteratively (cite something). Hence, the design matrix used to model the mean, $\B x_{i}$, is a matrix of values from the $m$th order B-spline basis functions taken at observation times $\B t_{i}$. We assume the mean pattern for the $k$th shape cluster is approximated by the function $\mu_{k}(t)=\sum^{L+p}_{j=1} \beta_{k,j} B_{j,p}(t) = \B x_{i}\bs\beta_{k}$ where $\bs\beta_{k}=(\beta_{k,1},...,\beta_{k, L+p}).$  

\subsection{Covariance Structure}
Various assumptions can be made about the covariance matrix, $\B \Sigma_{k}(\B t_{i})$. For this paper, we allow the covariance to differ between clusters. Since it is common for longitudinal data to have sparse, irregular time sampling, we must impose some structure on the covariance matrix to allow for parameter estimation as described by Jennrich and Schluchter in their seminal paper \cite{jennrich1986}. A common parameterization is conditional independence with constant variance where $\B \Sigma_{k}(\B t_{i}) = \sigma_{k}^{2}I_{m_{i}}$. This is an unrealistic assumption for longitudinal data since there is inherent dependence between repeated measures on the same unit. \\

Compound symmetry, which is also known as exchangeable correlation, is a popular correlation structure in longitudinal analysis where all repeated measures are equally correlated. This is typically paired with constant variance such that $\B \Sigma_{k}(\B t_{i}) = \sigma_{k}^{2}(\rho_{k}\B1_{m_{i}}\B1_{m_{i}}^{T}+(1-\rho_{k})I_{m_{i}})$ where $-1\leq\rho_{k}\leq 1$ is the correlation coefficient. This dependence structure describes the resulting correlation matrix of a random intercept model which may not be realistic in many situations but improves upon the independence model. \\

A more general structure that provides a compromise between the two is the exponential correlation structure in which the dependence decays as the time between observations increases---$\B \Sigma_{k}(\B t_{i})_{jl} = \sigma_{k}^{2}\exp(-| t_{ij}-t_{il}| / r_{k})$ where $r_{k}> 0$ is the range of the dependence. If the range, $r_{k}$, is small, the correlation decays quickly, but if the $r_{k}$ is large, there is long range dependence between measurements. This structure is similar to the correlation matrix generated from an autoregressive model of order one where $\B\Sigma_{k}(\B t_{i})_{jl} = \sigma^{2}\rho_{k}^{|t_{ij}-t_{il}|}$ where $\rho_{k}$ is the correlation for measurements observed one unit of time apart. If we set $\rho_{k} = \exp(-1/r_{k})$, then $0\leq\rho_{k}\leq1$ and the two parameterization result in the same structure when the correlation between two measures is constrained to be positive. This is a reasonable assumption for longitudinal data in the original form but it many not be acceptable for the transformed data as we discuss later.\\

The previous two structures can be combined by assuming a random intercept model plus serial correlation described by the exponential structure \cite{diggle2002}. Then the covariance matrix equals the variance of the random intercept plus the covariance of the serial correlation---$\B \Sigma_{k}(\B t_{i})_{jl}=\nu^{2}_{k}+\sigma_{k}^{2}\exp(-| t_{ij}-t_{il}| / r_{k})$.\\

The covariance structures mentioned above are associated with weakly stationary processes in that they have constant variance and the correlation only depends on the time lag between observations. If the variance or correlation function is non-constant but varying continuously, it could be potentially modeled as a function, but estimation is more difficult.\\

It is important to model the covariance structure correctly as misspecification can highly impact mixture model results in terms of parameter estimates and the final clustering if the groups are not well-separated (cite Heggeseth and Jewell). By placing the trajectories in the same range, transforming the data brings individuals with similar shapes closer but also brings others closer as well. In general, this decreases the separation between groups, which may force us to accurately model the correlation. 

\subsection{Implementation}
Given a collection of independent observed outcome vectors, $\B y_{1},...,\B y_{n}$, there are three important decisions to make before fitting the model. The first step is to calculate the mean for each subject, $\bar{y}_{i}$, $i=1,...,n$ and subtract the subject-specific mean from the observed outcome vector. This transformation leaves vertically shifted independent vectors, $\B y^{*}_{1},...,\B y^{*}_{n}$.  \\

The second step is to choose the order of the spline and the number and location of internal knots for the mean structure. The B-spline basis is kept constant for all shape groups so the simplest way to select the number of knots is through visual inspection of the full data set while considering the number of observations per subject. If the most complex shape patterns is a lower order polynomial, no internal knots are necessary. However, if the most complex curve has local activity, adding knots and increasing the order of the spline functions can flexibly accommodate the twists and turns of the mean patterns. In choosing both the order of the polynomials and the number of knots, it is important to balance the number of mean parameters with the sample size. Every unit increase in the order or in the number of knots increases the number of parameters by $K,$ the number of groups. In terms of location of the knots, it has been suggested to place knots at sample quantiles based on the sampling times of all the observations \cite{ruppert2002}. However, this strategy may not work well if the median time is not the point of deviation. If possible, it is best to place knots at local maxima, minima, and inflection points of the overall trends as to accommodate the differences from a polynomial function \cite{eubank1999}.  Once these are decided, the design matrixes $\B x_{i}$ are calculated using widely available B-spline algorithms. \\

Thirdly, the model requires the number of clusters, $K$, to be known. In practice, this is not the case and we must choose $K$. The most popular procedure is to chose a maximum value of $K$ such that $K_{max}<<n$, fit the model under all values of $K=2,...,K_{max}$, and choose the value that optimizes a chosen criteria. In this thesis, we use the Bayesian Information Criterion (BIC) \cite{schwarz1978} for choosing $K$. It is defined as
$$BIC = -2\log L(\hat{\bs\theta},K)- d\log(n)$$
where $d$ is the length of $\bs\theta$, the number of parameters in the mixture model, and $L(\bs\theta,K)$ is the log likelihood function for the parameter vector. The BIC has been widely used for model selection with mixture models since Roeder and Wasserman's use in 1997 \cite{roeder1997}. In particular, the criteria has been to select the number of clusters \cite{dasgupta1999,fraley1999} with good results in practice. For regular models, the BIC was derived as an approximation to twice the log integrated likelihood using the Laplace method \cite{tierney1986}, but the necessary regularity conditions do not hold for mixture models in general \cite{aitkin1985}. However, Roeder and Wasserman \cite{roeder1997} showed that the BIC leads to a consistent estimator of the mixture density, and Keribin \cite{keribin2000} showed that the BIC is consistent for choosing the number of components in a mixture model.\\

In order to fit the model and estimate the parameters, we use maximum likelihood estimation via the EM algorithm. Under the assumption that $\B y^{*}_{1},...,\B y^{*}_{n}$ are independent realizations from the mixture distribution, $f(\B y^{*} | \B t, \B z, \bs\theta)$, defined in \ref{mixdens}, the log likelihood function for the parameter vector, $\bs \theta$, is given by
$$\log L(\bs\theta,K)=\sum^{n}_{i=1}\log f(\B y^{*}_{i}|\B t_{i},\B z_{i},\bs \theta).$$
The ML estimate of $\bs\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\bs\theta)/\partial \bs\theta=\B 0.$ Solutions of this equation corresponding to local maxima can be found iteratively through the Expectation-Maximization (EM) algorithm \cite{dempster1977}. This algorithm is applied in the framework where given $(\B t_{i},\B z_{i})$ each $\B y^{*}_{i}$ is assumed to have stemmed from one of the components and the indicator denoting its originating component is missing. The complete-data log likelihood is based on these indicator variables as well as the observed data $\{(\B y^{*}_{i}, \B t_{i}, \B z_{i})\}$. The Expectation step (E-step) involves replacing the indicators by current values of the conditional expectation, which is the posterior probability of component membership, written as
$$\alpha_{ik}=\pi_{k}(\B z_{i},\bs\gamma)f_{k}(\B y^{*}_{i}|\B t_{i})/\sum_{j=1}^{K}\pi_{j}(\B z_{i},\bs\gamma)f_{j}(\B y^{*}_{i}|\B t_{i},\bs \theta_{j})$$
for $i=1,...,n$ and $k=1,...,K$ using current estimates of the parameters. In the Maximization step (M-step), the parameter estimates for the mixing proportions, regression effects, and covariance matrices are updated by maximizing the complete-data log likelihood using the posterior probabilities from the E-step in place of the indicator variables using numerical optimization. The E- and M-steps are alternated repeatedly until convergence. The EM algorithm guarantees convergence to a local maximum; global convergence may be attained through initializing the algorithm by randomly assigning individuals to initial components, running the algorithm multiple times and using the estimates associated with the highest log likelihood. Besides the parameter estimates, the algorithm returns the posterior probability estimates of component membership. These probabilities can be used to partition individuals into distinct clusters by selecting the cluster with the maximum posterior probability. However, unlike K-means, the posterior probability provides some measure of uncertainty in the hard clustering. 
\section{Potential Issues}
There are issues of identifiability with Gaussian mixture models that can be mitigated through some minor constraints (CITE MCLACHLAN). In this section, we discuss some unique consequences of vertically shifting the data on the model and inference.
\subsection{Covariance of Transformed Data}
We let $\B Y=(Y_{1},...,Y_{m})$ be a random vector observed at times $\B t=(t_{1},...,t_{m})$ such that
$\B Y = \lambda + \bs\mu(\B t) + \bs\epsilon(\B t)$
such that $\lambda\sim F$, $\bs\mu(\B t)$ is a vectorized deterministic function of time, and $\bs\epsilon(\B t)\sim(0,\bs\Sigma(\B t)$. We let $\B\Sigma(\B t) =\B V^{1/2}\B R(\rho)\B V^{1/2}$ where $\B R(\rho)$ is an $m\times m$ correlation matrix based on the parameter $\rho$ and potentially the associated observation times, $\B t$, and $\B V$ is a $m\times m$ matrix with variance parameters along the diagonal. If we linearly transform the data to subtract the mean of the elements according to the symmetric matrix $A$ previously defined, the covariance of the resulting random vector is
\begin{align*}
Cov(\B A\B Y) &= Cov(\B A\bs\mu(\B t)+\bs\epsilon(\B t)))\\
&= \B A^{T}Cov(\bs\mu(\B t)+\bs\epsilon(\B t))\B A
\end{align*}
by the properties of covariance. The random intercept, $\lambda$, disappears so we do not need to worry about its distribution. For the moment, let us assume that the observation times, $\B t$, are fixed. Then
\begin{align*}
Cov(\B A\B Y)&= \B A^{T}Cov(\bs\epsilon(\B t))\B A\\
&= \B A^{T}\B\Sigma \B A
\end{align*}
If the variance is constant over time, $\B V=\sigma^{2}I$, and the elements of the vector are independent, $\B R(\rho)=I$, then
\begin{align*} 
Cov(\B A\B Y) &= \sigma^{2}\B A\B A^{T} \\
&=\sigma^{2}\B A\\
&= \sigma^{2}(I - m^{-1}\B1_{m}\B1_{m}^{T})\\
&=\sigma^{2}\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)I)
\end{align*}
 where $a=\frac{-1}{m-1}$ since $\B A$ is an idempotent matrix. Therefore, if the data has independent errors, by subtracting the estimated mean, negative exchangeable correction is induced between the observations of magnitude $\frac{-1}{m-1}$. Additionally, the variance decreases to $\sigma^{2}\frac{m-1}{m}$. If $m$ is large, the resulting correlation structure is approximately independence with variance $\sigma^{2}$.\\
 
 If the errors in the original data have constant variance, $\B V=\sigma^{2}I$, and are exchangeable with $\B R(\rho) = \rho\B 1_{m} \B 1_{m} + (1-\rho) I$, then 
 \begin{align*}
 Cov(\B A\B Y) &= \sigma^{2}\B A\B R(\rho)\B A^{T}\\
 &= \sigma^{2}(I-m^{-1}\B1_{m}\B1_{m}^{T})(\rho\B1_{m}\B1_{m}^{T}+(1-\rho)I)(I-m^{-1}\B1_{m}\B1_{m}^{T})^{T}\\
 &= \sigma^{2}(1-\rho)(I-m^{-1}\B1_{m}\B1_{m}^{T})\\
 &=\sigma^{2}(1-\rho)\left(\frac{m-1}{m}\right)(a\B 1_{m}\B 1_{m}^{T}+ (1-a)I)
 \end{align*} 
 where $a=\frac{-1}{m-1}$ as before. This transformation maintains its exchangeable structure but with negative correlation on the off diagonal and decreased variance. Therefore, if the dependence in the errors for the original vector is either independence or exchangeable, then the errors of the transformed vector has a covariance matrix with exchangeable but smaller variance and negative correlation of $\frac{-1}{m-1}$. This value is the lower boundary for an exchangeable structure while maintaining the covariance matrix non-negative definite. Again, if the number of observed data points is large, then the structure is approximately independent with variance $\sigma^{2}(1-\rho)$.\\
 
 On the other hand, if the original correlation is exponential such that $Cor(Y_{j},Y_{l}) = \exp(-|t_{j}-t_{l}|/\rho)$, the resulting covariance after transformation is no longer exponential. In fact, the covariance can no longer be written as a function of time lags. The covariance matrix is a linear combination of the original correlation matrix, column and row means, and the overall mean correlation:
   \begin{align*}
 Cov(\B A\B Y) &= \sigma^{2}\left[\B R(\alpha)-m^{-1}\B1_{m}\B1_{m}^{T}\B R(\alpha)-m^{-1}\B R(\alpha)\B1_{m}\B1_{m}^{T} + m^{-2}\B1_{m}\B1_{m}^{T}\B R(\alpha)\B1_{m}\B1_{m}^{T}\right]\\
  &= \sigma^{2}\left[\B R(\alpha)-\text{ column mean vector }-\text{ row mean vector } + \text{ overall mean}\right].
 \end{align*} 
 This results in a non-stationary covariance matrix since the correlation is no longer just a function of the difference in observation times. Besides inducing non-stationarity in terms of the correlation, the transformation also results in negative correlations when the mean of the correlations within each column and within each row are substantial. For example, if $\sigma^{2}=1$, $\alpha = 2$ and $\B t=(1,2,3,4)$, then the symmetric covariance matrix of the transformed vector is
$$ Cov(\B A\B Y) = \left[ \begin{array}{cccc}
 0.499&  0.009& -0.229& -0.278\\
  0.009&  0.307& -0.087& -0.229\\
 -0.229& -0.087&  0.307&  0.009\\
 -0.278& -0.229&  0.009&  0.499
\end{array}\right]$$
The variance and covariance changes over time with the correlation becoming negative at the time lag increases. There are no functions to summarize the covariance due to the non-stationarity and negative correlation. If the number of observation times increase such that the observation period increases, the covariance of the transformed vector will be close to the covariance as column, row, and overall means will decrease to zero as the number of large time lags increases. However, if the observation period remains fixed as the number of observations increases, the covariance after transformation continues to be non-stationary and have negative correlations.  If the dependence decays quickly relative to the time lags, an exchangeable model that allows negative correlation seems to be an adequate model as the covariance values seem to stabilize. \\ 

We have calculated the covariance of the transformed random vector when we know the original covariance. In practice, we need to estimate the covariance based on observed data vectors of length $m$. We showed that if prior to transformation, the errors are independent or exchangeable, the correlation of the resulting transformed data is exchangeable equal to $\frac{-1}{m-1}$. This is the lower bound for exchangeable correlation so that the covariance is non-negative definite. This means that the true parameter value is on the boundary of the parameter space. Regularity conditions for maximum likelihood are not met; consistency and asymptotic normality no longer hold. So even though we know the true structure is exchangeable, estimating a model with exchangeable structure is difficult. Since the value of the correlation is so small, it may be best to assume conditional independence in this situation. \\
  
Up until now, we have assumed that the observation times are fixed. However, in most cases, individuals are not observed at exactly the same times but rather at random times. We assume the outcome at observation time $t$ is equal to the shape function evaluated at time $t$ plus a random intercept and potentially dependent errors. When the times are random, the function is evaluated at different times resulting in a random average values for the shape function. Therefore, the transformed vector will not only have variability due to the errors but also an induced random intercept from the average function value. For example, imagine if the outcome had no error such that
$\B Y =  \bs\mu(\B t). $
If we evaluate the function at different sampling times, we induce a random intercept by subtracting the means, the variance of which depends not only on the variability in the random times but also the shape of the function. When the original covariance of the error is independent or exchangeable, the induced random intercept is independent of the transformed errors; therefore, the covariance of the transformed outcomes is the sum of the two resulting in an exchangeable structure.\\

On the other hand, if the original covariance is dependent on the observation times such as the exponential structure, the random intercept and error structure have a complex relationship both being indirectly dependent on the times of observation. We explore the impact of transforming the data through empirical simulations. We assume that observation times are equal to random perturbations around specified goal times such that $\B t = \B T + \B \tau$ where $\tau\sim N(0,\sigma^{2}_{tau})$ and $\B T = (1,2,...,9,10)$. We generate $n=200$ data realizations such that
$$\B y_{i} = \mu(\B t_{i}) + \epsilon_{i}\quad\text{ where }\epsilon_{i}\sim N(0,1)$$
for $i=1,...,n$ with different functions, $\mu(t)$, and variances of $\tau$. Figure \ref{fig:cov} shows the estimated autocorrelation functions under different conditions for observations times and shape functions.
INSERT FIGURE\\
COMMENT ON THE FIGURE\\
In practice, if the data are regularly sampled, negative correlations are problematic since in order for covariance matrices to be non-negative definite, off-diagonal terms WORKING HERE. However, if the data are irregularly sampled, using an additive model of a random intercept plus exponential correlation structure  \cite{diggle2002} may provide the flexibility to model both components of variation. 

\section{Simulation}
To compare the performance of the proposed method with competing methods, we completed a simulation study with three trajectory shapes and three levels for individuals to follow over time. To induce a relationship between level and shape, we restricted some shapes to particular levels. The five mean functions for generating the data with shapes at different levels are below:   \begin{align*}
g_{1}(t) &= -1 - t\\
g_{2}(t) &= 11 - t\\
g_{3}(t) &= 0\\
g_{4}(t) &= -11 + t\\
g_{5}(t) &= 1 + t
\end{align*} 
With the mean functions set, we specify the mechanism with which to determine an individual's shape and level group and thus their mean function. We introduce two binary baseline factors to determine function group membership, the first of which impacts the shape and the other which impacts the level. Individuals are randomly assigned values of these two binary factors, $z_{1}$ and $z_{2}$, with the independent simulated tosses of a fair coin such that $P(z_{1}=1) = P(z_{1}=0) = 0.50$ and $P(z_{2}=1) = P(z_{2}=0)=0.50$. \\

Let $S$ be a categorical random variable that indicates the shape group. If $S=1$, then the individual is in the negative slope group. $S=2$ refers to the horizontal group and $S=3$ indicates the positive slope group. Then conditional on the baseline factors, the probability of being in a shape group equals
$$P(S=k |z_{1}) = \frac{\exp(\gamma_{0k}+\gamma_{1k}z_{1})}{\sum^{3}_{l=1} \exp(\gamma_{0l}+\gamma_{1l}z_{1})}$$
for $k=1,2,3$ where $\gamma_{01}=2,\gamma_{11} = -4,\gamma_{02}=1.5,\gamma_{12}=-2,\gamma_{03}=\gamma_{13} = 0$ and $z_{1}\in\{0,1\}$. Therefore, $P(S=2|z_{1}=1) = P(S=2|z_{1}=0) = 0.34$. The chances of being in the positive and negative slope group do depend on $z_{1}$ with $P(S=1|z_{1}=1) = 0.58 = P(S=3|z_{1}=0)$ and $P(S=1|z_{1}=0) = 0.08 = P(S=3|z_{1}=1)$. Since the value of $z_{1}$ is determined by a coin toss, on average, each shape group has about an equal  frequency. \\

The second factor impacts the level, but I placed restrictions when creating the mean functions; therefore, level and shape are not independent of each other. Let $L$ be a categorial random variable that indicates level group. If $L=1$, an individual is in the low group. $L=2$ indicates middle and $L=3$, high. Conditional on the shape group, all of the horizontal and only the horizontal lines can be in the middle level despite the value of the second factor:
\begin{align*}
P(L=2|S=2, z_{2}) & = 1\\
P(L=2|S=1 \text{ or } S=3, z_{2}) & = 0
\end{align*}
The second factor does impact the level for those in either the negative or positive slope groups with
\begin{align*}
P(L=k|S=1 \text{ or } S=3,z_{2}) & = \frac{\exp(\zeta_{0k}+\zeta_{1k} z_{2})}{\sum_{l\in\{1,3\} }\exp(\zeta_{0l}+\zeta_{1l}z_{2})}\\
P(L=k|S=2,z_{2}) & =0
\end{align*}
for $k=1,3$ and $z_{2}\in\{0,1\}$ where $\eta_{01}=0,\zeta_{11}=0,\zeta_{03}=-3,\zeta_{13}=6$. Therefore, $P(L=1|S=1 \text{ or }S=3,z_{2}=0) = 0.95 = P(L=3|S=1 \text{ or }S=3,z_{2}=1)$ and $P(L=1|S=1 \text{ or }S=3,z_{2}=1) = 0.05 = P(L=3|S=1 \text{ or }S=3,z_{2}=0)$.  Due to the contraction of the shape groups and the baseline factors, this results in about equal frequencies within each level on average.\\

To summarize, individual trajectories are generated by first simulating two coin tosses to determine values for the two baseline binary factors. Then conditional on the factors, shape and level groups are randomly assigned by plugging the factors into the generalized logit functions above and drawing from a multinomial distribution. At this point, the mean function is set for a particular individual since the groups map to the functions
\begin{align*}
S = 1,\; L=1 \rightarrow g_{1}\\
S = 1,\; L=3 \rightarrow g_{2}\\
S = 2,\; L=2 \rightarrow g_{3}\\
S = 3,\; L=1 \rightarrow g_{4}\\
S = 3,\; L=3 \rightarrow g_{5}
\end{align*}

In order to generate data for an individual, I evaluate the mean function at five equidistant observation times $t=1,3.25,5.5,7.75,10$ that span the period 1 to 10 units. Then,  random noise is added to the mean values to create variability. The random noise is made up of two components: individual-specific level perturbation and time-specific Gaussian measurement error. For individual $i$ ($i=1,...,n$) at the $j$th observation time ($j=1,..,5$) with the $l$th mean function, the observed outcome equals
$$y_{ij} = g_{l}(t_{j})+\lambda_{i}+\epsilon_{ij}\quad\text{where}\quad \epsilon_{ij}\sim N(0,\sigma_{\epsilon}^{2}), \lambda_{i}\sim N(0,\sigma_{\lambda}^{2})$$
where $\sigma_{\epsilon}$  is the standard deviation of the measurement error and $\sigma_{\lambda}$ is the standard deviation of the level perturbation. To impact the overlap of groups, I choose the standard deviation of the perturbation by dividing the magnitude of the vertical shift so that there are 4 or 6 standard deviations between the mean functions. This results in standard deviations of 2 and 3. Lastly the size of measurement error influences the signal to noise ratio and I use extreme standard deviations of 0.5 and 2. There are four possible combinations of these two properties, representing the four  conditions of the data-generating process I use in the simulation study. \\

For each condition, we generate a data set of $n=500$ individuals using the process described above apply four clustering methods: independence Gaussian mixture model, K-means on the quotient differences, partitioning around medoids with a correlation-based dissimilarity measure, and vertically shifting mixture models with exchangeable correlation. For each method, we calculate the optimal $K$ using the silhouette measure \cite{kaufman}  for the partition methods or the BIC \cite{schartz} for the models and the misclassification rate when $K=3$ to detect whether the method discovered the underlying shape structure. This is then repeated $B=500$ times such that we get 500 unique data sets under each condition on which we can apply each methods and summarize the results. \\

To calculate the misclassification rate, I let $K=3$ and compare the cluster memberships labels to the true shape group membership using a contingency table. To measure how well the method aligned with the truth, I first reordered the cluster label columns of the contingency table such that the trace of the 3 by 3 inner matrix is maximized. Then the number of individuals on the off-diagonal of the newly permuted table represents the number of misclassified individuals. The best possible performance for a method would result in zero misclassifications. To summarize the replications, we present the mean misclassification rate across replications.\\

Table \ref{tab:freq1} summarizes the simulations in terms of the number of groups and average misclassification rate It is clear from these tables that the Gaussian mixture model assuming independence---do not select three groups as the optimal number of groups. The BIC with the independent mixture model consistently chooses five groups, the maximum we allow in the simulation, under all conditions. This method does not perform well when forced to choose $K=3$. Only about 50\% of the data is correctly specified in terms of the generating shape groups.\\

Of the established methods that are intended to group on shape, K-means on quotient differences can only pick the correct number of groups if the magnitude of the measurement error is small (Table \ref{tab:freq1}. If the variability around the individual mean is large, the method chooses 2 groups and misclassifies about 38\% of the individuals when forced to have 3 groups. Using the correlation dissimilarity measure with the PAM algorithm gives slightly better results with only 24-28\% misclassification, but the it does not consistently choose three groups. The chosen number is quite variable with 2 and 5 groups being chosen the most.\\

Lastly, the method that prevailed amongst the competition is the vertically shifted mixture model. For every condition, the method chose three groups as the optimal partition and when forced to $K=3$, the method discovered the shape groups with little misclassification. Only when the measurement error is large ($\sigma_{epsilon}=2$) did the method misclassify 5\% (about 25 individuals) in terms of shape. \\ 
\section{Conclusion}
 WORKS BETTER THAN THE ALTERNATIVES\\
 PROBLEMATIC WITH COVARIANCE\\
 ROBUST PROCEDURE EASY TO IMPLEMENT



\end{document}