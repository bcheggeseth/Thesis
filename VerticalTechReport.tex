\documentclass[12pt]{article}
\usepackage{fullpage,amsmath,amsfonts,graphicx,amsthm}
\usepackage[doi=true,url=false,eprint=false,isbn=false,firstinits=true]{biblatex}
\usepackage{setspace}
\title{Vertically Shifted Mixture Models \\ Clustering longitudinal data by shape}
\author{Brianna C. Heggeseth}
\bibliography{Dissertation}
\newtheorem{theorem}{Theorem}
  \newcommand{\B}[0]{\mathbf}
    \newcommand{\bs}[0]{\boldsymbol}


\begin{document}
%\doublespace
\maketitle

A key advantage of a longitudinal study is its ability to measure individual change over time and to distinguish between ``aging'' affects and cohort variability. Typical longitudinal data analysis often involves modeling the conditional mean outcome as a function of time and explanatory variables while taking the inherent time dependence into account. This type of analysis provides a useful summary of relationships present within observed groups defined by categorical variables such as race/ethnicity or sex; however, if there are unmeasured subgroups in which the trajectories differ, averaging over the data can mask interesting and useful patterns. In these circumstances, it is more informative to partition the subjects into data-driven groups prior to estimating the associations.

Cluster analysis methods have been adapted and developed specifically for approaching longitudinal data in this way, focusing on estimating longitudinal trajectories in genomics and physical, behavioral, and cognitive development \cite{schneiderman1993,genolini2010, jones2001, muthen2010, mcnicholas2010}. Most of these methods involve fitting a finite mixture model to the outcome measurements. Other techniques cluster subjects via a partitioning method such as K-means \cite{macqueen1967,hartigan1979} or partitioning around mediods (PAM) \cite{kaufman1990} using on a dissimilarity measure developed specifically to take the time-ordered structure of longitudinal data into account. There are many applications of such methods. 

These clustering methods are now being applied in other fields such as clinical medicine, public health, education, and economics. For example, researchers try to discover distinct childhood growth patterns by clustering body mass index measurements over time with a view to determining which early life factors may contribute to following that pattern \cite{pryor2011,carter2012}. The goal is to group children who grow similarly over time; however, none of the popular methods explicitly group subjects based on the shape of the pattern while ignoring the level over time. Many authors have either been deceived or simply misunderstand this fact and interpret their clustering results as if the data were grouped by shape \cite{pryor2011,carter2012,nagin1999}. 

There has been little work and discussion on the best methods to utilize when shape, as distinct from level, is the feature of interest. The few proposals involve partitioning methods using dissimilarity measures based either on the derivative function or the Pearson correlation coefficient. 

With longitudinal data, we do not directly observe the derivative function for each individual. Therefore, \textcite{moller2003} and \textcite{d2000} independently suggested estimating the derivative function via the difference quotient, calculating the slope of a linear interpolation between adjacent repeated measures. The dissimilarity between two individuals is measured as the derivative estimates. As a result, individuals are compared on the shape of their underlying trend over time ignoring the original level of the data. 

We assume that the observed data is a realization of the model
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma_{i}^{2})$ and $f_{i}$ is a differentiable, continuous function. This clustering method essentially requires balanced data so the observations are fixed across individuals, $t_{ij} = t_{j}$ for $j=1,..,m$ and $i=1,...,n$. For longitudinal data, $m$ is relatively small. This ensures the estimated slopes are comparable. The forward difference quotient equals 
$$\hat{f}_{i}^{'}(t_{j}) = (y_i(t_{j+1})-y_i(t_j))/(t_{j+1}-t_j).$$
It is an unbiased estimate of $f_{i}^{'}(\tau)$ for $\tau\in[t_{j},t_{j+1}]$ with
$$E(\hat{f}_{i}^{'}(t_{j})) = f_i(t_{j+1})-f_i(t_j))/(t_{j+1}-t_j)$$
by the mean value theorem but is highly variable if $\sigma_{i}^{2}$ is large since
$$\text{Var}(\hat{f}_{i}^{'}(\tau)) =  2\sigma^{2}_{i}/ (t_{j+1}-t_j)^{2}.$$
This variability can impact the final clusters if enough estimates have the opposite sign of the true derivative. Now, one way to minimize the variance is to maximize the time between observations. For example, only two observations, one at baseline and another at the end of follow up, minimizes the variance but at the expense of observing the rate of change during follow up. If time of observations are densely sampled, a functional approach is to smooth out the noise using splines to estimate the function and then the derivative function. In either circumstance, the derivatives are independently estimated for each individual and there is no direct way to borrow strength between individuals to better estimate the derivative even if subjects are thought to have a common shape. 

The Pearson correlation coefficient has been used to measure dissimilarity between two vectors based on the shape of the data  \cite{chouakria2007,  eisen1998, chiou2008}. In the context of functional clustering, \textcite{chiou2008} suggested using the functional correlation as a similarity measure to cluster similar functions. This clustering approach requires densely collected longitudinal data and therefore, it is not applicable in our circumstance since the number of observations is relatively small, say less than 15. Despite the wide use, there has been little to no discussion about how well the measure does to discriminate between shapes.

In the multivariate setting, a dissimilarity measure based on the Pearson correlation coefficient between two vectors equals 
$$d_{Cor}(\B x,\B y) = 1-Cor(\B x,\B y)$$ 
where $$Cor(\B x,\B y) = \frac{\sum^{m}_{j=1}(x_{j}-\bar{x})(y_{j}-\bar{y})}{\sqrt{\sum^{m}_{j=1}(x_{j}-\bar{x})^{2}}\sqrt{\sum^{m}_{j=1}(y_{j}-\bar{y})^{2}}}.$$
We assume the data vectors are generated from the model presented above. Let $\epsilon_{ij}=0$ for all $i$ and $j$. Then a vector with an underlying functional shape of $f(x)$ is perfectly positively correlated with a vector with function shape $af(x) + b$ where $a\geq0$ and $b\in\mathbb{R}$. As a result, two vectors that are vertical shifts of each other have a dissimilarity of zero since $Cor(\B x,\B y) = 1$, but two vectors that are multiples of each other with drastically different shapes are placed in the same cluster as well. While this method succeeds to group individuals with the same shape at different levels, it fails in the sense that is also groups individuals with different shapes. If the data is observed with error such that $\epsilon_{ij}\sim(0,\sigma_{i}^{2},$ the dissimilarities are be larger when the noise is large relative to the functional shape. 

Now, if two vectors are constant such that $f(x)=c$ where $c\in\mathbb{R}$,  the correlation coefficient is undefined. When this type of data is observed with noise, the correlation and thus the dissimilarity will be quite variable when the number of observations is small. In other words, the Pearson correlation coefficient cannot consistently detect that two horizontal curves have the same shape. This methods fails to group by shape when $\sigma_{i}^{2}$ is large, number of observations is low, and the data set includes horizontal patterns over time. These characteristics do not get discussed in the literature, but they have huge ramifications for clustering longitudinal data.

Both of these proposals implicitly remove the level while clustering, but they fail to detect shape when data is observed with high noise. Additionally, the dissimilarity measures based on comparing vectors are not conducive to use on irregularly sampled longitudinal data with inconsistent observation times. Also, the partitioning methods do not provide a way to estimate the impact of baseline factors on group membership while taking into account membership uncertainty.  

In this paper, we propose vertically shifting each subject by subtracting its mean before fitting a multivariate mixture model in order to cluster longitudinal data by the shape over time. This method provides a probability framework and works with irregularly sampled longitudinal data. In the first section, we discuss related work and then we present the model specification and provide details for the mean and covariance structure in Section 2. Implementation including parameter estimation is discussed in Section 3. A simulation study demonstrates the merits of the proposed method in comparison to standard clustering methods and those that explicitly attempt to group based on shape. Lastly, we discuss potential challenges when modeling the transformed data.

\section{Related Work}
A finite mixture model is a standard method for clustering multivariate data \cite{everitt2009} and has been used for longitudinal applications \cite{muthen2010, jones2001}. See \cite{mclachlan2000} for an extensive summary of finite mixture models. However, for longitudinal data, the models are commonly used for the observed data without much regard to the goal of clustering by shape. 

We suggest removing the level by subtracting out the mean outcome level prior to modeling. Subtracting the mean is not a novel idea in statistics or even cluster analysis. In fact, experimental data such as gene expression microarrays are often normalized to compensate for variability in the measurement device between samples. In cluster analysis of multivariate data, it is recommended that each variable is standardized by subtracting the mean of the variable measures and dividing by the standard deviation so that each standardized variable is in comparable units and equally contributes to the grouping process. This is not recommended for the longitudinal setting where each variable is a repeated measurement at a different time point. To compare shapes, we want to maintain the original scale since the relationship between measurements within individuals is of interest. Therefore, any transformation performed should only be additive in nature to preserve the original shape of the data over time. In general, pre-processing the data can provide the path to answering the research question but any transformation of the data should be carefully studied for potential unintended consequences.

A version of this idea has been implemented in the functional data analysis literature. For processes in a Hilbert space of square integrable functions with respect to the Lebesgue measure, $dt$, on the interval $\mathcal{T}=[0,T]$, \textcite{chiou2008} propose using a mixture model and the Karhunen-Lo{\`e}ve expansion for centered stochastic processes within their correlation-based clustering algorithm. The integral of the random function over interval $\mathcal{T}$ divided by $T$, the length of the interval, is subtracted to center the process; the resulting process integrates to zero. The integral of the process is the functional analogue to a mean vector in vector space; similarly, the resulting vector has mean zero after subtraction.

 Although centering a process and a shifting a vector stems from the same idea, there are distinct consequences of subtracting the estimated level of a noisy curve observed at a finite number of points that do not arise when centering a smooth function. The term centering is used in the stochastic processes literature, but we use the term vertically shifting to refer to the procedure of subtracting the mean since it graphically describes the transformation of the noisy longitudinal data.

\section{Model Specification}
 Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote an outcome vector of repeated observations for individual $i$, $i=1,...,n$. The vector of corresponding times of observation for individual $i$ is denoted as $\B t_{i}=(t_{i1},...,t_{im_{i}})$ and $\B w_{i}$ is a $q$-length design vector based on time-fixed factors that are typically collected at or before time $t_{i1}$. We assume that there are $K$ mean shape functions, $\mu_{k}(t)$, in the population such that the outcome vector for individual $i$ in shape group $k$ is
 $$\B y_{i} = \lambda_{i}\B 1_{m_{i}}+\bs\mu_{i}+\bs\epsilon_{i},\quad \lambda_{i}\sim F, \quad \bs\epsilon_{i}\sim N(0,\bs\Sigma_{k})$$
 where $F$ is a probability distribution, $\B 1_{m_{i}}$ is a m-length vector of 1's, and $\mu_{ij} = \mu_{k}(t_{ij})$ is the $j$th element of a $m_{i}$-length vector of mean values evaluated at the observation times, $\B t_{i}$. Therefore, this outcome vector is determined by a mean shape, a random intercept, and potentially correlated errors. The probability of being in a particular shape group could depend on baseline covariates $\B w_{i}$. Let $\bar{y}_{i}= m_{i}^{-1}\sum^{m_{i}}_{j=1} y_{ij} = \lambda_{i}+\bar{\mu}_{i}+\bar{\epsilon}_{i}$ be the mean of the outcomes for individual $i$. This is one measure of the vertical level of the data vector that we can remove by applying a linear transform, $\B A_{i} = \B I_{m_{i}} - m_{i}^{-1}\B 1_{m_{i}}\B 1_{m_{i}}^{T}$, to the vector of observations. The vertically shifted vector equals 
\begin{align*}
\B y^{*}_{i} &= \B A_{i}\B y_{i}\\
&=\B A_{i}(\lambda_{i}\B 1_{m_{i}}+\bs\mu_{ik}+\bs\epsilon_{i})\\
&=\B A_{i}(\bs\mu_{i}+\bs\epsilon_{i})\\
&=\bs\mu_{ik} - \bar{\mu}_{i}+\epsilon_{i}-\bar{\epsilon}_{i}.
\end{align*}
Applying the symmetric matrix $\B A_{i}$ to the vector $\B y_{i}$ subtracts the individual mean, $\bar{y}_{i}$, from each element $\B y_{i}$. This results in the removal of the random intercept, $\lambda_{i}$, leaving the mean function and error shifted by a random constant. Hence, we do not have to worry about the distribution of the random intercept, $\lambda_{i}$.

Once the level is removed, we assume the vertically shifted data, $\B y_{i}^{*}$, follow a Gaussian mixture of $K$ groups with mean shape functions and random noise. If the observation times are fixed, vertically shifted data generated from the model above would exactly follow this Gaussian mixture. We assume that, conditional on observation times $\B t$ and baseline covariates $\B w$, $\B y^{*}$ is a realization from a finite mixture model with density
\begin{align*}
 f(\B y^{*}|\B t,\B w,\bs\theta) =  \sum^{K}_{k=1}\pi_{k}f_{k}( \B y^{*}|\B t,\bs\theta_{k})
\end{align*}
where $\pi_{k}$ is the prior probability of being in the $k$th component ($\pi_{k}>0$ for $k=1,...,K$ and $\sum^{K}_{k=1}\pi_{k}=1$) and $\bs\theta = (\bs\gamma,\bs\theta_{1},...,\bs\theta_{K})$. To allow baseline covariates to impact the probability of having a certain shape pattern over time, we parameterize the prior probabilities using the generalized logit function of the form
$$\pi_{k}=\frac{\exp(\B w^{T}\bs\gamma_{k})}{\sum_{j=1}^{K}\exp(\B w^{T}\bs\gamma_{j})}$$ 
for $k=1,...,K$ where $\bs \gamma_{k}\in\mathbb{R}^{q}$, $\bs\gamma = (\bs\gamma_{1},...,\bs\gamma_{K})$, and we fix $\bs\gamma_{K}=\B 0$. We assume the component densities $f_{k}(\B y^{*}|\B t,\bs\theta_{k})$ are multivariate Gaussian densities with mean and covariance functions of time.

\subsection{Mean Structure}
The mean shape is modeled as a smooth function of time represented using a chosen functional basis. If the shape is periodic in nature, a Fourier basis is appropriate. Another common basis is a lower order polynomial basis such as $\{1, t, t^{2}\}$. However, this basis cannot capture complex shapes with drastic changes. 

To allow for local changes, we break the time interval, $[a,b]$, up into smaller interval using $L$ knots, $a<\tau_{1}<\cdots<\tau_{L}<b$, and fit polynomials of order $p$ in each subinterval. This piecewise polynomial can be expressed as a linear combination of truncated power functions and polynomials of order $p$. In other words,
$\{1,t,t^{2},...,t^{p-1},(t-\tau_{1})_{+}^{p-1},...,(t-\tau_{L})_{+}^{p-1}\}$
is a basis for a piecewise polynomial with knots at $\tau_{1},...,\tau_{L}$. However, the normal equations associated with the truncated power basis are highly ill-conditioned. 

A better conditioned basis for the same function space is the B-spline basis \cite{deboor1978, schumaker1981,curry1966, de1976}. A B-spline function of order $p$ with $L$ internal knots, $\tau_{1},...,\tau_{L}$, is defined by a linear combination of coefficients and B-spline basis functions
$$\mu(t) = \sum^{L+p}_{j=1} \beta_j B_{j,p}(t)$$
where the basis functions, $B_{j,p}(t)$, are defined iteratively \cite{deboor1972,cox1972}. We construct a matrix  of values from the $p$th order B-spline basis functions taken at observation times $\B t_{i}$ to model the mean values. We assume the mean pattern for the $k$th shape cluster is approximated by the function $\mu_{k}(t)=\sum^{L+p}_{j=1} \beta_{k,j} B_{j,p}(t) = \B x_{i}\bs\beta_{k}$ where $\bs\beta_{k}=(\beta_{k,1},...,\beta_{k, L+p}).$  

\subsection{Covariance Structure}
Various assumptions can be made about the covariance matrix. Here, we allow the covariance to differ between clusters. Since it is common for longitudinal data to have sparse, irregular time sampling, we must impose some structure on the covariance matrix to allow for parameter estimation as described by \textcite{jennrich1986} in their seminal paper. A common parameterization is conditional independence with constant variance where $\B \Sigma^{*}_{k}= \sigma_{k}^{2}I_{m_{i}}$. This is an unrealistic assumption for longitudinal data since there is inherent dependence between repeated measures on the same unit. 

Compound symmetry, which is also known as exchangeable correlation, is a popular correlation structure in longitudinal analysis where all repeated measures are equally correlated. This is typically paired with constant variance such that $\B \Sigma^{*}_{k} = \sigma_{k}^{2}(\rho_{k}\B1_{m_{i}}\B1_{m_{i}}^{T}+(1-\rho_{k})I_{m_{i}})$ where $-1\leq\rho_{k}\leq 1$ is the correlation between any two distinct measurements within an individual. This dependence structure describes the resulting correlation matrix of a random intercept model.

Another structure that provides a compromise between the two is the exponential correlation structure in which the dependence decays as the time between observations increases such that $[\B \Sigma^{*}_{k}]_{jl} = \sigma_{k}^{2}\exp(-| t_{ij}-t_{il}| / r_{k})$ where $r_{k}> 0$ is the range of the dependence. If the range, $r_{k}$, is small, the correlation decays quickly, but if the $r_{k}$ is large, there is long range dependence between measurements within an individual. This structure is similar to the correlation matrix generated from a continuous autoregressive model of order one such that $[\B\Sigma^{*}_{k}]_{jl} = \sigma^{2}\rho_{k}^{|t_{ij}-t_{ill}|}$ where $\rho_{k}$ is the correlation for measurements observed one unit of time apart. If we set $\rho_{k} = \exp(-1/r_{k})$, then the two parameterization result in the same structure if the correlation between two measures is constrained to be positive. This is a reasonable assumption for longitudinal data in the original form but it many not be acceptable for the transformed data as we discuss later.

The previous two structures can be combined by assuming a random intercept model plus serial correlation described by the exponential structure \cite{diggle2002}. Then the covariance matrix equals the variance of the random intercept plus the covariance of the serial correlation such that $[\B \Sigma^{*}_{k}]_{jl}=\nu^{2}_{k}\B 1_{m_{i}}\B 1^{T}_{m_{i}}+\sigma_{k}^{2}\exp(-| t_{ij}-t_{il}| / r_{k})$.\\

All of the covariance structures mentioned above are associated with weakly stationary processes with constant variance and correlation dependent only on the time lag between observations. If the variance or correlation function is non-constant but varying continuously, it could be potentially modeled as a function, but estimation is more difficult.

It is important to model the covariance structure correctly as misspecification can highly impact mixture model results in terms of parameter estimates and the final clustering if the groups are not well-separated \cite{heggeseth2013}. Transforming the data brings individuals with similar shapes closer but also brings others closer as well. In general, this decreases the separation between groups, which may force us to accurately model the correlation. 

\section{Implementation}
Given a collection of independent observed outcome vectors, $\B y_{1},...,\B y_{n}$. The first step is to calculate the mean for each subject, $\bar{y}_{i}$, $i=1,...,n$ and subtract the subject-specific mean from the observed outcome vector. This transformation leaves vertically shifted independent vectors, $\B y^{*}_{1},...,\B y^{*}_{n}$.  

Then, we must choose the order of the spline and the number and location of internal knots for the mean structure. The B-spline basis should be kept constant for all shape groups so the simplest way to select the number of knots is through visual inspection of the full data set. If the most complex shape patterns is a lower order polynomial, no internal knots are necessary. However, if the most complex curve has localized activity, adding knots and increasing the order of the spline functions can flexibly accommodate the twists and turns of the mean patterns. In choosing both the order of the polynomials and the number of knots, it is important to balance the number of mean parameters with the sample size. Every unit increase in the order or in the number of knots increases the number of parameters by $K,$ the number of groups. In terms of location of the knots, one suggestion is to place knots at sample quantiles based on the sampling times of all the observations \cite{ruppert2002}. However, this strategy may not work well if the median time is not the point of deviation. If possible, it is best to place knots at local maxima, minima, and inflection points of the overall trends as to accommodate the differences from a polynomial function \cite{eubank1999}.  Once these are decided, the design matrixes $\B x_{i}$ are calculated using widely available B-spline algorithms. 

In order to fit the model and estimate the parameters, we use maximum likelihood estimation via the EM algorithm. Under the assumption that $\B y^{*}_{1},...,\B y^{*}_{n}$ are independent realizations from the mixture distribution, $f(\B y^{*} | \B t, \B w, \bs\theta)$, defined above, the log likelihood function for the parameter vector, $\bs \theta$, is given by
$$\log L(\bs\theta)=\sum^{n}_{i=1}\log f(\B y^{*}_{i}|\B t_{i},\B z_{i},\bs \theta).$$
The maximum likelihood estimate of $\bs\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\bs\theta)/\partial \bs\theta=\B 0.$ Solutions of this equation corresponding to local maxima can be found iteratively through the expectation-maximization (EM) algorithm \cite{dempster1977}. This algorithm is applied in the framework where given $(\B t_{i},\B w_{i})$ each $\B y^{*}_{i}$ is assumed to have stemmed from one of the components and the indicator denoting its originating component is missing. The complete-data log likelihood is based on these indicator variables as well as the observed data $\{(\B y^{*}_{i}, \B t_{i}, \B w_{i})\}$. The expectation step (E-step) involves replacing the indicators by current values of their conditional expectation, which is the posterior probability of component membership, written as
$$\alpha_{ik}=\pi_{k}(\B w_{i},\bs\gamma)f_{k}(\B y^{*}_{i}|\B t_{i},\bs\theta_{k})/\sum_{j=1}^{K}\pi_{j}(\B w_{i},\bs\gamma)f_{j}(\B y^{*}_{i}|\B t_{i},\bs \theta_{j})$$
for $i=1,...,n$ and $k=1,...,K$ using current estimates of the parameters. In the maximization step (M-step), the parameter estimates for the prior probabilities, linear mean, and covariance matrices are updated by maximizing the complete-data log likelihood using the posterior probabilities from the E-step in place of the indicator variables using numerical optimization. The E- and M-steps are alternated repeatedly until convergence. The EM algorithm guarantees convergence to a local maximum; global convergence may be attained through initializing the algorithm by randomly assigning individuals to initial components, running the algorithm multiple times and using the estimates associated with the highest log likelihood. Besides the parameter estimates, the algorithm returns the posterior probability estimates of component membership. These probabilities can be used to partition individuals into distinct clusters by selecting the cluster with the maximum posterior probability. However, unlike K-means, the posterior probability provides some measure of uncertainty in the hard clustering. 

The model requires the number of clusters, $K$, to be known. In practice, this is not the case and we must choose $K$. The most popular procedure is to chose a maximum value of $K$ such that $K_{max}<<n$, fit the model under all values of $K=2,...,K_{max}$, and choose the value that optimizes a chosen criteria. In this thesis, we use the Bayesian Information Criterion (BIC) \cite{schwarz1978} for choosing $K$. It is defined as
$$BIC = -2\log L(\hat{\bs\theta})- d\log(n)$$
where $d$ is the length of $\bs\theta$, the number of parameters in the mixture model, and $L(\bs\theta,K)$ is the log likelihood function for the parameter vector. The BIC has been widely used for model selection with mixture models since Roeder and Wasserman's use in 1997 \cite{roeder1997}. In particular, the criteria has been to select the number of clusters \cite{dasgupta1999,fraley1999} with good results in practice. For regular models, the BIC was derived as an approximation to twice the log integrated likelihood using the Laplace method \cite{tierney1986}, but the necessary regularity conditions do not hold for mixture models in general \cite{aitkin1985}. However, Roeder and Wasserman \cite{roeder1997} showed that the BIC leads to a consistent estimator of the mixture density, and Keribin \cite{keribin2000} showed that the BIC is consistent for choosing the number of components in a mixture model.

\section{Potential modeling challenges}
There are issues of identifiability with Gaussian mixture models that can be mitigated through some minor constraints \cite{mclachlan2000}. In this section, we discuss some unique consequences of vertically shifting the data on the model and inference.

\subsection{Covariance of transformed data vectors}
We let $\B Y_{i}=(Y_{i1},...,Y_{im})$ be a random vector observed at times $\B t_{i}=(t_{i1},...,t_{im})$ such that
$\B Y_{i} = \lambda_{i} + \bs\mu_{i} + \bs\epsilon_{i}$
such that $\lambda_{i}\sim F$, $\bs\mu_{i}$ is a vector based on a deterministic function of time, and $\bs\epsilon_{i}\sim(0,\bs\Sigma_{i})$. We let $\B\Sigma_{i} =\B V^{1/2}\B R_{i}(\rho)\B V^{1/2}$ where $\B R_{i}(\rho)$ is an $m\times m$ correlation matrix based on the parameter $\rho$ and potentially the associated observation times, and $\B V$ is a $m\times m$ matrix with variance parameters along the diagonal. If we linearly transform the data to subtract the mean of the elements according to the symmetric matrix $\B A = I_{m}-m^{-1}\B 1_{m}\B 1_{m}^{T}$, the covariance of the resulting random vector is
\begin{align*}
Cov(\B A\B Y_{i}) &= \B A^{T}Cov(\B Y_{i})\B A
\end{align*}
by the properties of covariance. From now on, $I_{m}$ will be written as $I$ and $\B 1_{m}$ as $\B 1$ for simplification.

\subsubsection{Fixed observation times}
For the moment, let us assume that the observation times are fixed, $\B t_{i}=\B t$. Then
\begin{align*}
Cov(\B A\B Y_{i})&= \B A^{T}Cov(\bs\epsilon_{i}\B A\\
&= \B A^{T}\B\Sigma_{i} \B A
\end{align*}
If the variance is constant over time, $\B V=\sigma^{2}I$, and the elements of the vector are independent, $\B R_{i}(\rho)=I$, then the covariance of the transformed vector is
\begin{align*} 
Cov(\B A\B Y_{i}) &= \sigma^{2}\B A\B A^{T} \\
&=\sigma^{2}\B A\\
&= \sigma^{2}(I - m^{-1}\B1\B1^{T})\\
&=\sigma^{2}\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)I)
\end{align*}
 where $a=\frac{-1}{m-1}$. Therefore, if the data has independent errors, subtracting the estimated mean induces negative exchangeable correction between the observations of magnitude $\frac{-1}{m-1}$. Additionally, the variance decreases to $\sigma^{2}\frac{m-1}{m}$. If $m$ is large, the resulting correlation structure is approximately independence with variance $\sigma^{2}$.
 
 If the errors in the original data have constant variance, $\B V=\sigma^{2}I$, and are exchangeable with $\B R_{i}(\rho) = \rho\B 1 \B 1^{T} + (1-\rho) I$, then the covariance of the transformed vector is
 \begin{align*}
 Cov(\B A\B Y_{i}) &= \sigma^{2}\B A\B R_{i}(\rho)\B A^{T}\\
 &= \sigma^{2}(I-m^{-1}\B1\B1^{T})(\rho\B1\B1^{T}+(1-\rho)I)(I-m^{-1}\B1\B1^{T})^{T}\\
 &= \sigma^{2}(1-\rho)(I-m^{-1}\B1\B1^{T})\\
 &=\sigma^{2}(1-\rho)\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)I)
 \end{align*} 
 where $a=\frac{-1}{m-1}$. This transformation maintains the exchangeable structure but with negative correlation on the off diagonal and decreased variance of $\sigma^{2}(1-\rho)\left(\frac{m-1}{m}\right)$.  Again, if the number of observed data points is large, then the structure is approximately independent with variance $\sigma^{2}(1-\rho)$.

 On the other hand, if the original correlation is exponential such that the correlation decreases as time lags increases, $Cor(Y_{ij},Y_{il}) = \exp(-|t_{ij}-t_{il}|/\rho)$, the resulting covariance after transformation is not a recognizable structure. In fact, the covariance can no longer be written as a function of time lags. The covariance matrix is a linear combination of the original correlation matrix, column and row means, and the overall mean correlation:
   \begin{align*}
 Cov(\B A\B Y_{i}) &= \sigma^{2}\left[\B R_{i}(\rho)-m^{-1}\B1\B1^{T}\B R_{i}(\rho)-m^{-1}\B R_{i}(\rho)\B1\B1^{T} + m^{-2}\B1\B1^{T}\B R_{i}(\rho)\B1\B1^{T}\right]\\
  &= \sigma^{2}\left[\B R_{i}(\rho)-\text{ column mean vector }-\text{ row mean vector } + \text{ overall mean}\right].
 \end{align*} 
 This non-stationary covariance matrix includes negative correlations when the mean of the correlations within each column and within each row are positive and substantial. For example, if $\sigma^{2}=1$, $\rho = 2$ and $\B t_{i}=(1,2,3,4)$, then the symmetric covariance matrix of the transformed vector is
$$ Cov(\B A\B Y_{i}) = \left[ \begin{array}{cccc}
 0.499&  0.009& -0.229& -0.278\\
  0.009&  0.307& -0.087& -0.229\\
 -0.229& -0.087&  0.307&  0.009\\
 -0.278& -0.229&  0.009&  0.499
\end{array}\right]$$
The variance and covariance changes over time with the correlation becoming negative as the time lag increases.  If the number of observation times increase such that the observation period increases as well, the covariance of the transformed vector will be close to the original covariance as column, row, and overall means will decrease to zero as the number of pairs of measurements with large time lags increases. However, if the observation period remains fixed as the number of observations increases, the covariance after transformation continues to be non-stationary and have negative correlations.  

We have calculated the covariance of the transformed random vector when we know the original covariance structure. In practice, we need to estimate the covariance based on observed data. We showed that if prior to transformation, the errors are independent or exchangeable, the correlation of the resulting transformed data is exchangeable equal to $\frac{-1}{m-1}$. This particular value has significant meaning as it is the lower bound for correlation in an exchangeable matrix. For $X_{1},...,X_{m}$ exchangeable random variables with $Var(X_{i})=\sigma^{2}$, we know that
\begin{align*}
0&\leq Var(X_{1}+\cdots +X_{m})\\
0&\leq m\sigma^{2}+m(m-1)Cov(X_{1},X_{2})\\
\frac{-\sigma^{2}}{m-1}&\leq Cov(X_{1},X_{2}).
\end{align*}
This means that the true parameter value of the correlation for the transformed vector is on the boundary of the parameter space; consistency and asymptotic normality no longer hold for maximum likelihood estimation. Additionally, the determinant of the covariance matrix for the transformed vector is zero thus the probability density function is not defined. Therefore, even though we know the true structure is exchangeable, estimating parameters for a model with exchangeable correlation structure is difficult. It may be best to assume conditional independence in this situation. 
 
\subsubsection{Random observation times} 
Up until now, we have assumed the observation times are fixed. However, in practice, individuals in a longitudinal study are not observed at exactly the same times but rather at random times. When the times are random, the deterministic function, $\mu(t)$, is evaluated at different times resulting in a random average values of function values. Therefore, the transformed vector will not only have variability due to the errors but also an induced random intercept from the average function value. 

To illustrate, imagine an error-free outcome vector such that the observations are twice the time of observation, $y_{ij}=2t_{ij}$. Let $t_{1}=(1,2,3)$ and $t_{2}=(1,2.5,4)$ be two sets of observation times. Then we have two realizations of the vector $y_{1}=(2,4,6)$ and $y_{2}=(2,5,8)$. The transformed vectors after removing the means equal $y^{*}_{1j}=2t_{1j}-4$ and  $y^{*}_{2j}=2t_{2j}-5$. Evaluating the function at different sets of observation times and subtracting the mean induces a random intercept, the variance of which depends not only on the variability in the random times but also the derivative of the function. For example, if the function has a steeper slope, $y_{it}=10t_{ij}$, then the transformed vectors would be $y^{*}_{1j} = 10t_{1j}-20$ and $y^{*}_{2j} = 10t_{1j}-25$. This variability needs to be incorporated into the covariance matrix of the model. 

When the original covariance of the error is independent or exchangeable, the induced random intercept is independent of the transformed errors; therefore, the covariance of the transformed outcomes continues to be exchangeable. Fortunately, the magnitude of the correlation should be far enough from the boundary for estimation to be possible if the function has a steep slope at times of irregular sampling.

If the original covariance is dependent on the observation times through an exponential function of time lags, the random intercept and error structure both indirectly depend on the times of observation. We explore the impact of transforming the data through empirical simulations. We assume that observation times are equal to random perturbations around specified goal times such that $\B t = \B T + \B \tau$ where $\tau\sim N(0,\sigma^{2}_{\tau})$ and $\B T = (1,2,...,9,10)$. We generate $n=300$ data realizations such that
$$\B Y_{i} = \bs\mu_{i} + \epsilon_{i}\quad\text{ where }\epsilon_{i}\sim N(0,\B R_{i}(\rho))$$
where $\mu_{ij}=\mu(t_{ij})$ and $\B R_{i}(\rho)$ is an exponential correlation matrix with $\rho=2$ under different assumptions for the mean function $\mu(t)$, and variance of the observation times through $\tau$. Figure \ref{fig:cov} shows the estimated autocorrelation functions under varying conditions for observations times and shape functions.
\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Cov}
\end{center}
\caption{Estimated autocorrelation functions from data generated with an exponential correlation error structure and perturbed observation times under different mean functions, $\mu(t)$, and variances of the perturbation.}
\label{fig:cov}
\end{figure}

As the variance of the perturbation in observation time and the magnitude of the derivative mean function increases, the estimated correlation at large lags becomes more positive. In general, the curves resemble a scaled and shifted version of the exponential function with an asymptote other than zero. If the asymptote is greater than zero,  an additive model combining a random intercept plus exponential covariance  \cite{diggle2002} can represent the scaled function..

In practice, if the data are regularly sampled, true negative correlations are problematic for estimation and an independence or exponential correlation structure may be the best option. If the data are irregularly sampled, an additive model the combines a random intercept with the exponential correlation may be appropriately flexible to represent the transformed covariance.

\subsubsection{Unbalanced observation times}
In addition to the issues of fixed versus random sampling, having an unequal number of observations per subject can impact the estimation of covariance of transformed vector. As we saw above, the length of the vector impacts the covariance of the transformed vector. Suppose we have a sample of individuals that have the same mean shape and covariance over time. However, we observe each subject a different number of times because they were unavailable for an interview or two. Transforming the vectors by subtracting means based on a variety of number of observations induces a different covariance structure for each individual. If there is quite a bit of variability in the number of observations, it may impact clustering to assume they share the same covariance structure during the estimation/clustering process. However, if the number of observation times is large for all subjects and the observation period is long, then the covariance matrices should be similar. 

Additionally, if the unbalanced nature of the data is due to lost to follow up during a longitudinal study, clustering based on the shape should be done with caution. If the general shape of the curve during the observation period is not measured adequately by the number of observations, it does not make sense to try and cluster those individuals with the rest who have fully observed curves. 

\section{Simulation}
To compare the performance of the proposed clustering method with competing methods, we completed a simulation study with three trajectory shapes and three levels in the population. We restricted some shapes to particular levels to induce a relationship between level and shape. The five mean functions for generating the data with shapes at different levels are below:   
\begin{align*}
g_{1}(t) &= -1 - t \quad\quad\text{ (negative slope, low level)}\\
g_{2}(t) &= 11 - t\quad\quad\text{ (negative slope, high level)}\\
g_{3}(t) &= 0\quad\quad\text{ (horizontal, middle level)}\\
g_{4}(t) &= -11 + t\quad\quad\text{ (positive slope, low level)}\\
g_{5}(t) &= 1 + t\quad\quad\text{ (positive slope, high level)}
\end{align*} 
Individuals follow these patterns with varying probabilities that depend on two factors. The first factor $w_{1}$ impacts the shape and $w_{2}$ impacts the level. Individuals are randomly assigned values of these two binary factors with the independent simulated tosses of a fair coin such that $P(w_{1}=1) = P(w_{1}=0) = 0.50$ and $P(w_{2}=1) = P(w_{2}=0)=0.50$. 

Let $S$ be a categorical random variable that indicates the shape/slope group. $S=1,2,3$ refers to the negative slope, horizontal, and positive slope groups, respectively. Conditional on the baseline factors, the probability of being in a shape group equals
$$P(S=k |w_{1}) = \frac{\exp(\gamma_{0k}+\gamma_{1k}w_{1})}{\sum^{3}_{l=1} \exp(\gamma_{0l}+\gamma_{1l}w_{1})}$$
for $k=1,2,3$ where $\gamma_{01}=2,\gamma_{11} = -4,\gamma_{02}=1.5,\gamma_{12}=-2,\gamma_{03}=\gamma_{13} = 0$ and $w_{1}\in\{0,1\}$. Since the value of $w_{1}$ is determined by a coin toss, each shape group has about an equal frequency, marginally. 

The second factor impacts the level, but I placed restrictions when creating the mean functions; therefore, level and shape are not independent of each other. Let $L$ be a categorial random variable that indicates level group. $L=1,2,3$ refers to the low, middle, and high group, respectively. Conditional on the shape group, all of the horizontal lines are in the middle level. For those in either the negative or positive slope groups, the chance of the high or low level equals
\begin{align*}
P(L=k|S=1 \text{ or } S=3,w_{2}) & = \frac{\exp(\zeta_{0k}+\zeta_{1k} w_{2})}{\sum_{l\in\{1,3\} }\exp(\zeta_{0l}+\zeta_{1l}w_{2})}
\end{align*}
for $k=1,3$ and $w_{2}\in\{0,1\}$ where $\eta_{01}=0,\zeta_{11}=0,\zeta_{03}=-3,\zeta_{13}=6$. Again, each level group marginally has about the same frequency.

To summarize, individual trajectories are generated by first simulating two coin tosses to determine values for the two baseline binary factors. Then conditional on the factors, shape and level groups are randomly assigned by plugging the factors into the generalized logit functions above and drawing from multinomial distributions. Then, the chosen mean function is evaluated at five equidistant observation times $t=1,3.25,5.5,7.75,10$ that span the period 1 to 10 units. Random noise is added to induce variability. 

The random noise is made up of two components: individual-specific level perturbation and time-specific Gaussian measurement error. For individual $i$ ($i=1,...,n$) at the $j$th observation time ($j=1,..,5$) with the $l$th mean function, the observed outcome equals
$$y_{ij} = g_{l}(t_{j})+\lambda_{i}+\epsilon_{ij}\quad\text{where}\quad \epsilon_{ij}\sim N(0,\sigma_{\epsilon}^{2}), \lambda_{i}\sim N(0,\sigma_{\lambda}^{2})$$
where $\sigma_{\epsilon}$  is the standard deviation of the measurement error and $\sigma_{\lambda}$ is the standard deviation of the level perturbation. To create conditions with differing amount of overlap between groups, I let $\sigma_{\lambda}=2,3$ such that there are 4 or 6 standard deviation between the mean functions with the same shape. The magnitude of measurement error influences the signal to noise ratio and I let $\sigma_{\epsilon}=0.5, 2$ to create two extreme conditions. There are four possible combinations of these two properties, representing the four  conditions of the data-generating process in the simulation study. 

For each condition, we generate a data set of $n=500$ individuals using the process described above and apply five clustering methods: independence Gaussian mixture model, K-means on the difference quotients, PAM with a correlation-based dissimilarity measure, vertically shifting mixture models with independence, and vertically shifting mixture models with exponential correlation. For each method, we calculate the optimal $K$ using the silhouette measure \cite{kaufman1990}  for the partition methods and the BIC \cite{schwarz1978} for the models and estimate the misclassification rate when $K=3$. The misclassification rate detects whether the method discovers the underlying shape structure. This is repeated $B=500$ times such that we get 500 unique data sets under each condition on which we can apply each methods and summarize the results. 

Table \ref{tab:freq1} summarizes the simulations in terms of the number of groups and average misclassification rate. It is clear from this table that the standard Gaussian mixture model assuming independence does not select three groups as the optimal number of groups. The BIC with the independent mixture model consistently chooses five groups, the maximum we allow in the simulation, under all conditions. This method also does not perform well when forced to have $K=3$. Only about 50\% of the data is correctly specified in terms of the generating shape groups.
\begin{table}[ht]
\begin{center}
\begin{tabular}{cc|ccccc}
  \hline &&\multicolumn{4}{c}{Number of Groups}&Misclassification\\ $\sigma_{\epsilon}$&$\sigma_{\lambda}$&$K=2$&$K=3$&$K=4$&$K=5$&Rate\\ \hline\multicolumn{7}{c}{\textbf{Independent Mixture}}\\ 0.50 & 2.00 & 0.00 & 0.00 & 10.00 & 490.00 & 0.41 \\ 
  2.00 & 2.00 & 0.00 & 0.00 & 24.00 & 476.00 & 0.39 \\ 
  0.50 & 3.00 & 0.00 & 0.00 & 1.00 & 499.00 & 0.45 \\ 
  2.00 & 3.00 & 0.00 & 0.00 & 5.00 & 495.00 & 0.45 \\ 
   \\ \multicolumn{7}{c}{\textbf{K-means on Difference Quotients}}\\0.50 & 2.00 & 0.00 & 500.00 & 0.00 & 0.00 & 0.00 \\ 
  2.00 & 2.00 & 483.00 & 17.00 & 0.00 & 0.00 & 0.38 \\ 
  0.50 & 3.00 & 0.00 & 500.00 & 0.00 & 0.00 & 0.00 \\ 
  2.00 & 3.00 & 483.00 & 17.00 & 0.00 & 0.00 & 0.38 \\ 
   \\ \multicolumn{7}{c}{\textbf{Correlation-based PAM}}\\0.50 & 2.00 & 403.00 & 0.00 & 0.00 & 97.00 & 0.25 \\ 
  2.00 & 2.00 & 500.00 & 0.00 & 0.00 & 0.00 & 0.27 \\ 
  0.50 & 3.00 & 403.00 & 0.00 & 0.00 & 97.00 & 0.25 \\ 
  2.00 & 3.00 & 500.00 & 0.00 & 0.00 & 0.00 & 0.27 \\ 
   \\ \multicolumn{7}{c}{\textbf{Vertically Shifted Independent Mixture}}\\0.50 & 2.00 & 0.00 & 499.00 & 0.00 & 1.00 & 0.00 \\ 
  2.00 & 2.00 & 0.00 & 498.00 & 2.00 & 0.00 & 0.05 \\ 
  0.50 & 3.00 & 0.00 & 499.00 & 0.00 & 1.00 & 0.00 \\ 
  2.00 & 3.00 & 0.00 & 498.00 & 2.00 & 0.00 & 0.05 \\ 
   \\ \multicolumn{7}{c}{\textbf{Vertically Shifted Exponential Mixture}}\\0.50 & 2.00 & 0.00 & 500.00 & 0.00 & 0.00 & 0.00 \\ 
  2.00 & 2.00 & 0.00 & 499.00 & 1.00 & 0.00 & 0.05 \\ 
  0.50 & 3.00 & 0.00 & 500.00 & 0.00 & 0.00 & 0.00 \\ 
  2.00 & 3.00 & 0.00 & 499.00 & 1.00 & 0.00 & 0.05 \\ 
   \hline\end{tabular}
\caption{Frequency table of the number of groups chosen and average misclassification rate ($K=3$) for 500 replications of clustering methods applied to data generated under different conditions for the standard deviation of $\epsilon$ and $\lambda$.}
\label{tab:freq1}
\end{center}
\end{table}

Of the established methods that are intended to group on shape, K-means on difference quotients selects the correct number of groups if the magnitude of the measurement error is small (Table \ref{tab:freq1}). If the variability around the individual mean is large, the method chooses two groups and misclassifies about 38\% of the individuals when forced to have three groups. Using the correlation dissimilarity measure with the PAM algorithm gives slightly better results with only 25-27\% misclassification, but the it does not consistently choose three groups. It only selects two and five groups.

Lastly, the method that prevailed amongst the competition is the vertically shifted mixture models. For every condition, the method chose three groups as the optimal number 99\% of the time and when forced to $K=3$, the method discovered the shape groups with little misclassification. Only when the measurement error is large ($\sigma_{\epsilon}=2$) did the method misclassify 5\% (about 25 individuals) in terms of shape. The method worked well under both assumptions of independence and exponential correlation even though we know the true correlation structure is exchangeable with correlation -0.25. Therefore, in this case, the shapes are distinct enough that either correlation assumption worked well.

\section{Conclusion}
This method to cluster longitudinal data by the shape of the trajectory over time directly removes the level by subtraction individual-specific means prior to modeling. In contrast to partition methods based on observed vectors, this approach is not only allows irregularly sampled data but may perform better when observation times are random, not fixed. The mixture model provides a probability framework to explicitly model the variability around an underlying smooth function. Therefore, the method does well even when the measurement error is large. 

The standard method of mixture models applied to the original data values does not directly cluster based on shape. The methods created to focus on shape fail under fairly common circumstances. K-means applied to difference quotients works tremendously when there is little measurement error, but fails to create distinct shape groups when the error is larger. The correlation-based dissimilarity measure fails to recognize horizontal trajectories as similar. Under our simulation, it is hard to detect the impact of measurement error since the misclassification rate is dominated by the horizontal issues. Since it is common to have horizontal trajectories as well as substantial measurement error in longitudinal data, both of these methods are not recommended for wide use. 

While the proposed methods drastically out performs the competition, there are two main issues. First, subtracting the observed mean impacts the covariance in a way that makes it harder to model with familiar correlation structures. However, if the shapes drastically differ, using the the simple independence correlation structure may work well enough to detect the shape groups. Second, care needs to be taken when there is sparse and irregularly sampling. 

\printbibliography

\end{document}