\documentclass[12pt]{article}
\usepackage{fullpage,amsmath,amsfonts,graphicx,amsthm,pdflscape}
\usepackage[doi=true,url=false,eprint=false,isbn=false,firstinits=true]{biblatex}
\usepackage{setspace}
\title{Vertically Shifted Mixture Models for Clustering Longitudinal Data by Shape}
\author{Brianna C. Heggeseth and Nicholas P. Jewell}
\bibliography{Dissertation}
\newtheorem{theorem}{Theorem}
  \newcommand{\B}[0]{\mathbf}
    \newcommand{\BS}[0]{\boldsymbol}
    
\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1.5pt
    \futurelet \reserved@a \@xhline
}
\makeatother

\begin{document}
%\doublespace
%\maketitle
\section{Introduction}
A key advantage of a longitudinal study is its ability to measure individual change over time. [Outcome measurements are repeated measured over a period of time and this vector of observations can be seen as a trajectory.] A typical longitudinal data analysis models the conditional mean outcome as a function of a time variable as well as time-fixed and time-dependent explanatory variables while taking the inherent time dependence into account. This analysis framework allows researchers to estimate how the mean outcome changes over time in the overall group as well as within defined time-fixed groups based on characteristics such as ethnicity/race or sex using interaction terms. However, if the variability in how individuals change over time do not coincide with defined groups, this type of analysis masks interesting and useful patterns. In this circumstance, it may be more informative to partition the subjects into data-driven groups to further study the relationships.

Cluster analysis methods have been adapted and developed specifically for approaching longitudinal data in this way, focusing on finding groups with similar longitudinal patterns over time in genomics and behavioral and cognitive development \cite{genolini2010, jones2001, muthen2010, mcnicholas2010,cruzmesia2008,ciampi2012}. The field of longitudinal cluster analysis can be separated into two approaches. The first, a model-based approach, uses mixture modelling techniques \cite{everitt1981,titterington1985,mclachlan1988,mclachlan2000,fruhwirth2006}. These methods assume a data-generating model for the longitudinal data that includes a group structure \cite{muthen2010,jones2001,leisch2004}. The other approach uses standard clustering techniques such as partitioning methods such as K-means \cite{macqueen1967,hartigan1979} and partitioning around medoids (PAM) \cite{kaufman1990}. These methods are based on defining a dissimilarity measure that incorporates the unique properties of longitudinal data \cite{genolini2010}.  Unfortunately, many proposed dissimilarity measures require the observation times to be consistent within the cohort. 

Despite the vast number of clustering methods for longitudinal data, most of them do not explicitly group subjects based on the how they change over time regardless of the overall outcome level. Rather, the dissimilarity between individuals is based on Euclidean distance between outcome vectors. Hence, if subsets of individuals with similarly shape trajectories are heterogenous in terms of outcome level, the resulting clusters will be driven by level rather than shape. Many authors are not aware of this and interpret their clustering results as if the data were grouped by shape \cite{pryor2011,carter2012,nagin1999}. There has been little work and discussion on the best clustering methods to utilize when shape, as distinct from level, is the feature of interest. The few proposals in the literature determine two individuals are similar by comparing their estimated derivative function or if the outcome vectors are highly correlated. 

The first derivative of a function describes the rate of change while ignoring the intercept or level of the original function. With longitudinal data, we do not directly observe the derivative function for each individual. Assume that the $j$th observed outcome for individual $i$ at time $t_{ij}$ is a realization of the model
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
for $i=1,...,n$ and $j=1,...,m_{i}$ where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma_{i}^{2})$, $f_{i}$ is a subject-specific differentiable, continuous function, and $m_{i}$ is relatively small (usually around 5 to 10). If the observation times are consistent across individuals with $m_{j}=m$ and $t_{ij} = t_{j}$ for $j=1,..,m$ and $i=1,...,n$, \Textcite{moller2003} and \textcite{d2000} independently suggested estimating the derivative of the underlying smooth function via the difference quotient by calculating the slope of a linear interpolation between adjacent repeated measures,
$$\hat{f}_{i}^{'}(t_{j}) = (y_{i\;j+1})-y_{ij})/(t_{j+1}-t_j).$$
By the mean value theorem, this is an unbiased estimate of the true derivative, $f_{i}^{'}(\tau),$ at a point $\tau\in[t_{j},t_{j+1}]$ such that
$$E(\hat{f}_{i}^{'}(t_{j})) = f_i(t_{j+1})-f_i(t_j))/(t_{j+1}-t_j) =f_{i}^{'}(\tau). $$
However, the estimate is highly variable if $\sigma_{i}^{2}$ is large since
$$\text{Var}(\hat{f}_{i}^{'}(\tau)) =  2\sigma^{2}_{i}/ (t_{j+1}-t_j)^{2}.$$
Large variability in the estimates impacts the cluster analysis if enough estimates are far from the true derivative. The dissimilarity between two individuals is measured as the squared Euclidean distance between the vectors of derivative estimates. High variability can lead to high dissimilarity for individuals with similarly underlying shape. One way to minimize the variance is to maximize the time between observations. Observing only two observations, one at baseline and another at the end of follow up, minimizes the variance but at the great expense of observing the rate of change during the follow up period. If time of observations are densely sampled, a functional approach smoothes out the noise using splines to estimate the function and then the derivative function \cite{tarpey2003}. In either circumstance, the derivatives are independently estimated for each individual and there is no direct way to borrow strength between individuals to better estimate the derivative even if some individuals are thought to have a common shape.

The Pearson correlation coefficient has also been used to measure dissimilarity between two longitudinal vectors based on the shape \cite{chouakria2007, eisen1998, chiou2008}. Despite the wide use, there has been little discussion about how well the correlation does to discriminate between shapes in this context. In the multivariate setting, one dissimilarity measure based on the Pearson correlation coefficient between two comparable vectors is 
$$d_{Cor}(\B x,\B y) = 1-Cor(\B x,\B y)$$ 
where $$Cor(\B x,\B y) = \frac{\sum^{m}_{j=1}(x_{j}-\bar{x})(y_{j}-\bar{y})}{\sqrt{\sum^{m}_{j=1}(x_{j}-\bar{x})^{2}}\sqrt{\sum^{m}_{j=1}(y_{j}-\bar{y})^{2}}}.$$
The values for this measure range between 0 and 2 with extremes attained only when there is perfect linear positive or negative correlation. Let's assume the data vectors are generated the same as above as noisy observations of an underlying function. If there is no noise and we observe only the function at a discrete number of times, a vector with an underlying functional shape of $f(t)$ is perfectly positively correlated with a vector with functional shape of $a\cdot f(t) + b$ where $a\geq0$ and $b\in\mathbb{R}$. As a result, two vectors that have the same shape over time but at different levels have a dissimilarity of zero since the correlation coefficient equals 1. However, two vectors with underlying functions that are scalar multiples of each other are placed in the same cluster despite having drastically different shapes. While this method succeeds in grouping individuals with the same shape at different levels, it fails in the sense that is also groups individuals with different shapes. If the data is observed with error such that $\epsilon_{ij}\sim(0,\sigma_{i}^{2}),$ the magnitude of the correlation is generally deflated such that the dissimilarity moves closer to the middle value of 1.

If two vectors are observations of constant functions, the correlation coefficient is undefined. With noise, the expected dissimilarity value is 1 with variation dependent on the number of observations. In other words, the Pearson correlation coefficient does not consistently detect two horizontal curves to have the same shape especially with a small number of observations, which is typical in longitudinal data. This dissimilarity measure fails to detect similar underlying shapes when $\sigma_{i}^{2}$ is large, the number of observations is low, and the data set includes constant or stable patterns over time. 

The related cosine-angle dissimilarity measure has also been used to determine the similarity of two vector profiles \cite{eisen1998}. The measure is the uncentered version of the Pearson correlation dissimilarity,
$$d_{Cos}(\B x,\B y) = 1- \frac{\sum^{m}_{j=1}x_{j}y_{j}}{\sqrt{\sum^{m}_{j=1}x_{j}^{2}}\sqrt{\sum^{m}_{j=1}y_{j}^{2}}}.$$
Since the mean is not subtracted from the vector elements, it is defined for two vectors based on constant functions. However, the measure is invariant to scaling transformations of the vectors and not invariant to vertical shifts, which are undesirable properties for a dissimilarity measure based on our definition of shape similarity. 

Both of these proposals implicitly remove the level while clustering, but they fail to provide answers to key research questions about longitudinal data. Dissimilarity measures based on the difference quotient and the Pearson correlation coefficient do not consistently reflect the shape similarity in underlying functions since they are highly sensitive to noise. The measured cannot be defined for most longitudinal data as they require outcomes observed at regularly sampled time points. Once groups are formed, it is important to study the relationship between baseline factors and group membership. Standard partitioning methods do not provide a framework in which to estimate these relationships while accounting for uncertainty in membership labels.  

In this paper, we propose vertically shifting each individual's outcome vector by subtracting its mean before fitting a multivariate mixture model in order to directly cluster longitudinal data by its shape over time. This method provides a probability framework to account for uncertainty and works with irregularly sampled longitudinal data. In Section 2, we discuss the pre-modeling translation and then we present the model specification and provide details for the mean and covariance structure in Section 3. Implementation including parameter estimation is discussed in Section 4. We discuss challenges when modeling the transformed data in Section 5. Lastly, a simulation study demonstrates the merits of the proposed method in comparison to standard clustering methods and those that explicitly attempt to group based on shape in Section 6.

\section{Pre-modeling translation}
A finite mixture model is a standard method for clustering multivariate data \cite{everitt2009} and has been used for longitudinal applications \cite{muthen2010, jones2001}. See \cite{mclachlan2000} for an extensive summary of finite mixture models. However, for longitudinal data, the models are commonly used for the observed data without much regard to the goal of clustering by shape. 

We suggest removing the level by subtracting out the mean outcome level prior to modeling. Subtracting the mean is not a novel idea in statistics or even cluster analysis. In fact, experimental data such as gene expression microarrays are often normalized to compensate for variability in the measurement device between samples \cite{park2003}. In cluster analysis of multivariate data, it is often recommended that each variable is standardized by subtracting the mean and dividing by the standard deviation within each variable so the variables are in comparable units and equally contribute to the grouping process \cite{everitt2009}. This is not necessary or recommended in the longitudinal setting where each variable is a repeated measurement at a different time point. 

To compare shapes, we want to maintain the original scale since the relationship between measurements within individuals is of interest. A translation via centering \cite{chiou2008} or vertically shifting preserves the shape of the data over time. In general, pre-processing the data can provide a path to answering the research question but any transformation of the data should be carefully studied for potential unintended consequences.

\section{Model Specification}
 Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote an outcome vector of repeated observations for individual $i$, $i=1,...,n$. The vector of corresponding times of observation for individual $i$ is denoted as $\B t_{i}=(t_{i1},...,t_{im_{i}})$ and $\B w_{i}$ is a $q$-length design vector based on time-fixed factors that are typically collected at or before time $t_{i1}$. We assume that there are $K$ mean shape functions, $\mu_{k}(t)$, in the population such that the outcome vector for individual $i$ in shape group $k$ is a realization of
 $$\B y_{i} = \lambda_{i}\B 1_{m_{i}}+\BS\mu_{i}+\BS\epsilon_{i},\quad \lambda_{i}\sim F_{\lambda}, \quad \BS\epsilon_{i}\sim N(0,\BS\Sigma_{k})$$
 where $F_{\lambda}$ the distribution for the random intercept, $\B 1_{m_{i}}$ is a m-length vector of 1's, and $\mu_{ij} = \mu_{k}(t_{ij})$ is the $j$th element of a $m_{i}$-length vector of mean values evaluated at the observation times, $\B t_{i}$. The outcome vector is determined by a mean shape function, a random intercept, and potentially correlated random errors. The probability of being in a particular shape group depends on baseline covariates included in the vector $\B w_{i}$. Let $\bar{y}_{i}= m_{i}^{-1}\sum^{m_{i}}_{j=1} y_{ij} = \lambda_{i}+\bar{\mu}_{i}+\bar{\epsilon}_{i}$ be the mean of the outcome measurements for individual $i$. This measure of the vertical level of the data vector can be removed by applying a linear transformation, $\B A_{i} = \B I_{m_{i}} - m_{i}^{-1}\B 1_{m_{i}}\B 1_{m_{i}}^{T}$, to the vector of observations. The vertically shifted vector for individual $i$ equals 
\begin{align*}
\B y^{*}_{i} &= \B A_{i}\B y_{i}\\
&=\B A_{i}(\lambda_{i}\B 1_{m_{i}}+\BS\mu_{ik}+\BS\epsilon_{i})\\
&=\B A_{i}(\BS\mu_{i}+\BS\epsilon_{i})\\
&=\BS\mu_{i} - \bar{\mu}_{i}+\epsilon_{i}-\bar{\epsilon}_{i}.
\end{align*}
Applying the symmetric matrix $\B A_{i}$ to the vector $\B y_{i}$ subtracts the individual mean, $\bar{y}_{i}$, from each element $\B y_{i}$. This results in the removal of the random intercept, $\lambda_{i}$, leaving the mean function evaluated at the observation times plus random error shifted by a random constant, $\bar{\mu}_{i}+\bar{\epsilon}_{i}$. Clearly, we do not have to worry about the distribution of the random intercept or any other time-fixed factors that impact the level of the outcome. 

Once the level is removed, we assume the vertically shifted data, $\B y_{i}^{*}$, follow a Gaussian mixture of $K$ groups with mean shape functions and random errors. If the observation times are fixed, vertically shifted data generated from the model above follows this Gaussian mixture. Thus, conditional on observation times $\B t$ and baseline covariates $\B w$, $\B y^{*}$ is assumed to be a realization from a finite mixture model with density
\begin{align*}
 f(\B y^{*}|\B t,\B w,\BS\theta) =  \sum^{K}_{k=1}\pi_{k}(\B w,\BS \gamma)f_{k}( \B y^{*}|\B t,\BS\theta_{k})
\end{align*}
where $\pi_{k}(\B w,\BS \gamma)$ is the prior probability of being in the $k$th component given baseline covariates, $\B w$. The full vector of parameters for the model is $\BS\theta = (\BS\gamma,\BS\theta_{1},...,\BS\theta_{K})$. To allow baseline covariates to impact the probability of having a certain shape pattern over time, the prior probabilities are parameterized using the generalized logit function of the form
$$\pi_{k}(\B w,\BS\gamma)=\frac{\exp(\B w^{T}\BS\gamma_{k})}{\sum_{j=1}^{K}\exp(\B w^{T}\BS\gamma_{j})}$$ 
for $k=1,...,K$ where $\BS \gamma_{k}\in\mathbb{R}^{q}$, $\BS\gamma = (\BS\gamma_{1},...,\BS\gamma_{K})$, and $\BS\gamma_{K}=\B 0$. For continuous outcome vectors, the component densities $f_{k}(\B y^{*}|\B t,\BS\theta_{k})$ are multivariate Gaussian densities with mean and covariance dependent on time. 
%What about if the times are not fixed, what is the actually generating model?

Only focusing on shape, the mean is modeled as a smooth function of time represented by a chosen functional basis such as the B-spline basis \cite{deboor1978, schumaker1981,curry1966, de1976}. A B-spline function of order $p$ with $L$ internal knots, $\tau_{1},...,\tau_{L}$, is defined by a linear combination of coefficients and B-spline basis functions
$$\mu(t) = \sum^{L+p}_{j=1} \beta_j B_{j,p}(t)$$
where the basis functions, $B_{j,p}(t)$, are defined iteratively \cite{deboor1972,cox1972}. Values from the $p$th order B-spline basis functions taken at observation times $\B t_{i}$ can be used in a design matrix, $\B x_{i}$, to linearly represent the mean vector for individual $i$. In the multivariate form, the mean vector at observation times $\B t_{i}$ equals $\B x_{i}\BS\beta_{k}$ where $\BS\beta_{k}=(\beta_{k,1},...,\beta_{k, L+p}).$  
%Condense the next four paragraphs into 1-2
There are many potential assumptions to be made about the [covariance of the random deviations from the mean]. Here, we allow the covariances to differ between clusters. Since it is common for longitudinal data to have sparse, irregular time sampling, we need to impose structure on the covariance matrix to allow for parameter estimation as described by \textcite{jennrich1986} in their seminal paper. A common structure is conditional independence with constant variance where $\B \Sigma_{k}= \sigma_{k}^{2}\B I_{m_{i}}$. This is typically an unrealistic assumption for longitudinal data since there is inherent dependence between repeated measures on the same unit. 

Compound symmetry, which is also known as exchangeable correlation, is a popular correlation structure in longitudinal analysis where all repeated measures are equally correlated. This is typically paired with constant variance such that $\B \Sigma_{k} = \sigma_{k}^{2}(\rho_{k}\B1_{m_{i}}\B1_{m_{i}}^{T}+(1-\rho_{k})\B I_{m_{i}})$ where $-1\leq\rho_{k}\leq 1$ is the correlation between any two distinct measurements within an individual. This dependence structure describes the resulting correlation matrix of a random intercept model.

Another structure that provides a compromise is the exponential correlation structure in which the dependence decays as the time between observations increases such that the element in the $j$th row and $l$th column of $\BS\Sigma_{k}$ is $\sigma_{k}^{2}\exp(-| t_{ij}-t_{il}| / r_{k})$ where $r_{k}> 0$ is the range of the dependence. If the range $r_{k}$ is small, the correlation decays quickly, but if $r_{k}$ is large, there is long range dependence between measurements within an individual. This structure is similar to the correlation matrix generated from a continuous autoregressive model of order one such that the element in the $j$th row and $l$th column of $\BS\Sigma_{k}$ is $\sigma^{2}\rho_{k}^{|t_{ij}-t_{ill}|}$ where $\rho_{k}$ is the correlation for measurements observed one unit of time apart. If $\rho_{k} = \exp(-1/r_{k})$, then the two parameterization result in the same structure as long as the correlation between two measures is constrained to be positive. This is a reasonable assumption for longitudinal data in the original form but it many not be acceptable for the transformed data as discussed later.

All of the covariance structures mentioned above are associated with weakly stationary processes with constant variance and correlation dependent only on the time lag between observations. If the variance or correlation function is non-constant but varying continuously, it could be potentially modeled as a function, but estimation is more difficult. It is important to model the covariance structure correctly as misspecification can highly impact mixture model results in terms of parameter estimates and the final clustering if the groups are not well separated \cite{heggeseth2013}. Transforming the data brings individuals with similar shapes closer but also brings others closer as well. In general, this decreases the separation between groups, which may force us to accurately model the correlation. 

\section{Model estimation}
Given a collection of independent observed outcome vectors from $n$ individuals, $\B y_{1},...,\B y_{n}$. The first step is to calculate the mean for each subject, $\bar{y}_{i}$,  and subtract the subject-specific mean from the observed outcome vector for $i=1,...,n$. This transformation results in independent vertically shifted vectors, $\B y^{*}_{1},...,\B y^{*}_{n}$.  

Then, the order of the spline and the number and location of internal knots for the mean structure is chosen. The B-spline basis should be kept constant for all shape groups, so the simplest way to select the number of knots is through visual inspection of the full data set. If the most complex shape patterns is a lower order polynomial, no internal knots are necessary. However, if the most complex function has local activity, adding knots and increasing the order of the spline functions flexibly accommodates the twists and turns of the mean patterns. In choosing both the order of the polynomials and the number of knots, it is important to balance the number of mean parameters with the sample size. Every unit increase in the order or in the number of knots increases the number of parameters by $K,$ the number of groups. In terms of location of the knots, one suggestion is to place knots at sample quantiles based on the sampling times of all the observations \cite{ruppert2002}. However, this strategy may not work well if the median time is not the point of deviation from a regular polynomial. If possible, it is best to place knots at local maxima, minima, and inflection points of the overall trends as to accommodate the differences from a polynomial function \cite{eubank1999}.  Once these are decided, the design matrices, $\B x_{i}$, are calculated using widely available B-spline algorithms for $i=1,...,n$. 

Parameters are estimated using maximum likelihood estimation via the EM algorithm. Under the assumption that $\B y^{*}_{1},...,\B y^{*}_{n}$ are independent realizations from the mixture distribution, $f(\B y^{*} | \B t, \B w, \BS\theta)$, defined in Section 3, the log likelihood function for the parameter vector, $\BS \theta^{*}$, is given by
$$\log L(\BS\theta)=\sum^{n}_{i=1}\log f(\B y^{*}_{i}|\B t_{i},\B w_{i},\BS \theta).$$
The maximum likelihood estimate of $\BS\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\BS\theta)/\partial \BS\theta=\B 0.$ Solutions of this equation corresponding to local maxima can be found iteratively through the expectation-maximization (EM) algorithm \cite{dempster1977}. This algorithm is applied in the framework where given $(\B t_{i},\B w_{i})$ each $\B y^{*}_{i}$ is assumed to have stemmed from one of the mixture components and the indicator denoting its originating component is missing. The complete-data log likelihood is based on these indicator variables as well as the observed data $\{(\B y^{*}_{i}, \B t_{i}, \B w_{i})\}$. The expectation step (E-step) involves replacing the indicators by current values of their conditional expectation, which is the posterior probability of component membership, written as
$$\alpha_{ik}=\pi_{k}(\B w_{i},\BS\gamma)f_{k}(\B y^{*}_{i}|\B t_{i},\BS\theta_{k})/\sum_{j=1}^{K}\pi_{j}(\B w_{i},\BS\gamma)f_{j}(\B y^{*}_{i}|\B t_{i},\BS \theta_{j})$$
for $i=1,...,n$ and $k=1,...,K$ using current estimates of the parameters. In the maximization step (M-step), the parameter estimates for the prior probabilities, linear mean, and covariance matrices are updated by maximizing the complete-data log likelihood using the posterior probabilities from the E-step in place of the indicator variables using numerical optimization. The E- and M-steps are alternated repeatedly until convergence. The EM algorithm guarantees convergence to a local maximum; global convergence may be attained through initializing the algorithm by randomly assigning individuals to initial components, running the algorithm multiple times and using the estimates associated with the highest log likelihood. Besides the parameter estimates, the algorithm returns the posterior probability estimates of component membership. These probabilities can be used to partition individuals into distinct clusters by selecting the cluster with the maximum posterior probability. However, unlike K-means, the posterior probability provides some measure of uncertainty in the group membership. 

Estimation requires the number of clusters, $K$, to be known. In practice, this is not the case and $K$ is chosen. The most popular way to choose $K$ is by setting a maximum value such that $K_{max}<n$, fitting the model under all values of $K=2,...,K_{max}$, and choosing the value that optimizes a chosen criteria \cite{celeux1996,fraley1998}. In this article, we use the Bayesian Information Criterion (BIC) \cite{schwarz1978}, defined as
$$BIC = -2\log L(\hat{\BS\theta})- d\log(n)$$
where $d$ is the length of $\BS\theta$, the number of parameters in the mixture model, and $L(\BS\theta)$ is the likelihood function for the parameter vector.  In particular, the criteria has been to select the number of clusters \cite{dasgupta1999,fraley1999} with good results in practice.

\section{Modeling Challenges}
There are issues of identifiability with Gaussian mixture models that can be mitigated through some minor constraints \cite{mclachlan2000}. In this section, we explore some unique consequences of vertically shifting the data on the model and estimation.
%%HERE
\subsection{Covariance of transformed data vectors}
Let $\B Y=(Y_{1},...,Y_{m})$ be a random vector observed at times $\B t=(t_{1},...,t_{m})$ such that
$\B Y = \lambda\B 1_{m} + \BS\mu + \BS\epsilon$
such that $\lambda\sim F$, $\BS\mu$ is a vector of evaluations of a known deterministic function, $\mu(t)$, at times $\B t$, and $\BS\epsilon\sim(0,\BS\Sigma)$. Let $\B\Sigma =\B V^{1/2}\B R(\rho)\B V^{1/2}$ where $\B R(\rho)$ is an $m\times m$ correlation matrix based on the parameter $\rho$ and potentially the associated observation times, and $\B V$ is a $m\times m$ matrix with variances along the diagonal. 

Subtracting the mean of the elements of the vector by applying the transformation matrix $\B A = \B I_{m}-m^{-1}\B 1_{m}\B 1_{m}^{T}$ changes the correlation structure of the data. The covariance of the transformed random vector, $\B Y^{*}$, equals
\begin{align*}
Cov(\B Y^{*})&=Cov(\B A\B Y)\\
&=Cov( \B A(\lambda\B 1_{m}+ \BS\mu + \BS\epsilon))\\
&=Cov( \B A( \BS\mu + \BS\epsilon))\\
&= \B ACov( \BS\mu + \BS\epsilon)\B A^{T}
\end{align*}
by the properties of covariance. One important property of this transformation is that it is non-invertible; once the mean is subtracted from the data, the original data cannot be recovered. This has tremendous consequences on the covariance matrix. Since $det(\B A) = 0$, the determinant of $Cov(\B Y^{*})$ is always zero and the covariance matrix is singular. Intuitively, the matrix has to be singular because the removal of the mean constrains the data to sum to zero. In other words, the transformation projects the data onto the $(m-1)$-dimensional subspace orthogonal to the nullspace $\B 1$. However, this presents challenges when trying to model the covariance of the transformed data. 

Rather than focusing on the covariance of the transformed vector, we work with the covariance of the transformed vectors after removing the relationship with time. Therefore, the covariance of the deviations of the transformed data from the known mean shape is
\begin{align*}
Cov(\B Y^{*} - \BS\mu) &= Cov(\B A\B Y - \BS\mu)\\
&= Cov(\B A(\lambda\B 1_{m} + \BS\mu + \BS\epsilon) - \BS\mu)\\
&= Cov(\B A(\BS\mu + \BS\epsilon) - \BS\mu)\\
&= Cov((\B A-\B I_{m})\BS\mu + \B A \BS\epsilon).
\end{align*}
If the observation times are fixed, then $\BS \mu$ is not random and the covariance simplifies to $\B A Cov(\BS\epsilon)\B A^{T}$. However, if the observation times are random, then $\BS \mu$ is a random vector and contributes variability. To better understand how best to model the transformed data, we explore this covariance matrix when the observation times are fixed and random. From this point on, $\B I_{m}$ will be written as $\B I$ and $\B 1_{m}$ as $\B 1$ for simplification.

\subsubsection{Fixed observation times}
For the moment, assume that the observation times, $\B t$, are fixed. Then
\begin{align*}
Cov(\B Y^{*} - \BS\mu)&= \B ACov(\BS\epsilon)\B A^{T}\\
&= \B A\BS\Sigma \B A^{T}
\end{align*}
where $\BS\Sigma$ is the covariance of the original random errors. If the variance is constant over time, $\B V=\sigma^{2}I$, and the elements of the original vector are independent, $\B R_{i}(\rho)=\B I$, then the covariance of the deviations of the transformed data from the known mean shape is
\begin{align*} 
Cov(\B Y^{*}- \BS\mu) &= \sigma^{2}\B A\B A^{T} \\
&=\sigma^{2}\B A\\
&= \sigma^{2}(\B I - m^{-1}\B1\B1^{T})\\
&=\sigma^{2}\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)\B I)
\end{align*}
 where $a=\frac{-1}{m-1}$. Therefore, if the observation times are fixed and the data has independent errors, subtracting the estimated mean induces negative exchangeable correction between the observations of magnitude $\frac{-1}{m-1}$. Additionally, the variance decreases to $\sigma^{2}\frac{m-1}{m}$. If $m$ is large, the resulting correlation structure is approximately independent with variance $\sigma^{2}$.
 
 If the errors in the original data have constant variance, $\B V=\sigma^{2}\B I$, and are exchangeable with $\B R(\rho) = \rho\B 1 \B 1^{T} + (1-\rho)\B I$, then the covariance of the deviations of the transformed data from the known mean shape is
 \begin{align*}
 Cov(\B Y^{*}- \BS\mu) &= \sigma^{2}\B A\B R(\rho)\B A^{T}\\
 &= \sigma^{2}(\B I-m^{-1}\B1\B1^{T})(\rho\B1\B1^{T}+(1-\rho)\B I)(\B I-m^{-1}\B1\B1^{T})^{T}\\
 &= \sigma^{2}(1-\rho)(\B I-m^{-1}\B1\B1^{T})\\
 &=\sigma^{2}(1-\rho)\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)\B I)
 \end{align*} 
 where $a=\frac{-1}{m-1}$. This transformation maintains the exchangeable structure but with negative correlation on the off diagonal and decreased variance of $\sigma^{2}(1-\rho)\left(\frac{m-1}{m}\right)$.  Again, if the number of observed data points is large, then the structure is approximately independent with variance $\sigma^{2}(1-\rho)$.

 On the other hand, if the original correlation is exponential such that the correlation decreases as time lags increases, $Cor(Y_{j},Y_{l}) = \exp(-|t_{j}-t_{l}|/\rho)$, the resulting covariance after transformation is not a recognizable structure. In fact, the covariance can no longer be written as a function of time lags. The covariance matrix is a linear combination of the original correlation matrix, column and row means, and the overall mean correlation,
   \begin{align*}
 Cov(\B Y^{*}- \BS\mu) &= \sigma^{2}\B A\B R(\rho)\B A^{T}\\
 &= \sigma^{2}\left[\B R(\rho)-m^{-1}\B1\B1^{T}\B R(\rho)-m^{-1}\B R(\rho)\B1\B1^{T} + m^{-2}\B1\B1^{T}\B R(\rho)\B1\B1^{T}\right]\\
  &= \sigma^{2}\left[\B R(\rho)-\text{ column mean vector }-\text{ row mean vector } + \text{ overall mean}\right].
 \end{align*} 
 This non-stationary covariance matrix includes negative correlations when the mean of the correlations within each column and within each row are positive and substantial. For example, if $\sigma^{2}=1$, $\rho = 2$ and $\B t=(1,2,3,4)$, then the covariance matrix of the deviations of the transformed data from the known mean shape is
$$ Cov(\B Y^{*}- \BS\mu) = \left[ \begin{array}{cccc}
 0.499&  0.009& -0.229& -0.278\\
  0.009&  0.307& -0.087& -0.229\\
 -0.229& -0.087&  0.307&  0.009\\
 -0.278& -0.229&  0.009&  0.499
\end{array}\right].$$
The variance and covariance changes over time with the covariance becoming negative as the time lag increases.  If the number of observation times increase such that the observation period expands, the covariance of the transformed vector becomes close to the original covariance as column, row, and overall means decrease to zero when the number of pairs of measurements with large time lags increases. However, if the observation period remains fixed as the number of observations increases, the covariance after transformation continues to be non-stationary and has negative correlations.  

We have calculated the covariance of the transformed random vector under three common covariance structures for the original data assume fixed observation times. All of these covariance matrices are not invertible since $det(\B A) = 0$. In particular, if prior to transformation, the errors are independent or exchangeable, the correlation of the resulting transformed data is exchangeable equal to $\frac{-1}{m-1}$. This particular value has significant meaning as it is the lower bound for correlation in an exchangeable matrix. This means that the true parameter value of the correlation for the transformed vector is on the boundary of the parameter space. Therefore, even if the true structure is known, estimating parameters for the true model is difficult. Conditional independence or the exponential structure may be an adequate approximation to regularize the estimation, especially if $m$ is moderately large.

\subsubsection{Random observation times} 
In practice, individuals in a longitudinal study are not typically observed at exactly the same times but rather at random times. When the times are random, the vector $\BS\mu$ is random because the elements are evaluations of the deterministic function, $\mu(t)$, at random times. Therefore, the transformed vector has variability due to the random times in addition to the errors. 

If the covariance of the original errors, $\BS \Sigma$, does not depend on time such as in the case of conditional independence or exchangeable, then the covariance simplifies to
\begin{align*}
Cov(\B Y^{*} - \BS\mu) &= Cov((\B A-\B I)\BS\mu + \B A \BS\epsilon)\\
&=(\B A-\B I) Cov(\BS\mu)(\B A-\B I)^{T} + \B A Cov(\BS\epsilon)\B A^{T}\\
&=m^{-2}\B 1\B 1^{T} Cov(\BS\mu)\B 1\B 1^{T} + \B A\BS\Sigma\B A^{T}.
\end{align*}
Let the random times, $t_{1},...,t_{m}$ be independent with potentially different expected values and variances. The covariance of $\BS\mu$ equals a diagonal matrix with the $j$th diagonal entry approximately equal to $Var(t_{j})[\mu'(E(t_{j}))]^{2}$ by the delta method. Then, the covariance matrix of $\B Y^{*}$ is the sum of two non-invertible matrices,
\begin{align*}
Cov(\B Y^{*} - \BS\mu) &=m^{-2}\left(\sum^{m}_{j=1}Var(t_{j})[\mu'(E(t_{j}))]^{2}\right) \B 1\B 1^{T}  + \B A\BS\Sigma\B A^{T},
\end{align*}
which need not be non-invertible. In fact, if the variance of the times and/or the derivative of the deterministic function, $\mu(t)$, is large, the positive magnitude of the first matrix may be large enough to counteract negative correlations in the second matrix. 

If the original covariance is dependent on the random times through an exponential function of time lags, the mean vector and error structure both depend on the times of observation. We explore the impact of transforming the data through empirical simulations. Let the observation times equal random perturbations around specified goal times such that $\B t = \B T + \B \tau$ where $\B \tau\sim N(0,\sigma^{2}_{\tau}\B I)$ and $\B T = (1,2,...,9,10)$. Therefore, $E(\B t) = \B T$ and $Cov(\B t ) = \sigma^{2}_{\tau}\B I$. We generate $n=500$ realizations of the model,
$$\B y_{i} = \BS\mu_{i} + \epsilon_{i}\quad\text{ where }\epsilon_{i}\sim N(0,\B R_{i}(\rho))$$
where the mean elements $\mu_{ij}=\mu(t_{ij})$. We repeat the simulation under different assumptions for the mean function, $\mu(t)$, and standard deviations of the observation times, $\sigma_{\tau}$. Figure \ref{fig:cov1} and Figure \ref{fig:cov} show the estimated autocorrelation functions of the deviations of the transformed data from the mean when $\B R_{i}(\rho)$ is an exchangeable correlation matrix with correlation parameter $\rho=0.5$ and when  $\B R_{i}(\rho)$ is an exponential correlation matrix with range parameter $\rho=2$, respectively, under varying conditions for observations times and shape functions.

As the variance of observation times and the magnitude of the derivative mean function increases, the estimated correlation between deviations from the mean becomes more positive. Thus, variability in the observations times can result in covariance structures that are no longer singular. 

In practice, if the data are regularly or very close to regularly sample, negative correlations are problematic for estimation and an independence or exponential correlation structure may be the best option. If the data are irregularly sampled, one potential covariance model is an additive model that combines a random intercept with the exponential correlation \cite{diggle2002}, which may be appropriately flexible to approximate the covariance of the deviations from the mean of the transformed data.

\subsubsection{Unbalanced observation times}
In addition to the issues of fixed versus random sampling, having an unequal number of observations per subject can impact the estimation of covariance of transformed vector. As we saw above, the length of the vector, $m$, impacts the covariance of the transformed vector. Suppose the outcome vectors for a sample of individuals has the same mean shape and covariance over time, but each individual is observed a different number of times because they were unavailable for an interview or two. Transforming the vectors by subtracting means based on a variety of number of observations induces a different covariance structure for each individual based on the length of outcome vector. If there is quite a bit of variability in the number of observations, it may impact clustering to assume they share the same covariance structure during the estimation/clustering process. However, if the number of observation times is large for all subjects and the observation period is long, then the covariance matrices should be similar. 

Additionally, if the unbalanced nature of the data is due to lost to follow up during a longitudinal study, clustering based on the shape should be done with caution. If the general shape of the curve during the observation period is not measured adequately by the number of observations, it does not make sense to try and cluster those individuals with the rest who have more fully observed curves. 

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[height=5.5in]{Chp4Cov2}
\end{center}
\caption{Estimated autocorrelation functions of the deviations from the mean from data generated with an exchangeable correlation error structure and random observation times under different mean functions, $\mu(t)$, and standard deviations of the random time perturbations, $\sigma_{\tau}$.}
\label{fig:cov1}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=5.5in]{Chp4Cov}
\end{center}
\caption{Estimated autocorrelation functions of the deviations from the mean from data generated with an exponential correlation error structure and random observation times under different mean functions, $\mu(t)$, and standard deviations of the random time perturbations, $\sigma_{\tau}$.}
\label{fig:cov}
\end{figure}
\end{landscape}


\section{Simulation}
To compare the performance of the proposed clustering method with competing methods, we completed a simulation study with three trajectory shapes and three levels in the population. We restricted some shapes to particular levels to induce a relationship between level and shape. The five mean functions for generating the data with shapes at different levels are below:   
\begin{align*}
\mu_{1}(t) &= -1 - t \quad\quad\text{ (negative slope, low level)}\\
\mu_{2}(t) &= 11 - t\quad\quad\text{ (negative slope, high level)}\\
\mu_{3}(t) &= 0\quad\quad\text{ (horizontal, middle level)}\\
\mu_{4}(t) &= -11 + t\quad\quad\text{ (positive slope, low level)}\\
\mu_{5}(t) &= 1 + t\quad\quad\text{ (positive slope, high level)}
\end{align*} 
Individuals follow these patterns with varying probabilities that depend on two factors. The first factor $w_{1}$ impacts the shape and $w_{2}$ impacts the level. Individuals are randomly assigned values of these two binary factors with the independent simulated tosses of a fair coin such that $P(w_{1}=1) = P(w_{1}=0) = 0.50$ and $P(w_{2}=1) = P(w_{2}=0)=0.50$. 

Let $S$ be a categorical random variable that indicates the shape/slope group. $S=1,2,3$ refers to the negative slope, horizontal, and positive slope groups, respectively. Conditional on the baseline factors, the probability of being in a shape group equals
$$P(S=k |w_{1}) = \frac{\exp(\gamma_{0k}+\gamma_{1k}w_{1})}{\sum^{3}_{l=1} \exp(\gamma_{0l}+\gamma_{1l}w_{1})}$$
for $k=1,2,3$ where $\gamma_{01}=2,\gamma_{11} = -4,\gamma_{02}=1.5,\gamma_{12}=-2,\gamma_{03}=\gamma_{13} = 0$ and $w_{1}\in\{0,1\}$. Since the value of $w_{1}$ is determined by a coin toss, each shape group has about an equal frequency, marginally. 

The second factor impacts the level, but I placed restrictions when creating the mean functions; therefore, level and shape are not independent of each other. Let $L$ be a categorial random variable that indicates level group. $L=1,2,3$ refers to the low, middle, and high group, respectively. Conditional on the shape group, all of the horizontal lines are in the middle level. For those in either the negative or positive slope groups, the chance of the high or low level equals
\begin{align*}
P(L=k|S=1 \text{ or } S=3,w_{2}) & = \frac{\exp(\zeta_{0k}+\zeta_{1k} w_{2})}{\sum_{l\in\{1,3\} }\exp(\zeta_{0l}+\zeta_{1l}w_{2})}
\end{align*}
for $k=1,3$ and $w_{2}\in\{0,1\}$ where $\eta_{01}=0,\zeta_{11}=0,\zeta_{03}=-3,\zeta_{13}=6$. Again, each level group marginally has about the same frequency.

To summarize, individual trajectories are generated by first simulating two coin tosses to determine values for the two baseline binary factors. Then conditional on the factors, shape and level groups are randomly assigned by plugging the factors into the generalized logit functions above and drawing from multinomial distributions. Then, the chosen mean function is evaluated at five equidistant observation times $t=1,3.25,5.5,7.75,10$ that span the period 1 to 10 units. Random noise is added to induce variability. 

The random noise is made up of two components: individual-specific level perturbation and time-specific Gaussian measurement error. For individual $i$ ($i=1,...,n$) at the $j$th observation time ($j=1,..,5$) with the $l$th mean function, the observed outcome equals
$$y_{ij} = \lambda_{i}+\mu_{l}(t_{j})+\epsilon_{ij}\quad\text{where}\quad \epsilon_{ij}\sim N(0,\sigma_{\epsilon}^{2}), \lambda_{i}\sim N(0,\sigma_{\lambda}^{2})$$
where $\sigma_{\epsilon}$  is the standard deviation of the measurement error and $\sigma_{\lambda}$ is the standard deviation of the level perturbation. To create conditions with differing amount of overlap between groups, I let $\sigma_{\lambda}=2,3$ such that there are 4 or 6 standard deviation between the mean functions with the same shape. The magnitude of measurement error influences the signal to noise ratio and I let $\sigma_{\epsilon}=0.5, 2$ to create two extreme conditions. There are four possible combinations of these two properties, representing the four  conditions of the data-generating process in the simulation study. 

For each condition, we generate a data set of $n=500$ individuals using the process described above and apply five clustering methods: independence Gaussian mixture model, K-means on the difference quotients, PAM with a correlation-based dissimilarity measure, vertically shifting mixture models with independence, and vertically shifting mixture models with exponential correlation. For each method, we calculate the optimal $K$ using the silhouette measure \cite{kaufman1990}  for the partition methods and the BIC \cite{schwarz1978} for the models and estimate the misclassification rate when $K=3$. The misclassification rate detects whether the method discovers the underlying shape structure. This is repeated $B=500$ times such that we get 500 unique data sets under each condition on which we can apply each methods and summarize the results. 
\subsection{Results}
Table \ref{tab:freq1} summarizes the simulations in terms of the number of groups and average misclassification rate. It is clear from this table that the standard Gaussian mixture model assuming independence does not select three groups as the optimal number of groups. The BIC with the independent mixture model consistently chooses five groups, the maximum we allow in the simulation, under all conditions. This method also does not perform well when forced to have $K=3$. Only about 50\% of the data is correctly specified in terms of the generating shape groups.
\begin{table}[ht]
\begin{center}
\begin{tabular}{ccccccc}
  \thickhline $\sigma_{\epsilon}$&$\sigma_{\lambda}$&$K=2$&$K=3$&$K=4$&$K=5$&MR\\ \hline\multicolumn{7}{c}{\textbf{Independent Mixture}}\\ 0.50 & 2.00 & 0.00 & 0.00 & 10.00 & 490.00 & 0.41 \\ 
  2.00 & 2.00 & 0.00 & 0.00 & 24.00 & 476.00 & 0.39 \\ 
  0.50 & 3.00 & 0.00 & 0.00 & 1.00 & 499.00 & 0.45 \\ 
  2.00 & 3.00 & 0.00 & 0.00 & 5.00 & 495.00 & 0.45 \\ 
   \\ \multicolumn{7}{c}{\textbf{K-means on Difference Quotients}}\\0.50 & 2.00 & 0.00 & 500.00 & 0.00 & 0.00 & 0.00 \\ 
  2.00 & 2.00 & 483.00 & 17.00 & 0.00 & 0.00 & 0.38 \\ 
  0.50 & 3.00 & 0.00 & 500.00 & 0.00 & 0.00 & 0.00 \\ 
  2.00 & 3.00 & 483.00 & 17.00 & 0.00 & 0.00 & 0.38 \\ 
   \\ \multicolumn{7}{c}{\textbf{Correlation-based PAM}}\\0.50 & 2.00 & 403.00 & 0.00 & 0.00 & 97.00 & 0.25 \\ 
  2.00 & 2.00 & 500.00 & 0.00 & 0.00 & 0.00 & 0.27 \\ 
  0.50 & 3.00 & 403.00 & 0.00 & 0.00 & 97.00 & 0.25 \\ 
  2.00 & 3.00 & 500.00 & 0.00 & 0.00 & 0.00 & 0.27 \\ 
   \\ \multicolumn{7}{c}{\textbf{Vertically Shifted Independent Mixture}}\\0.50 & 2.00 & 0.00 & 499.00 & 0.00 & 1.00 & 0.00 \\ 
  2.00 & 2.00 & 0.00 & 498.00 & 2.00 & 0.00 & 0.05 \\ 
  0.50 & 3.00 & 0.00 & 499.00 & 0.00 & 1.00 & 0.00 \\ 
  2.00 & 3.00 & 0.00 & 498.00 & 2.00 & 0.00 & 0.05 \\ 
   \\ \multicolumn{7}{c}{\textbf{Vertically Shifted Exponential Mixture}}\\0.50 & 2.00 & 0.00 & 500.00 & 0.00 & 0.00 & 0.00 \\ 
  2.00 & 2.00 & 0.00 & 499.00 & 1.00 & 0.00 & 0.05 \\ 
  0.50 & 3.00 & 0.00 & 500.00 & 0.00 & 0.00 & 0.00 \\ 
  2.00 & 3.00 & 0.00 & 499.00 & 1.00 & 0.00 & 0.05 \\ 
   \thickhline\end{tabular}
\caption{Frequency table of the number of groups chosen and average misclassification rate (MR) ($K=3$) for 500 replications of clustering methods applied to data generated under different values for $\sigma_{\epsilon}$ and $\sigma_{\lambda}$.}
\label{tab:freq1}
\end{center}
\end{table}

Of the established methods that are intended to group on shape, K-means on difference quotients selects the correct number of groups if the magnitude of the measurement error is small (Table \ref{tab:freq1}). If the variability around the individual mean is large, the method chooses two groups and misclassifies about 38\% of the individuals when forced to have three groups. Using the correlation dissimilarity measure with the PAM algorithm gives slightly better results with only 25-27\% misclassification, but the it does not consistently choose three groups. It only selects two and five groups.

Lastly, the method that prevailed amongst the competition is the vertically shifted mixture models. For every condition, the method chose three groups as the optimal number 99\% of the time and when forced to $K=3$, the method discovered the shape groups with little misclassification. Only when the measurement error is large ($\sigma_{\epsilon}=2$) did the method misclassify 5\% (about 25) of the individuals in terms of shape. The method worked well under both assumptions of independence and exponential correlation even though we know the true correlation structure is exchangeable with correlation -0.25. Therefore, in this case, the shapes are distinct enough that either correlation assumption worked well.

\section{Conclusion}
This method to cluster longitudinal data by the shape of the trajectory over time directly removes the level by subtraction individual-specific means prior to modeling. In contrast to partition methods based on observed vectors, this approach not only allows irregularly sampled data but may perform better when observation times are random, not fixed. The mixture model provides a probability framework to explicitly model the variability around an underlying smooth function. Therefore, the method does well even when the measurement error is large. 

The standard method of mixture models applied to the original data values does not directly cluster based on shape. The methods created to focus on shape fail under fairly common circumstances. K-means applied to difference quotients works well when there is little measurement error, but fails to create distinct shape groups when there is moderate error. The correlation-based dissimilarity measure fails to group horizontal trajectories as similar. Therefore, it is hard to tell whether the high misclassification rate is due primarily to measurement error or the horizontal issues. Since it is common to have horizontal trajectories as well as substantial measurement error in longitudinal data, both of these methods are not recommended for wide use. 

While the proposed method drastically out performs the competition, there are two main issues. First, subtracting the observed mean impacts the covariance in a way that makes it harder to model with familiar correlation structures. However, if the shapes drastically differ, using the the simple independence correlation structure may work well enough to detect the shape groups. Second, care needs to be taken when there is sparse and irregularly sampling. 

\printbibliography

\end{document}