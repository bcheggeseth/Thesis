\documentclass[12pt]{article}
\usepackage{fullpage,amsmath,amsfonts,graphicx,amsthm}
\usepackage{setspace}
\title{Introduction Chapter}
\author{Brianna C. Heggeseth}

\newtheorem{theorem}{Theorem}
  \newcommand{\B}[0]{\mathbf}
    \newcommand{\bs}[0]{\boldsymbol}

\begin{document}
\doublespace
\maketitle
This thesis treats the general problem of clustering longitudinal data. Both statisticians, machine learning communities, and applied researchers have investigated this problem with reference to time series, longitudinal, and functional data. In this thesis, I focus on longitudinal data which is characterized by time ordered vectors of outcomes with temporal autocorrelation and sparse, irregular time sampling. Longitudinal studies play a prominent role in health, social and behavioral sciences as well as in the biological sciences, economics, and marketing. By following subjects over time, temporal changes in an outcome of interest can be directly observed and studied. I refer to a time-ordered vector of outcomes as a trajectory throughout this thesis. One common technique to visually explore and reduce the dimension of the data is to cluster subjects with similar trajectory patterns. There are conventional methods to cluster multivariate data and most of these methods can be classified into two approaches: dissimilarity-based and model-based methods. The first approach groups subjects based on a selected measure of dissimilarity by using partition algorithms such as K-means \cite{macqueen1967, hartigan1979} and K-medoids/partitioning around medoids \cite{kaufman1990}, or hierarchical clustering algorithms when there is a priori scientific knowledge of a hierarchical structure \cite{murtagh1983}. The later approach involves a finite mixture to model the heterogeneity in the data. Mean trajectories for each cluster and group membership probabilities for each subject are calculated by using maximum likelihood estimation for the model parameters \cite{mclachlan1988,mclachlan2000,everitt1981}. Both of these approaches are widely used and the pros and cons of the approaches have been discussed in depth \cite{magidson2002, everitt1981}.

The bulk of the standard clustering methods are only applicable to data with independent measurements and therefore are not appropriate to use on longitudinal data \cite{everitt2009}. Recent work has focus on extending conventional clustering methods by forming measures of dissimilarity that are sensitive to permutations in the time order \cite{chouakria2007}, forcing a functional basis on the mean structure \cite{nagin1999,gaffney1999}, and explicitly modeling the temporal correlation structure \cite{muthen1999,fraley1999,mcnicholas2010}. Others have used functional data analysis methods \cite{ramsay2002} to project the data into a given functional space and then calculate the dissimilarity measure using the fitted curves or coefficients \cite{serban2005, tarpey2003, abraham2003, tarpey2007,hitchcock2007}.

\section{Longitudinal data}
In classical univariate statistics, each of a number of individuals, or subjects, gives rise to a single measurement, termed the outcome. In multivariate statistics, that one measurement is replaced by a vector of different measurements. In longitudinal studies, a vector of measurements are observed for each subject, but now they represent the same quantity measured at a sequence of observation times. Therefore, the data has characteristics of both multivariate and time series data. But, they differ from multivariate data in that the time ordering imparts a much more highly structure pattern of interdependence among measurements than a typical multivariate data set. They differ from classical time series data because the data consists of a large number of independent short trajectories, one for each subject, rather than a few, long dependent series. Functional data is a fairly new category \cite{ramsay2006} where the data for a subject are assumed to be noisy observations from a smooth random function. If the observed times are a dense sample of the time period of interest, it may be reasonable to assume that a data set could be classified as both longitudinal and functional. However, typical longitudinal data sets have sparse and irregular observation times, which make it difficult to estimate each subjects' function. If we assume that there are common attributes among the subjects, we can borrow strength across people in the modeling stage to make up for the sparsity in the time sampling. 

Turning to notation, I let $Y_{ij}$ represent an outcome random variable and $\B x_{ij}$ a vector of length $p$ of explanatory variables observed at time $t_{ij}$, for observation $j=1,...,m_{i}$ on individual $i=1,...,n$. Also, i let $\B z_{i}$ be a vector of length $q$ of baseline variables observed at or before $t_{i1}$.The set of repeated outcomes for individual $i$ together in a vector is denoted as $\B Y_{i} = (Y_{i1},...,Y_{in_{i}})$ with mean $E(\B Y_{i}) = \bs\mu_{i}$ and $n_{i}\times n_{i}$ covariance matrix $Var(\B Y_{i}) = \B\Sigma_{i}$. I denote $\B y_{i}$ as the observed outcome vectors. 

\section{Clustering}
One of the most natural abilities of human beings involves the grouping and sorting objects into categories. Young children learning how to distinguish between objects by naming them: shapes, colors, types of animals. Group labels represent a convenient way of summarizing a large amount of information so that it can be understood more easily. This is even more vital when summarizing a large data set so that information can be retrieved more efficiently by providing a concise description of patterns of similarities and differences in the data. Cluster analysis is one tool to explore groups within a data set and it has found its way into various scientific disciplines: from mathematics and statistics to biology and genetics to economics and market research, each of which uses different terminology to describe the grouping process. It is called numerical taxonomy in biology, unsupervised pattern recognition in the machine learning literature, and data segmentation in market research, but the problem is same---to find groups of similar objects.

The problem is well-defined, but the solution is not. There is no agreed upon criteria that determines one grouping is better than another. In general, most statisticians agree that clusters should be formed by maximizing the similarity within groups and maximizing the dissimilarity between groups. Some believe that there are naturally occurring groups that need to be discovered, but other remark that cluster analysis can always find clusters even if there is no true underlying groups. However, Bonner \cite{bonner1964} suggests that the user has the ultimate judgement on meaning and value of the clusters. Clusters should not be judged on whether they are true or false but rather in their usefulness. No method works on every data set to find meaningful groups. For this reason, there is a wealth of clustering algorithms proposed in the literature. In this thesis, I maintain the general goal of clustering is to maximize similarity within groups understanding that the analysis is highly dependent on how the user determines individuals to be similar. In this way, clusters are meaningful to the user while maintaining the basic mathematical philosophy of clustering.  I overview two approaches to clustering as they provide a basis of the methods discussed in this thesis. 

\subsection{Dissimilarity-based methods}
Since clustering is the grouping of similar individuals, a quantitative measure to determine the extent to which two subjects are similar or dissimilar is required.  There are two main types of measures: distances metrics and dissimilarity measures. A distance metric is a function that satisfies four mathematical properties. For two vectors $\B x$ and $\B y$ such that $\B x,\B y\in\mathbb{R}^{m}$, $d(\B x,\B y)$ is a metric if the following conditions hold:
\begin{align*}
d(\B x,\B y)&\geq 0 \\
d(\B x,\B y) &= 0\text{ if and only if } \B x=\B y\\
d(\B x,\B y) &= d(\B y,\B x)\\
d(\B x,\B z) &\leq d(\B x,\B y) + d(\B y,\B z)\text{ for any }\B z\in \mathbb{R}^{m}
\end{align*}
The first three conditions express intuitive notions about the concept of distance. The distance between two objects is positive and only can be zero when comparing the same vectors. Therefore, the fourth condition, also known as the triangle inequality, determines whether a function is a metric. The most commonly used metric for continuous data is the Minkowski distance of order $p$ defined as
$$d(\B x,\B y) = \|\B x - \B y\|_{p} = \left(\sum^{m}_{i=1}|x_{i}-y_{i}|^{p}\right)^{1/p}$$
where $\B x = (x_{1},...,x_{m})$ and $\B y = (y_{1},...,y_{m})$. Typically, the Minkowski distance is used with order $p=1$ or $p=2$. The latter is defined the the Euclidean distance and the former is usually called the Manhattan or taxi cab distance. As $p$ increases to infinity, we obtain the Chebychev distance. In general, the measurement unit of the vector elements used can affect the clustering analysis. To avoid the dependence on the choice of measurement units, it is recommend to standardize each element over all of the data. However, in the context of longitudinal data, the elements are repeated measures and all have the same units so this is not necessary in this case. It is also important to note that these distances are invariant to permutation of the order of the elements in the vector.\\\\
The above distances are used frequently and have nice mathematical properties, but these measures may not quantify the dissimilarity in a meaningful way to all users. A dissimilarity measure is any other measure of distance that fulfills the first three conditions above but may not fulfill the triangle inequality and large values indicate more dissimilarity. Correlation-based measures may be appropriate. The Pearson correlation as a dissimilarity is defined as
$$d(\B x,\B y) = \frac{1}{2}\left(1 - \frac{(\B x - \bar{x})^{T}(\B y - \bar{y})}{\|\B x - \bar{x}\|_{2}\|\B y - \bar{y}\|_{2}}\right)$$
where $\bar{x}$ represents the average over all the elements in $\B x$ This range of this distance is $[0,1]$ where 1 represents perfect negative linear correlation defined as dissimilarity, 0 represents perfect positive linear correlation and thus similarity, and 0.5 when there is not linear correlation. Since this only measures the dissimilarity in terms of linear association, an extension would be to use the Spearman rank correlation in place of the Pearson correlation as it measures general association and it not limited the linear association.\\\\
Note that all of these measures required vectors of equal length. I will come back to this throughout this thesis. Now that the data can be described by a chosen dissimilarity measure, an clustering algorithm is used to group individuals. I highlight two partitioning algorithms here.
\subsubsection{Kmeans Algorithm}
K-means \cite{macqueen1967} is one of the simplest unsupervised learning algorithms used to solve the well-known clustering problem. Given a set of vectors $(\B y_{1},\B y_{2},...,\B y_{n})$, where $\B y_{i}\in\mathbb{R}^{m}$, the K-means clustering algorithm aims to partition the $n$ observational units into $K$ sets $\{C_{1},...,C_{K}\}$ so as to minimize the sum of squares from points to the assigned cluster centroids denoted below
$$\min_{\{C_{k}\}_{k=1}^{K}}\sum_{k=1}^{K}\sum_{i: \B y_{i}\in C_{k}}\|\B y_{i} -\bs \mu_{k}\|_{2}^{2}$$
where $\bs\mu_{1},...,\bs\mu_{K}$ are the mean vectors, also known as the centroids, of the $K$ clusters and $\|\B x-\B y\|_{2}^{2}$ denotes the squared Euclidean distance between the vectors $x$ and $y$. To find the sets $\B C$ that minimize the objective function, $K$ individuals are  strategically or randomly chosen as the centroids. The general algorithm proceeds by alternating between two steps:
\begin{enumerate}
\item Assignment step: Assign each observation to the cluster with the closest centroid
$$ C_{k}^{(t)} = \{\B y_{i}: \|\B y_{i} - \bs\mu_{k}\|_{2}^{2}\geq \|\B y_{i} - \bs\mu_{j}\|_{2}^{2} \;\;\forall\;\; 1\leq j\leq k\}$$
such that ever $y_{i}$ is in one and only set.
\item Update step: Calculate the centroid of the new cluster.
$$\bs\mu_{k} = \frac{1}{|C_{k}^{(t)}|}\sum_{i: \B y_{i}\in\B C_{k}^{(t)}} y_{i}$$
\end{enumerate}
The algorithm continues to iterate until the clusters no longer change. Depending on the initial partition, the algorithm is expected to converge to local optima; therefore, completing multiple random starts offers the best way to obtain a global optimum. \\\\
The K-means algorithm works best when data clusters are about equal size and shape. One downside to the K-means algorithm it that is based on the Euclidean distance; Therefore, by construction, the algorithm attempts to find ``spherical'' clusters. Also, the calculation of the centroids is sensitive to outliers and thus the centroid may not be representative of any of the cluster members.\\\\

 \subsubsection{Partitioning around medoids}
 One variation of the K-means algorithm is partitioning around medoids, also know as PAM, attempts to overcome some of the issues stated above \cite{kaufman1990}.  Unlike K-means, the algorithm operates on a user-provided dissimilarity matrix rather than calculating Euclidean distances by default. It is robust to outliers since the representative curves are selected from the observed data vectors rather than based on mean calculations. But, the number of clusters, $K$, has to be specified a priori similar to K-means. To find $K$ sets of points to minimize the sum of dissimilarity from the points to their respective medoids, $K$ individuals are randomly chosen as medoids denoted as $\B m_{1},...,\B m_{K}$. The algorithm alternates between the following two steps:
 \begin{enumerate}
\item Build step: Assign each observational unit to the closest medoid with respect to the given dissimilarity matrix.
\item Swap step: For each $k=1,...,K$, swap the medoid, $m_{k}$, with each non-medoid observation and compute the sum of the dissimilarities of the points to their closest medoid. Select the configuration with the smallest sum of dissimilarities.
\end{enumerate}
The algorithm continues until there is not change in the medoids. Although it has a longer run time, this building and swapping procedure returns a smaller sum of dissimilarity in contrast to a simple assignment and update procedure as in the K-means algorithm. 
\subsubsection{Choosing the number of components}
A major challenge in clustering is to determine the optimal number of clusters. Most global criteria choose $K$ based on minimizing the dissimilarity within clusters \cite{gordon1999}. However, there are many suggestions including the Gap statistic which compares the logarithm of the pooled within-cluster sum of square to its expectation under the appropriate null distribution \cite{tibshirani2001}. Kaufmann and Rousseeuw suggest using the silhouette to choose $K$ \cite{kaufmann1990} when compact and clearly separated clusters are sought. For each vector $i$, the silhouette, $s(i)$, is calculated defined as
$$s(i)=\frac{b(i)-a(i)}{\max\{a(i),b(i)\}}$$
where $a(i)$ is the average dissimilarity of  vector $i$ to all other points in its cluster, $A$, $d(i,C)$ is average dissimilarity of vector $i$ to all points in cluster $C$ and $b(i)=\min_{C\not= A} d(i,C)$. The overall average silhouette width, $\bar{s}$, is the average of $s(i)$ over all points in the data set. A maximum value for the number of clusters, $K$, is chosen such that $K<n$ and the given algorithm is run and the overall average silhouette is calculated under all integer values between 2 and the maximum chosen. Then $K$ is chosen to maximize the average silhouette. The idea behind this criteria is to determine whether the within dissimilarities are small when compared to the between dissimilarities. They recognized that everything is relative and to create well-separated clusters, both components need to be taken into account.\\\\

\subsection{Model-based methods}
Partitioning algorithm do just that; they partition the data into non-overlapping and non-empty sets. Therefore, each subject is in one and only one cluster. This is true if the point is near the centroid or medoid or near the edge of two groups. This is referred to as hard clustering in contrast to soft clustering where each subject can be in multiple groups with varying degrees. The subject on the edge of two groups can then contribute to both of those clustering equally and the clustering results indicate that there is uncertainty in group membership. Although there are fuzzy partitioning methods that allow for soft clustering, I focus on model-based methods for this property. By assuming a probability distribution for the data, there is a framework in which to estimate the probability of group membership as well as to estimate and make inferences on the relationship between covariates and group membership. The main model used for clustering is the finite mixture model. 

\subsubsection{Mixture Model}
In a finite multivariate mixture, the density of a random vector $\B y$ takes the form
$$f(\B y)=\pi_{1}f_{1}(\B y)+\cdots+\pi_{K}f_{K}(\B y)$$
where $\pi_{k}>0$ for $k=1,...,K$ and $\sum^{K}_{k}\pi_{k}=1$. The parameters $\pi_{k}$ are mixing proportions and the functions $f_{1},...,f_{K}$ are component densities. In many situations, these densities are assume multivariate Gaussian with mean $\bs\mu_{k}$ and covariance matrix $\bs\Sigma_{k}$. That is, the conditional mixture density for $\B y$ is defined by
\begin{align}
f(\B y|\bs \theta)=\sum^{K}_{k=1}\pi_{k}f_{k}(\B y|\bs\theta_{k}) \label{mixdens}
\end{align}
where $f_{k}(\B y|\bs\theta_{k})$ denotes the $m$-variate Gaussian probability density function with mean $\bs\mu_{k}$ and covariance matrix $\bs\Sigma_{k}$ $(k=1,...,K)$, $\bs\theta_{k}$ includes both $\bs\mu_{k}$ and $\bs\Sigma_{k}$. 
\subsubsection{EM Algorithm}
Under the assumption that $\B y_{1},...,\B y_{n}$ are independent realizations from the mixture distribution, $f(\B y | \bs\theta)$, defined in \ref{mixdens}, the log likelihood function for the parameter vector, $\bs \theta$ is given by
$$\log L(\bs\theta)=\sum^{n}_{i=1}\log f(\B y_{i}|\bs \theta).$$
The ML estimate of $\bs\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\bs\theta)/\partial \bs\theta=\B 0.$
Solutions of this equation corresponding to local maxima can be found iteratively through the Expectation-Maximization (EM) algorithm \cite{dempster1977}. This algorithm is applied in the framework where each $\B y_{i}$ is assumed to have stemmed from one of the components and the indicator denoting its originating component is missing. The complete-data log likelihood is based on these indicator variables as well as the observed data $\{(\B y_{i}\}$. The Expectation step (E-step) involves replacing the indicators by current values of the conditional expectation, which is the posterior probability of component membership, written as
$$\alpha_{ik}=\pi_{k}f_{k}(\B y_{i}|\bs\theta_{k})/\sum_{j=1}^{K}\pi_{j}f_{j}(\B y_{i}|\bs \theta_{j})$$
for $i=1,...,n$ and $k=1,...,K$ using current estimates of the parameters. In the Maximization step (M-step), the parameter estimates for the mixing proportions, means, and covariance matrices are updated by maximizing the complete-data log likelihood using the posterior probabilities from the E-step in place of the indicator variables using numerical optimization. The E- and M-steps are alternated repeatedly until convergence. The EM algorithm guarantees convergence to a local maximum; global convergence may be attained through initializing the algorithm by randomly assigning individuals to initial components, running the algorithm multiple times and using the estimates associated with the highest log likelihood.\\\\
The EM algorithm for the mixture of Gaussians is quite similar to K-means. By letting $\Sigma_{k} =\sigma I$ with fixed $\sigma$, the M-step is equivalent to the update step in the K-means algorithm. As $\sigma\rightarrow 0$, soft clustering becomes hard clustering as the posterior probabilities for each individual calculated in the E-step converge to zero for all but one cluster which converges to one. 

\subsubsection{Choosing the number of components}
Just as with partitioning methods, it is difficult to choose the number of components, $K$, and the problem has not been completely resolved \cite{mclachlan2000}. Often mixture models are used for nonparametric density estimation and in this context, Roeder and Wasserman have shown that the density estimate that uses the Bayesian Information Criteria (BIC) is consistent \cite{roeder1997}. However, in the context of clustering, the solution is not as clear. If the shape of clusters is not exactly Gaussian, since Gaussian mixtures are used for density estimation, a model selection criteria may suggest a larger number of components so as to model the cluster better. For the sake of parsimony, the smallest number of components that fit the data is chosen.

BIC approximates the integrated likelihood \cite{schwarz1978}, but the regularity conditions do not hold for mixtures. Nevertheless, Raftery and Fraley note considerable theoretical and practical evidence to support the use in this context \cite{fraley1998}. If the densities assumptions do not hold, Biernacki et. al. \cite{biernacki2000} have been found that it tends fit too many components. 

ICL-BIC - based on the integrated classification likelihood to overcome shortcomings of the BIC. this approximation is only appropriate for large sample sizes, but has found the performance differs little from the more accurate version \cite{biernacki2000}.


\section{Thesis Outline}
This thesis is organized in the following fashion. In Chapter 2, I develop a series of simulations to study the impact of misspecifying the covariance structure in finite mixture models and present finite-sample and asymptotic bias results. In Chapter 3, I motivate the rest of the thesis by presenting the general clustering methodology for longitudinal data and illustrate how these methods do not cluster based on the shape of change over time. Chapter 4 presents three proposed methods to cluster based solely on the shape and compares them to standard methods through a simulation study. Chapter 5 applies these methods to a data example. In Chapter 6, I will overview the results of the thesis, discuss the contributions, and present areas of future work. 
\section{Contributions}
This thesis contributes the following to the field:
\begin{itemize}
\item highlights the impact of misspecifying the covariance structure in finite mixture models in terms of bias and explore the behavior of this bias
\item presents practical advice to determine how to choose a covariance model
\item highlights the need for methodology to cluster longitudinal data based on the shape and patten of change
\item presents three proposed models to cluster trajectories on the basis of shape
\item compare these methods with standard clustering methods under a simulation study
\item apply these methods to hormone trajectories
\end{itemize}
\end{document}