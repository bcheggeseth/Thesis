\chapter{Introduction}

This thesis treats the general problem of clustering longitudinal data. Both statisticians, machine learning communities, and applied researchers have investigated this problem with reference to time series, longitudinal, and functional data. In this thesis, I focus on longitudinal data which is characterized by time ordered vectors of outcomes with temporal autocorrelation and sparse, irregular time sampling. Longitudinal studies play a prominent role in health, social and behavioral sciences as well as in the biological sciences, economics, and marketing. By following subjects over time, temporal changes in an outcome of interest can be directly observed and studied. I refer to a time-ordered vector of outcomes as a trajectory throughout this thesis. One common technique to visually explore and reduce the dimension of the data is to cluster subjects with similar trajectory patterns. There are conventional methods to cluster multivariate data and most of these methods can be classified into two approaches: dissimilarity-based and model-based methods. The first approach groups subjects based on a selected measure of dissimilarity by using partition algorithms such as K-means \cite{macqueen1967, hartigan1979} and K-medoids/partitioning around medoids \cite{kaufman1990}, or hierarchical clustering algorithms when there is a priori scientific knowledge of a hierarchical structure \cite{murtagh1983}. The later approach involves a finite mixture to model the heterogeneity in the data. Mean trajectories for each cluster and group membership probabilities for each subject are calculated by using maximum likelihood estimation for the model parameters \cite{mclachlan1988,mclachlan2000,everitt1981}. Both of these approaches are widely used and the pros and cons of the approaches have been discussed in depth \cite{magidson2002, everitt1981}.

The bulk of the standard clustering methods are only applicable to data with independent measurements and therefore are not appropriate to use on longitudinal data \cite{everitt2009}. Recent work has focus on extending conventional clustering methods by forming measures of dissimilarity that are sensitive to permutations in the time order \cite{chouakria2007}, forcing a functional basis on the mean structure \cite{nagin1999,gaffney1999}, and explicitly modeling the temporal correlation structure \cite{muthen1999,fraley1999,mcnicholas2010}. Others have used functional data analysis methods \cite{ramsay2002,ramsay2005} to project the data into a given functional space and then calculate the dissimilarity measure using the fitted curves or coefficients \cite{serban2005, tarpey2003, abraham2003, tarpey2007,hitchcock2007}.

\section{Longitudinal data}
In classical univariate statistics, each of a number of individuals, or subjects, gives rise to a single measurement, termed the outcome. In multivariate statistics, that one measurement is replaced by a vector of different measurements. In longitudinal studies, a vector of measurements are observed for each subject, but now they represent the same quantity measured at a sequence of observation times. Therefore, the data has characteristics of both multivariate and time series data. But, they differ from multivariate data in that the time ordering imparts a much more highly structure pattern of interdependence among measurements than a typical multivariate data set. They differ from classical time series data because the data consists of a large number of independent short trajectories, one for each subject, rather than a few, long dependent series. Functional data is a fairly new category \cite{ramsay2005} where the data for a subject are assumed to be noisy observations from a smooth random function. If the observed times are a dense sample of the time period of interest, it may be reasonable to assume that a data set could be classified as both longitudinal and functional. However, typical longitudinal data sets have sparse and irregular observation times, which make it difficult to estimate each subjects' function. If we assume that there are common attributes among the subjects, we can borrow strength across people in the modeling stage to make up for the sparsity in the time sampling. 

Turning to notation, I let $Y_{ij}$ represent an outcome random variable and $\B x_{ij}$ a vector of length $p$ of explanatory variables observed at time $t_{ij}$, for observation $j=1,...,m_{i}$ on individual $i=1,...,n$. Also, i let $\B z_{i}$ be a vector of length $q$ of baseline variables observed at or before $t_{i1}$.The set of repeated outcomes for individual $i$ together in a vector is denoted as $\B Y_{i} = (Y_{i1},...,Y_{in_{i}})$ with mean $E(\B Y_{i}) = \BS\mu_{i}$ and $n_{i}\times n_{i}$ covariance matrix $Var(\B Y_{i}) = \B\Sigma_{i}$. I denote $\B y_{i}$ as the observed outcome vectors. 

\section{Cluster analysis}
One of the most natural abilities of human beings involves the grouping and sorting objects into categories. Young children learning how to distinguish between objects by naming them: shapes, colors, types of animals. Group labels represent a convenient way of summarizing a large amount of information so that it can be understood more easily. This is even more vital when summarizing a large data set so that information can be retrieved more efficiently by providing a concise description of patterns of similarities and differences in the data. Cluster analysis is one tool to explore groups within a data set and it has found its way into various scientific disciplines: from mathematics and statistics to biology and genetics to economics and market research, each of which uses different terminology to describe the grouping process. It is called numerical taxonomy in biology, unsupervised pattern recognition in the machine learning literature, and data segmentation in market research, but the problem is same---to find groups of similar objects.

The problem is well-defined, but the solution is not. There is no agreed upon criteria that determines one grouping is better than another. In general, most statisticians agree that clusters should be formed by maximizing the similarity within groups and maximizing the dissimilarity between groups. Some believe that there are naturally occurring groups that need to be discovered, but other remark that cluster analysis can always find clusters even if there is no true underlying groups. However, Bonner \cite{bonner1964} suggests that the user has the ultimate judgement on meaning and value of the clusters. Clusters should not be judged on whether they are true or false but rather in their usefulness. No method works on every data set to find meaningful groups. For this reason, there is a wealth of clustering algorithms proposed in the literature. In this thesis, I maintain the general goal of clustering is to maximize similarity within groups understanding that the analysis is highly dependent on how the user determines individuals to be similar. In this way, clusters are meaningful to the user while maintaining the basic mathematical philosophy of clustering.  I overview two approaches to clustering as they provide a basis of the methods discussed in this thesis. 

\subsection{Dissimilarity-based methods}
Since clustering is the grouping of similar individuals, a quantitative measure to determine the extent to which two subjects are similar or dissimilar is required.  There are two main types of measures: distances metrics and dissimilarity measures. A distance metric is a function that satisfies four mathematical properties. For two vectors $\B x$ and $\B y$ such that $\B x,\B y\in\mathbb{R}^{m}$, $d(\B x,\B y)$ is a metric if the following conditions hold:
\begin{align*}
d(\B x,\B y)&\geq 0 \\
d(\B x,\B y) &= 0\text{ if and only if } \B x=\B y\\
d(\B x,\B y) &= d(\B y,\B x)\\
d(\B x,\B z) &\leq d(\B x,\B y) + d(\B y,\B z)\text{ for any }\B z\in \mathbb{R}^{m}
\end{align*}
The first three conditions express intuitive notions about the concept of distance. The distance between two objects is positive and only can be zero when comparing the same vectors. Therefore, the fourth condition, also known as the triangle inequality, determines whether a function is a metric. The most commonly used metric for continuous data is the Minkowski distance of order $p$ defined as
$$d(\B x,\B y) = \|\B x - \B y\|_{p} = \left(\sum^{m}_{i=1}|x_{i}-y_{i}|^{p}\right)^{1/p}$$
where $\B x = (x_{1},...,x_{m})$ and $\B y = (y_{1},...,y_{m})$. Typically, the Minkowski distance is used with order $p=1$ or $p=2$. The latter is defined the the Euclidean distance and the former is usually called the Manhattan or taxi cab distance. As $p$ increases to infinity, we obtain the Chebychev distance. In general, the measurement unit of the vector elements used can affect the clustering analysis. To avoid the dependence on the choice of measurement units, it is recommend to standardize each element over all of the data. However, in the context of longitudinal data, the elements are repeated measures and all have the same units so this is not necessary in this case. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Other distance measures are based on variants of the correlation coefficient. The distance measure based on the Pearson correlation coefficient is defined as
$$d(\B x,\B y) =1 - \frac{(\B x - \bar{x})^{T}(\B y - \bar{y})}{\|\B x - \bar{x}\|_{2}\|\B y - \bar{y}\|_{2}}$$
where $\bar{x}$ represents the average of the elements in $\B x$. This range of this distance is $[0,2]$ where 2 represents perfect negative linear correlation, 0 indicates perfect positive linear correlation, and 1 denotes no linear correlation. The Pearson correlation coefficient can be replaced with its uncentered version to calculate the cosine angle distance,
$$d(\B x,\B y) =1 - \frac{\B x ^{T}\B y }{\|\B x\|_{2}\|\B y \|_{2}}.$$
When the cosine of the angle between the vectors is 0, the distance equals 0, and the maximum value is 2. Both of these distance measures are invariant to scaling such that $d(\B x, a\cdot \B x) = 0$ for $a>0$ and the Pearson correlation distance is additionally invariant to shifts such that $d(\B x, a\cdot \B x +b) = 0$ for $a>0$ and $b\in \mathbb{R}$. 

It is also important to note that these distances require vectors of equal length and are invariant to permutation of the order of the elements in the vector.

\subsubsection{Algorithms}
Clustering algorithms aim to optimize a criterion based on the chosen dissimilarity measure. The choice of the criterion needs to be made after careful study as it can have a dramatic effect on the final clustering result. The algorithm then attempts to globally maximize the selected criterion. Most clustering algorithms can be classified into two group according to their general search strategies: hierarchical and partitioning algorithms. Hierarchical methods involve constructing a tree of clusters in which the root node is a cluster containing all objects and the leaves are clusters each containing one object. The tree can be constructed in a divisive manner (i.e., top down by recursively dividing groups) or agglomerative manner (i.e., bottom up by recursively combining groups). In order to combine groups, there are different ways of measuring the distance between groups of objects: single-linkage, complete-linkage, average-linkage. This approach is best used when there a priori scientific knowledge of a hierarchical structure in the data.

Partition methods aim to map objects into $k\geq 2$ groups by maximizing a criterion without any larger hierarchical structure. Two popular methods include K-means \cite{macqueen1967} and partitioning around medoids (PAM) \cite{kaufman1990}. K-means is one of the simplest unsupervised learning algorithms used to solve the well-known clustering problem. Given a set of vectors $(\B y_{1},\B y_{2},...,\B y_{n})$, where $\B y_{i}\in\mathbb{R}^{m}$, the K-means clustering algorithm aims to partition the $n$ observational units into $K$ sets $\{C_{1},...,C_{K}\}$ so as to minimize the sum of squares from points to the assigned cluster centroids denoted below
$$\min_{\{C_{k}\}_{k=1}^{K}}\sum_{k=1}^{K}\sum_{i: \B y_{i}\in C_{k}}\|\B y_{i} -\BS \mu_{k}\|_{2}^{2}$$
where $\BS\mu_{1},...,\BS\mu_{K}$ are the centroids, mean vectors, of the $K$ clusters. To find the sets $\B C$ that minimize the criterion, $K$ individuals are strategically or randomly chosen as the centroids. The general algorithm proceeds by alternating between two steps:
\begin{itemize}
\item Assignment step: Assign each observation to the cluster with the closest centroid
$$ C_{k} = \{\B y_{i}: \|\B y_{i} - \BS\mu_{k}\|_{2}^{2}\geq \|\B y_{i} - \BS\mu_{j}\|_{2}^{2} \;\;\forall\;\; 1\leq j\leq k\}$$
such that ever $y_{i}$ is in one and only set.
\item Update step: Calculate the centroid of the new cluster.
$$\BS\mu_{k} = \frac{1}{|C_{k}|}\sum_{i: \B y_{i}\in\B C_{k}} y_{i}$$
\end{itemize}
The algorithm continues to iterate until the clusters no longer change. Depending on the initial partition, the algorithm is expected to converge to local optima; therefore, completing multiple random starts offers the best way to obtain a global optimum. The K-means algorithm works best when data clusters are about equal size and shape. Since the K-means algorithm it that is based on the squared Euclidean distanc, the algorithm attempts to find ``spherical'' clusters. Also, the calculation of the centroids is sensitive to outliers and thus the centroid may not be representative of any of the cluster members.

Partitioning around medoids (PAM) attempts to overcome some of the issues with K-means. The algorithm operates on a user-provided dissimilarity matrix rather than only Euclidean distances. It is robust to outliers since the representative curves are selected from the observed data vectors rather than based on mean calculations. To find $K$ sets of points to minimize the sum of dissimilarity from the points to their respective medoids, $K$ individuals are randomly chosen as medoids denoted as $\B m_{1},...,\B m_{K}$. The algorithm alternates between the following two steps:
 \begin{itemize}
\item Build step: Assign each observational unit to the closest medoid with respect to the given dissimilarity matrix.
\item Swap step: For each $k=1,...,K$, swap the medoid, $m_{k}$, with each non-medoid observation and compute the sum of the dissimilarities of the points to their closest medoid. Select the configuration with the smallest sum of dissimilarities.
\end{itemize}
The algorithm continues until there is not change in the medoids. Although it has a longer run time, this building and swapping procedure returns a smaller sum of dissimilarity in contrast to a simple assignment and update procedure as in the K-means algorithm. 

\subsubsection{Choosing the number of components}
A major challenge in clustering is to determine the optimal number of clusters. For the partition methods, a maximum number of clusters is chosen $K_{max}<n$ and the algorithm is run for each value $K=2,3,....,K_{max}$. Some take a direct approach to choose the optimal $K$  by optimizing functions of within and between cluster dissimilarity \cite{mulligan1985} or the average silhouette \cite{kaufman1990}. Others take a testing approach by comparing cluster summaries with its expectation under an appropriate null distribution such as the Gap statistic \cite{tibshirani2001} or the CLEST approach \cite{dudoitfridlyand2002}. The average silhouette has the advantage working with any cluster routine and dissimilarity measure. For each vector $i$, the silhouette, $s(i)$, is calculated defined as
$$s(i)=\frac{b(i)-a(i)}{\max\{a(i),b(i)\}}$$
where $a(i)$ is the average dissimilarity of vector $i$ to all other points in its cluster, $A$, $d(i,C)$ is average dissimilarity of vector $i$ to all points in cluster $C$ and $b(i)=\min_{C\not= A} d(i,C)$. The overall average silhouette width, $\bar{s}$, is the average of $s(i)$ over all points in the data set. A maximum value for the number of clusters, $K$, is chosen such that $K<n$ and the given algorithm is run and the overall average silhouette is calculated under all integer values between 2 and the maximum chosen. Then $K$ is chosen to maximize the average silhouette. The idea behind this criteria is to determine whether the within dissimilarities are small when compared to the between dissimilarities. 



\subsection{Model-based methods}
For a chosen number of clusters, $K$, partitioning and hierarchical algorithms partition data into non-overlapping, non-empty sets. Each subject is in one and only one cluster. This is referred to as hard clustering in contrast to soft or fuzzy clustering where each subject can be in multiple groups to varying degrees. This differentiation impacts subjects that are mathematically near the edge between two groups. With hard clustering, those subjects are forced in one or the other group. With soft clustering, those subject on the edge can contribute to both clusters equally and the clustering results include association levels that indicate the uncertainty in group membership. Probabilistic model-based methods allow for soft clustering with the possibility of formal inference \cite{fraley2002}. Assuming a probability distribution for the data provides a framework in which to estimate the probability of group membership as well as to estimate and make inferences on the relationship between covariates and group membership. The main model used for clustering is the finite mixture model.

\subsubsection{Finite mixture model}
In a general finite mixture, the density of a random variable $\B y$ takes the form
$$f(\B y|\BS\theta)=\pi_{1}f_{1}(\B y|\BS\theta_1)+\cdots+\pi_{K}f_{K}(\B y|\BS\theta_K)$$
where $f_k$ and $\bs\theta_k$ are the density and parameters of the $k$th component and $\pi_{k}$ is the probability an observation belongs to the $k$th component ($\pi_{k}>0$; $\sum^{K}_{k}\pi_{k}=1$). The full parameter vector $\BS\theta$ includes the prior probabilities $\pi_k$ and the component parameters $\BS\theta_k$ for each component. In many situations, the component densities are assume multivariate Gaussian parameterized by its mean $\BS\mu_{k}$ and covariance matrix $\BS\Sigma_{k}$,
$$f_k(\B y| \BS\mu_k,\BS\Sigma_k) = \frac{\exp\left(-\frac{1}{2}(\B y - \BS\mu_k)^T\BS\Sigma_k^{-1}(\B y-\BS\mu_k)\right){\sqrt{\det(2\pi \BS\Sigma_k)}}.$$
This general model has possibilities for more complexity by further parameterizations. The mean vector can include linear dependence on a design matrix based on explanatory variables $\B x$ such that $\mu_k = \B x \BS \beta_k$. The covariance matrix can be parameterized through the eigenvalue decomposition in the form
$$\BS\Sigma_k=\lambda_k D_kA_kD_k^T$$
where $D_k$ is the orthogonal matrix of eigenvectors, $A_k$ is a diagonal matrix whose elements are proportional to the eigenvalues, and $\lambda_k$ is a proportional constant \cite{banfield1993} or the matrix can be simplified assuming a structure such as independence, compound symmetry, or exponential covariance structure. Lastly, if baseline factors $\B z$ influence group membership, the prior group probabilities can be parameterized using a generalized logit model
$$\pi_k(\B z, \BS\gamma) = \frac{\exp(\B z^T \BS \gamma_k) }{\sum_{j=1}^K \exp(\B z^T \BS \gamma_j)}.$$
The full parameter vector $\BS\theta$ includes all model parameters such as $\BS\beta_k$ and $\BS\gamma_k$.

\subsubsection{Expectation maximization algorithm}
Under the assumption that $\B y_{1},...,\B y_{n}$ are independent realizations from the mixture distribution, $f(\B y | \BS\theta)$, defined above, the log-likelihood function for the parameter vector, $\BS \theta$ is given by
$$\log L(\BS\theta)=\sum^{n}_{i=1}\log f(\B y_{i}|\BS \theta).$$
The maximum likelihood estimate of $\BS\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\BS\theta)/\partial \BS\theta=\B 0.$
Solutions of this equation corresponding to local maxima can be found iteratively through the expectation-maximization (EM) algorithm \cite{dempster1977}. This algorithm is applied in the framework where each $\B y_{i}$ is assumed to have stemmed from one of the components and the indicator denoting its originating component is missing. The complete-data log likelihood is based on these indicator variables as well as the observed data $\{(\B y_{i}\}$. The expectation step (E-step) involves replacing the indicators by current values of the conditional expectation, which is the posterior probability of component membership, written as
$$\alpha_{ik}=\pi_{k}f_{k}(\B y_{i}|\BS\theta_{k})/\sum_{j=1}^{K}\pi_{j}f_{j}(\B y_{i}|\BS \theta_{j})$$
for $i=1,...,n$ and $k=1,...,K$ using current estimates of the parameters. In the maximization step (M-step), the parameter estimates for the mixing proportions, means, and covariance matrices are updated by maximizing the complete-data log likelihood using the posterior probabilities from the E-step in place of the indicator variables using numerical optimization. The E-step and M-step are alternated repeatedly until convergence. The EM algorithm guarantees convergence to a local maximum; global convergence may be attained through initializing the algorithm by randomly assigning individuals to initial components, running the algorithm multiple times and using the estimates associated with the highest log likelihood.

Once the algorithm has converged, the posterior probabilities for subject $i$ provide the strength of group association for the soft clustering. This clustering can be translated into a hard clustering by choosing group labels that maximizes the posterior probability, $\arg\max_k \alpha_{ik}$. Then, one measure of uncertainty in the classification is $(1-\max_k\alpha_{ik}).$ A variant of the EM algorithm called the classification EM \cite{celeuxgovaert1992}, in which the posterior probabilities, $\alpha_{ik}$, are converted to discrete classification before the M-step, is equivalent to the K-means algorithm when $\Sigma_{k} =\sigma I$ in a Gaussian mixture. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Choosing the number of components}
Just as with partitioning methods, it is difficult to choose the number of components, $K$, and the problem has not been completely resolved \cite{mclachlan2000}. Often mixture models are used for nonparametric density estimation and in this context, Roeder and Wasserman have shown that the density estimate that uses the Bayesian Information Criteria (BIC) is consistent \cite{roeder1997}. However, in the context of clustering, the solution is not as clear. If the shape of clusters is not exactly Gaussian, since Gaussian mixtures are used for density estimation, a model selection criteria may suggest a larger number of components so as to model the cluster better. For the sake of parsimony, the smallest number of components that fit the data is chosen.

BIC approximates the integrated likelihood \cite{schwarz1978}, but the regularity conditions do not hold for mixtures. Nevertheless, Raftery and Fraley note considerable theoretical and practical evidence to support the use in this context \cite{fraley1998}. If the densities assumptions do not hold, Biernacki et. al. \cite{biernacki2000} have been found that it tends fit too many components. 

ICL-BIC - based on the integrated classification likelihood to overcome shortcomings of the BIC. this approximation is only appropriate for large sample sizes, but has found the performance differs little from the more accurate version \cite{biernacki2000}.

\section{Thesis outline}
This thesis is organized in the following fashion. In Chapter 2, I develop a series of simulations to study the impact of misspecifying the covariance structure in finite mixture models and present finite-sample and asymptotic bias results. In Chapter 3, I motivate the rest of the thesis by presenting the general clustering methodology for longitudinal data and illustrate how these methods do not cluster based on the shape of change over time. Chapter 4 presents three proposed methods to cluster based solely on the shape and compares them to standard methods through a simulation study. Chapter 5 applies these methods to a data example. In Chapter 6, I will overview the results of the thesis, discuss the contributions, and present areas of future work. 

\section{Contributions}
This thesis contributes the following to the field:
\begin{itemize}
\item highlights the impact of misspecifying the covariance structure in finite mixture models in terms of bias and explore the behavior of this bias
\item presents practical advice to determine how to choose a covariance model
\item highlights the need for methodology to cluster longitudinal data based on the shape and patten of change
\item presents three proposed models to cluster trajectories on the basis of shape
\item compare these methods with standard clustering methods under a simulation study
\item apply these methods to hormone trajectories
\end{itemize}
