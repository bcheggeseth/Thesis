\chapter{Introduction}
\label{chap: intro}

This thesis treats the task of clustering longitudinal data. Statisticians, machine learning communities, and applied researchers have investigated this problem for many highly structured multivariate data sets such as time series, functional data, as well as longitudinal data. While all of these data structures are ordered by a variable such as time, they have different properties that call for different methods. Time series are typically characterized as sequences of data points measured at successive points at uniform into intervals for a few random processes. Examples include the monthly unemployment figures, yearly global temperatures, and hourly stock prices over time \cite{shumway2011}. Functional data is generally assumed to represent smooth curves or functions that have been measured of a densely sampled grid. The data may include many curves, one for each observational unit, and may be observed with substantial measurement error. Data sets such as hip angles over walking cycles for 29 children and the height of 10 girls observed 31 times in 17 years that have underlying smooth curves are categorized as functional data \cite{ramsay2002}. In this thesis, I focus on longitudinal data which includes repeated measurements of the same outcomes observed on many subjects at sparse, irregular times over a potentially long period of time. Well-known longitudinal studies include the National Longitudinal Survey of Youth, the Framington Heart Study, and the Millennium Cohort Study in the U.K. 

Longitudinal studies play a prominent role in health, social and behavioral sciences as well as in the biological sciences, economics, and marketing. By following subjects over time, temporal changes in an outcome of interest can be directly observed and studied. I often refer to a time-ordered vector of outcomes as a trajectory as a reminder that the data vectors are ordered and represent a path of development over time. 

An important question concerns the existence of distinct trajectory patterns. Cluster analysis seeks to separate objects (subjects, patients, observational units) into homogeneous clusters. There are many ways to cluster multivariate data and there is multitude of methods and algorithms available to use. Most of these methods can be categorized into two approaches: non-parametric and model-based methods. The first approach makes no assumptions about how the data were generated and produces a sequence of clustering results index by the number of clusters $k=2,3,...$ and the choose of dissimilarity measure. The later approach assumes data vectors are \emph{i.i.d.} from a finite mixture of distributions. The clustering results include measures, such as the conditional probability of group membership and cluster parameters, of the maximum likelihood estimator of the assumed data generation distribution \cite{mclachlan2000,fraley1998}. Both of these approaches are widely used and the pros and cons of the approaches have been discussed in depth \cite{magidson2002, everitt1981}.

The bulk of the available clustering methods are intended for use on data vectors with exchangeable, independent elements and are not appropriate to use on longitudinal data \cite{everitt2009}. Recent work to cluster longitudinal data has focused on extending and adjusting the standard clustering methods. Dissimilarity measures have been designed to be sensitive to permutations in the time order by incorporating neighboring relationships \cite{chouakria2007} and calculating dissimilarity between coefficients of a functional projection rather than the raw vectors \cite{serban2005, tarpey2003, abraham2003, tarpey2007,hitchcock2007}. In the model-based approach, one can incorporate a functional basis in the mean structure \cite{nagin1999,gaffney1999}, and explicitly model the temporal correlation structure \cite{muthen1999,fraley1999,mcnicholas2010}. 

\section{Longitudinal data}
In classical univariate statistics, each individual, or subject, gives rise to a single measurement, termed the outcome. In multivariate statistics, that one measurement is replaced by a vector of multiple outcome measurements. In longitudinal studies, a vector of measurements is observed for each subject, but the vector represents one outcome repeatedly measured at a sequence of observation times. Therefore, the data have characteristics of both multivariate and time series data.

Longitudinal data differ from multivariate data in that the time ordering imparts a highly structured pattern of interdependence among measurements that is not present in a typical multivariate data set. They differ from classical time series data because the data consists of a large number of independent, sparsely and irregularly sampled trajectories, one for each subject, rather than a few long, uniformly sampled dependent series. Functional data is a fairly new type \cite{ramsay2005} where the data for a subject are assumed to be noisy observations from a smooth random function. If the observed times are a dense sample of the time period of interest, it may be reasonable to assume that a data set could be classified as both longitudinal and functional. However, typical longitudinal data sets have sparse and irregular observation times, which make it difficult to estimate each subject's function without borrowing strength between individuals with similar trajectories/functions.

Turning to notation, I let $Y_{ij}$ represent an outcome random variable and $\B x_{ij}$ a design vector of length $p$ based on explanatory variables observed at time $t_{ij}$, for observation $j=1,...,m_{i}$ on individual $i=1,...,n$. Also, I let $\B w_{i}$ be a design vector of length $q$ based on baseline variables observed at or before $t_{i1}$. The set of repeated outcomes for individual $i$ together in a vector is denoted as $\B Y_{i} = (Y_{i1},...,Y_{in_{i}})$ with mean $E(\B Y_{i}) = \BS\mu_{i}$ and $n_{i}\times n_{i}$ covariance matrix $Cov(\B Y_{i}) = \BS\Sigma_{i}$, where the $jl$th element of $\BS\Sigma_{i}$ is the covariance between $Y_{ij}$ and $Y_{il}$. I use $\B R_{i}$ for the $n_{i}\times n_{i}$ correlation matrix of $\B Y_{i}$. I generally assume $Y_{i}$ and $Y_{i'}$ are independent for $i\not = i'$. In general, I will use capital letters to represent random variables or matrices, bold font for vectors and matrices, and small letters for specific observations. Hence, I denote $\B y_{i}$ as the observed outcome vector for individual $i$. 

\section{Cluster analysis}
One of the most natural abilities of human beings involves the grouping and sorting objects into categories. We simplify our wold by recognizing similarities and naming the groups. For example, when children first learn colors, different shades and hues of blue are all categorized as just blue. Similarly, golden retrievers, poodles, and terriers are all dogs. These group labels provide a convenient way of summarizing a large amount of information so that it can be understood more easily. This is even more vital when summarizing a large data set so that information can be retrieved more efficiently by providing a concise description of patterns of similarities and differences in the data. 

Cluster analysis is one tool to explore groups within a data set and it has found its way into various scientific disciplines: from mathematics and statistics to biology and genetics to economics and market research, each of which uses different terminology to describe the grouping process. It is called numerical taxonomy in biology, unsupervised pattern recognition in the machine learning literature, and data segmentation in market research, but the problem is same---to find groups of similar objects.
 
The problem is well-defined but the solution is not. There is no agreed upon criteria that determines that one grouping is better than another. In general, most statisticians agree that clusters should be formed by maximizing the similarity within groups and maximizing the dissimilarity between groups. Some believe that there are naturally occurring groups that need to be discovered, but others remark that cluster analysis can always find clusters even if there is no true underlying group structure. However, Bonner \cite{bonner1964} suggests that the user is the ultimate judge on meaning and value of the clusters. Clusters should not be judged on whether they are true or false but rather in their usefulness. No method works on every data set to find meaningful groups. For this reason, there is a wealth of clustering algorithms proposed in the literature. 

In this thesis, I maintain the general goal of clustering is to maximize similarity within groups understanding that the analysis is highly dependent on how the user determines individuals to be similar. In this way, clusters are meaningful to the user while maintaining the basic mathematical philosophy of clustering.  I overview two approaches to clustering as they provide a basis of the methods discussed in this thesis. 

\section{Nonparametric clustering methods}
Rather than making assumptions about the data generating distribution, the first clustering approach focus on explicitly defining similarity between subjects. The three ingredients to these methods are the dissimilarity measure, the algorithm and the number of clusters.

\subsection{Dissimilarity measures}
There are many terms used for a measure to quantify the distinctiveness of a pair of subjects. Metric, distance, dissimilarity, and similarity are all related concepts. A metric is any function, $d$, that satisfies the following five mathematical properties.
\begin{align*}
(i)& \quad d(\B x,\B y)\geq 0 \\
(ii)& \quad d(\B x,\B y) = d(\B y,\B x)\\
(iii)&\quad d(\B x,\B x) = 0\\
(iv)&\quad d(\B x,\B y) = 0\text{ if and only if } \B x=\B y\\
(v)&\quad d(\B x,\B z) \leq d(\B x,\B y) + d(\B y,\B z)
\end{align*}
A function that satisfies the first three conditions is termed a distance. The distance between two objects is positive and only can be zero when comparing the same vectors. The fifth condition, known as the triangle inequality, mainly determines whether a distance function is a metric. A similarity function satisfies the following three conditions.
\begin{align*}
(i)&\quad S(\B x,\B y)\geq 0 \\
(ii)&\quad S(\B x,\B y) = S(\B y,\B x)\\
(iii)& \quad S(\B x,\B y)\text{ increases in a monotone fashion as }\B x\text{ and }\B y\text{ are more similar}
\end{align*}
A dissimilarity function is the same except that the function increases as the vectors are more dissimilar. 

The most commonly used metric on Euclidean space is the Minkowski distance of order $p$ defined as
$$d(\B x,\B y) = \|\B x - \B y\|_{p} = \left(\sum^{m}_{j=1}|x_{j}-y_{j}|^{p}\right)^{1/p}$$
where $\B x = (x_{1},...,x_{m})$ and $\B y = (y_{1},...,y_{m})$. The distance equals the Euclidean distance when $p=2$,
$$||\B x - \B y||_{2}=\sqrt{\sum_{j=1}(x_{j}-y_{j})^{2}},$$
and the Manhattan or taxi cab distance when $p=1$,
$$||\B x - \B y||_{1}=\sum_{j=1}|x_{j}-y_{j}|.$$
As $p$ increases to infinity, the distance converges to the Chebychev distance,
$$||\B x - \B y||_{\infty}=\max_{j}|x_{j}-y_{j}|.$$

Other distance measures are based on variants of the correlation coefficient. The distance measure based on the Pearson correlation coefficient is defined as
$$d(\B x,\B y) =1 - \frac{(\B x - \bar{x})^{T}(\B y - \bar{y})}{\|\B x - \bar{x}\|_{2}\|\B y - \bar{y}\|_{2}}$$
where $\bar{x}$ represents the average of the elements in $\B x$. This range of this distance is $[0,2]$ where 2 represents perfect negative linear correlation, 0 indicates perfect positive linear correlation, and 1 denotes no linear correlation. The Pearson correlation coefficient can be replaced with its uncentered version to calculate the cosine angle distance,
$$d(\B x,\B y) =1 - \frac{\B x ^{T}\B y }{\|\B x\|_{2}\|\B y \|_{2}}.$$
When the cosine of the angle between the vectors is 0, the distance equals 0, and the maximum value is 2. Both of these distance measures are invariant to scaling such that $d(\B x, a\cdot \B x) = 0$ for $a>0$ and the Pearson correlation distance is additionally invariant to shifts such that $d(\B x, a\cdot \B x +b) = 0$ for $a>0$ and $b\in \mathbb{R}$. 

It is also important to note that these distances require vectors of equal length and are invariant to permutation of the order of the elements in the vector.

\subsection{Algorithms}
Clustering algorithms aim to optimize a criterion based on the chosen dissimilarity measure. The choice of the criterion needs to be made after careful study as it can have a dramatic effect on the final clustering result. The algorithm then attempts to globally maximize the selected criterion. Most clustering algorithms can be classified into two group according to their general search strategies: hierarchical and partitioning algorithms. Hierarchical methods involve constructing a tree of clusters in which the root node is a cluster containing all objects and the leaves are clusters each containing one object. The tree can be constructed in a divisive manner (i.e., top down by recursively dividing groups) or agglomerative manner (i.e., bottom up by recursively combining groups). In order to combine groups, there are different ways of measuring the distance between groups of objects: single-linkage, complete-linkage, average-linkage. This approach is best used when there a priori scientific knowledge of a hierarchical structure in the data.

Partition methods aim to map objects into $k\geq 2$ groups by maximizing a criterion without any larger hierarchical structure. Two popular methods include K-means \cite{macqueen1967} and partitioning around medoids (PAM) \cite{kaufman1990}. K-means is one of the simplest unsupervised learning algorithms used to solve the well-known clustering problem. Given a set of vectors $(\B y_{1},\B y_{2},...,\B y_{n})$, where $\B y_{i}\in\mathbb{R}^{m}$, the K-means clustering algorithm aims to partition the $n$ observational units into $K$ sets $\{C_{1},...,C_{K}\}$ so as to minimize the sum of squares from points to the assigned cluster centroids denoted below
$$\min_{\{C_{k}\}_{k=1}^{K}}\sum_{k=1}^{K}\sum_{i: \B y_{i}\in C_{k}}\|\B y_{i} -\BS \mu_{k}\|_{2}^{2}$$
where $\BS\mu_{1},...,\BS\mu_{K}$ are the centroids, mean vectors, of the $K$ clusters. To find the sets $\B C$ that minimize the criterion, $K$ individuals are strategically or randomly chosen as the centroids. The general algorithm proceeds by alternating between two steps:
\begin{itemize}
\item Assignment step: Assign each observation to the cluster with the closest centroid
$$ C_{k} = \{\B y_{i}: \|\B y_{i} - \BS\mu_{k}\|_{2}^{2}\geq \|\B y_{i} - \BS\mu_{j}\|_{2}^{2} \;\;\forall\;\; 1\leq j\leq k\}$$
such that ever $y_{i}$ is in one and only set.
\item Update step: Calculate the centroid of the new cluster.
$$\BS\mu_{k} = \frac{1}{|C_{k}|}\sum_{i: \B y_{i}\in\B C_{k}} y_{i}$$
\end{itemize}
The algorithm continues to iterate until the clusters no longer change. Depending on the initial partition, the algorithm is expected to converge to local optima; therefore, completing multiple random starts offers the best way to obtain a global optimum. The K-means algorithm works best when data clusters are about equal size and shape. Since the K-means algorithm it that is based on the squared Euclidean distance, the algorithm attempts to find ``spherical'' clusters. Also, the calculation of the centroids is sensitive to outliers and thus the centroid may not be representative of any of the cluster members.

Partitioning around medoids (PAM) attempts to overcome some of the issues with K-means. The algorithm operates on a user-provided dissimilarity matrix rather than only Euclidean distances. It is robust to outliers since the representative curves are selected from the observed data vectors rather than based on mean calculations. To find $K$ sets of points to minimize the sum of dissimilarity from the points to their respective medoids, $K$ individuals are randomly chosen as medoids denoted as $\B m_{1},...,\B m_{K}$. The algorithm alternates between the following two steps:
 \begin{itemize}
\item Build step: Assign each observational unit to the closest medoid with respect to the given dissimilarity matrix.
\item Swap step: For each $k=1,...,K$, swap the medoid, $m_{k}$, with each non-medoid observation and compute the sum of the dissimilarities of the points to their closest medoid. Select the configuration with the smallest sum of dissimilarities.
\end{itemize}
The algorithm continues until there is not change in the medoids. Although it has a longer run time, this building and swapping procedure returns a smaller sum of dissimilarity in contrast to a simple assignment and update procedure as in the K-means algorithm. 

\subsection{Choosing the number of components}
A major challenge in clustering is to determine the optimal number of clusters. For the partition methods, a maximum number of clusters is chosen $K_{max}<n$ and the algorithm is run for each value $K=2,3,....,K_{max}$. Some take a direct approach to choose the optimal $K$  by optimizing functions of within and between cluster dissimilarity \cite{mulligan1985} or the average silhouette \cite{kaufman1990}. Others take a testing approach by comparing cluster summaries with its expectation under an appropriate null distribution such as the Gap statistic \cite{tibshirani2001} or the CLEST approach \cite{dudoit2002}. The average silhouette has the advantage working with any cluster routine and dissimilarity measure. For each vector $i$, the silhouette, $s(i)$, is calculated defined as
$$s(i)=\frac{b(i)-a(i)}{\max\{a(i),b(i)\}}$$
where $a(i)$ is the average dissimilarity of vector $i$ to all other points in its cluster, $A$, $d(i,C)$ is average dissimilarity of vector $i$ to all points in cluster $C$ and $b(i)=\min_{C\not= A} d(i,C)$. The overall average silhouette width, $\bar{s}$, is the average of $s(i)$ over all points in the data set. A maximum value for the number of clusters, $K$, is chosen such that $K<n$ and the given algorithm is run and the overall average silhouette is calculated under all integer values between 2 and the maximum chosen. Then $K$ is chosen to maximize the average silhouette. The idea behind this criteria is to determine whether the within dissimilarities are small when compared to the between dissimilarities. 

\section{Model-based clustering methods}
For a chosen number of clusters, $K$, partitioning and hierarchical algorithms partition data into non-overlapping, non-empty sets. Each subject is in one and only one cluster. This is referred to as hard clustering in contrast to soft or fuzzy clustering where each subject can be in multiple groups to varying degrees. This differentiation impacts subjects that are mathematically near the edge between two groups. With hard clustering, those subjects are forced in one or the other group. With soft clustering, those subject on the edge can contribute to both clusters equally and the clustering results include association levels that indicate the uncertainty in group membership. Probabilistic model-based methods allow for soft clustering with the possibility of formal inference \cite{fraley2002}. Assuming a probability distribution for the data provides a framework in which to estimate the probability of group membership as well as to estimate and make inferences on the relationship between covariates and group membership. The main model used for clustering is the finite mixture model.

\subsection{Finite mixture model}
In a general finite mixture, the density of a random variable $\B y$ takes the form
$$f(\B y|\BS\theta)=\pi_{1}f_{1}(\B y|\BS\theta_1)+\cdots+\pi_{K}f_{K}(\B y|\BS\theta_K)$$
where $f_k$ and $\bs\theta_k$ are the density and parameters of the $k$th component and $\pi_{k}$ is the probability an observation belongs to the $k$th component ($\pi_{k}>0$; $\sum^{K}_{k}\pi_{k}=1$). The full parameter vector $\BS\theta$ includes the prior probabilities $\pi_k$ and the component parameters $\BS\theta_k$ for each component. In many situations, the component densities are assume multivariate Gaussian parameterized by its mean $\BS\mu_{k}$ and covariance matrix $\BS\Sigma_{k}$,
$$f_k(\B y| \BS\mu_k,\BS\Sigma_k) = \frac{\exp\left(-\frac{1}{2}(\B y - \BS\mu_k)^T\BS\Sigma_k^{-1}(\B y-\BS\mu_k)\right)}{\sqrt{\det(2\pi \BS\Sigma_k)}}.$$
This general model has possibilities for more complexity by further parameterizations. If the mean outcome value are thought to depend on explanatory variables, the mean vector parameter is replaced $\mu_k = \B x \BS \beta_k$ such that $\B x$ is a design matrix based on those variables that impact the mean. The covariance matrix can be parameterized through the eigenvalue decomposition in the form
$$\BS\Sigma_k=\lambda_k D_kA_kD_k^T$$
where $D_k$ is the orthogonal matrix of eigenvectors, $A_k$ is a diagonal matrix whose elements are proportional to the eigenvalues, and $\lambda_k$ is a proportional constant \cite{banfield1993} or the matrix can be simplified assuming a structure such as independence, compound symmetry, or exponential covariance structure. Lastly, if baseline factors $\B w$ are though to influence group membership, the prior group probabilities can be parameterized using a generalized logit model
$$\pi_k(\B w,\BS\gamma)= \frac{\exp(\B w^T \BS \gamma_k) }{\sum_{j=1}^K \exp(\B w^T \BS \gamma_j)}$$
where $\BS\gamma_{K}=\B 0$. The full parameter vector $\BS\theta$ includes all model parameters such as $\BS\beta_k$ and $\BS\gamma_k$.

\subsection{Expectation maximization algorithm}\label{subsec:em}
Under the assumption that $\B y = \{\B y_{1},...,\B y_{n}\}$ are independent realizations from the mixture distribution, $f(\B y | \BS\theta)$, defined above, the log-likelihood function for the parameter vector, $\BS \theta$ is given by
$$\log L(\BS\theta)=\sum^{n}_{i=1}\log f(\B y_{i}|\BS \theta).$$
The maximum likelihood estimate of $\BS\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\BS\theta)/\partial \BS\theta=\B 0.$ Solutions of this equation corresponding to local maxima can be found iteratively through the expectation-maximization (EM) algorithm \cite{dempster1977}, which was originally presented in the context of an incomplete data problem. In the clustering case, $\B y_{i}$ is assumed to have stemmed from one of the components and the label denoting its originating component is missing. Let $z_{ik}$ equal 1 if $\B y_{i}$ was generated from component $k$ and 0 otherwise. The component label vectors $\B z_{i}=(z_{i1},...,z_{iK})$ ($i=1,..,n$) are assumed to be realizations of random vectors $\B Z_{i}=(Z_{i1},...,Z_{iK})$ that follow the categorical distribution. The complete-data log likelihood, based on these component labels as well as the observed data $\B y$, is given by
$$\log L_{c}(\BS \theta) =  \sum^{K}_{k=1}\sum^{n}_{i=1}z_{ik}\left[\log \pi_{k}+\log f_{k}(\B y_{i}|\BS\theta_k)\right].$$

The EM algorithm treats the $z_{ik}$ as missing data and iteratively imputes the missing values and then  estimates the parameters. The expectation step (E-step) involves taking the conditional expectation of the complete log-likelihood, given the data $\B y$ and the current value of the parameter estimates. Since the complete log-likelihood is linear in the unobserved data $z_{ik}$, this step on the $t$th iteration simply involves calculating the conditional expectation of $Z_{ik}$ given the observed data,
\begin{align*}
E_{\BS\theta^{(t-1)}}( Z_{ik}|\B y) &= P_{\BS\theta^{(t-1)}}( Z_{ik}|\B y)\\
&=\alpha_{ik}
\end{align*}
using current estimates of the parameters. The quantity $\alpha_{ik}$ is the posterior probability that the $i$th individual belongs to component $k$, written as
$$\alpha_{ik}=\pi_{k}f_{k}(\B y_{i}|\BS\theta_{k})/\sum_{j=1}^{K}\pi_{j}f_{j}(\B y_{i}|\BS \theta_{j})$$
for $i=1,...,n$ and $k=1,...,K$.

In the maximization step (M-step), the parameter estimates for the prior probabilities, means, and covariance matrices are updated by maximizing the conditional expectation of the complete-data log-likelihood from the E-step. The updated estimates $\pi_{k}^{(t)}$ of the prior probabilities are calculated independently of the estimates $\BS\theta_{k}^{(t)}$ of the component parameters. The prior probability estimates equal 
$$\pi_{k}^{(t)}=n^{-1}\sum^{n}_{i=1}\alpha_{ik}.$$
If the generalized logit function is used for the prior probabilities such that $\pi_{k} = \frac{\exp(\B w^T \BS \gamma_k) }{\sum_{j=1}^K \exp(\B w^T \BS \gamma_j)}$, then the updated estimate of $\BS\gamma^{(t)}=(\BS\gamma_{1}^{(t)},...,\BS\gamma_{K-1}^{(t)})$ is an appropriate root of 
$$\sum^{K}_{k=1}\sum^{n}_{i=1}\alpha_{ik}\partial\log \pi_{k}(\B w_{i},\BS\gamma)/\partial \BS\gamma = \B 0$$
calculated with a numerical optimization routine. The component parameters are updated by finding an appropriate root of 
$$\sum^{n}_{i=1}\alpha_{ik}\partial\log f_{k}(\B y_{i}|\BS\theta_k) /\partial \BS\theta_{k} =\B 0$$
for each $k=1,...,K$. Closed-form solutions to this equation often exist for Gaussian mixture models.  

The E-step and M-step are alternated repeatedly until convergence. \Textcite{dempster1977} showed the incomplete likelihood increases monotonically at each iteration. The EM algorithm guarantees convergence to a local maximum; global convergence may be attained through initializing the algorithm by randomly assigning individuals to initial components, running the algorithm multiple times and using the estimates associated with the highest log-likelihood. For more details about the EM algorithm, see \textcite{mclachlan1997}. 

When the algorithm has converged, the values of the parameters and posterior probabilities from the last iteration are taken as the final estimates. The component estimates characterize the mean and variances within the clusters and the posterior probabilities provide the strength of group association for the soft clustering. This clustering can be translated into a hard clustering by choosing group labels that maximizes the posterior probability, $\arg\max_k \alpha_{ik}$. Then, one measure of uncertainty in the classification is $(1-\max_k\alpha_{ik}).$ A variant of the EM algorithm called the classification EM \cite{celeux1992}, in which the posterior probabilities, $\alpha_{ik}$, are converted to discrete classification before the M-step, is equivalent to the K-means algorithm when $\Sigma_{k} =\sigma I$ in a Gaussian mixture. For further discussion about other estimation approaches besides maximum likelihood through the EM algorithm, see \textcite{mclachlan2000}.

\subsection{Estimation issues}
Although an estimation tool exists, there are potential issues of parameter identifiability with mixture models. Fr{\"u}hwirth-Schnatter \cite{fruhwirth2006} distinguished between three types of nonidentifiability: invariance to  relabeling of components, potential overfitting, and nonidentifiability due to the family of component distribution and the covariate design matrix. The first two issues are resolved through constraints such as $\BS\theta_{k}\not=\BS\theta_{k'}$ for all $k,k'=1,...,K$, $k\not=k'$. The last concern is solved by assuming Gaussian components since finite mixtures of multivariate Gaussians are identifiable \cite{teicher1963,yakowitz1968}. However, Hennig \cite{hennig2000} suggested that the introduction of a regression structure to a Gaussian mixture requires a full rank design matrix as well as a rich covariate domain for regression parameters to be identifiable. On the other hand, prior probabilities parameters from a generalized logit based on concomitant variables are identifiable by setting the parameters of one component to zero such as $\BS\gamma_{K}=\BS0$ \cite{jiang1999}.

Besides identifiability, there are other known issues with finite mixture models. McLachlan and Peel \cite{mclachlan2000} noted that the sample size must be quite large for asymptotic theory to accurately describe the finite sampling properties of the estimator. Also, when component variances are allowed to vary between components, the mixture likelihood function is unbounded, and each observation gives rise to a singularity on the boundary of the parameter space \cite{day1969,kiefer1956}. However, Kiefer \cite{kiefer1978} outlined theory that guarantees that there exists a particular local maximizer of the mixture likelihood function that is consistent, efficient, and asymptotically normal if the mixture is not overfit. To avoid issues of singularities and spurious local modes in the EM algorithm, Hathaway \cite{hathaway1985} considered constrained maximum likelihood estimation for multivariate Gaussian mixtures based on the following constraint on the smallest eigenvalue of the matrix $\BS\Sigma_{h}\BS\Sigma_{j}^{-1}$, denoted as $\lambda_{\text{min}}(\BS\Sigma_{h}\BS\Sigma_{j}^{-1})$,
$$\min_{1\leq h\not=j\leq K}\lambda_{\text{min}}(\BS\Sigma_{h}\BS\Sigma_{j}^{-1})\geq c > 0$$ for some positive constant $c\in[0,1]$ to ensure a global maximizer. 



\subsection{Choosing the number of components}
Just as with partitioning methods, it is difficult to choose the number of components, $K$, and the problem has not been completely resolved \cite{mclachlan2000}. A natural approach s to test the hypothesis that $K=K_{0}$ against the alternative that $K=K_{1}$ with a likelihood ratio test. However, the regularity conditions do not hold for the test statistic to have its usual asymptotic null distribution. This problem has ben considered by many authors (see \textcite{mclachlan2000} for a summary) who have suggested modified test statistics \cite{wolfe1971}, approximate null distributions \cite{lo2001}, and resampling methods to empirically estimate the null distribution \cite{mclachlan1987}.

Another approach is to use information theoretic methods used for model selection. A set of candidate models are compared by measuring the information lost when the model is used to approximate the true realty. The model with the lowest information loss, thus lowest information criterion, is the chosen as the best model. Two popular information criterion are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). 

The information loss can be described in term of the Kullback-Leibler (KL) information \cite{kullback1951} of the true distribution with respect to the fitted model. If $g(\B y)$ is the true density, the KL information of $g(\B y)$ with respect to the assumed model, $f(\B y|\BS \theta)$, is
$$\int \log[g(\B y)/f(\B y|\BS\theta)]g(\B y)d\B y=\int \log[g(\B y)]g(\B y)d\B y-\int \log[f(\B y|\BS\theta)]g(\B y)d\B y.$$
Since the first term does not involve the model, only the second term is relevant. This integral can be simply estimated using the empirical distribution, which places mass $1/n$ at each observation $\B y_{i}$,
\begin{align*}
\int \log[f(\B y|\BS\theta)]g(\B y)d\B y &\approx \frac{1}{n} \sum^{n}_{i=1}\log f(\B y_{i}|\hat{\BS\theta})\\
&=\frac{1}{n} \log L(\hat{\BS\theta}).
\end{align*}
This is usually a biased estimate of the expected log density. Akaike \cite{akaike1973,akaike1974} showed that the bias is asymptotically equal to $d$, where $d$ is equal to the number of parameters in the model. Thus, he proposed choosing a model that minimizes
$$AIC = -2\log L(\hat{\BS\theta}) + 2d.$$
The AIC criterion is often used to choose the number of components in a mixture model. However, many authors have observed that it is inconsistent \cite{koehler1988} and tends to over estimate the number of components in a mixture \cite{soromenho1994,celeux1996}.

The BIC was originally derived in a Bayesian framework as an approximate to the integrated likelihood  \cite{schwarz1978}, which are used in Bayes factors to compare two models. For further information about Bayes factors, see \textcite{kass1995}. The BIC is calculated as
$$BIC= -2 \log L(\hat{\BS\theta}) + d\log n $$
where $d$ is the number of parameters in the model and $n$ is the sample size. The criterion can be justified in a frequentist framework, but the regularity conditions breakdown for mixture models. Nevertheless, Raftery and Fraley note considerable theoretical and practical evidence to support the use for clustering \cite{fraley1998}. Unlike the AIC, the BIC has been shown not to asymptotically underestimate the true number of components \cite{leroux1992}. In the context of nonparametric density estimation, Roeder and Wasserman have shown that the density estimate chosen with the BIC is consistent \cite{roeder1997}. However if the component densities assumptions do not hold, \textcite{biernacki2000} have been found that it tends fit too many components. 

To overcome the shortcomings of the BIC for the mixture model, \textcite{biernacki2000} proposed the ICL-BIC based on the integrated complete likelihood for a mixture with $K$ components,
$$ICL-BIC = -2\log L(\hat{\BS\theta}) + d\log n - 2\sum^{K}_{k=1}\sum^{n}_{i=1}\alpha_{ik}\log \alpha_{ik}$$
where $\alpha_{ik}$ is the posterior probability of individual $i$ belonging to component $k$, $d$ is the total number of parameters, and $n$ is the sample size. The last term is an entropy term based on the strength of the group classification. If the components of the mixture are well separated, then the entropy will be close to zero, but when the components are largely overlapping, the entropy is large and acts as a penalty for having poorly separated clusters. 

When comparing information criterion between competing models, large differences correspond to strong evidence for one model over the other. \textcite{kass1995} provide standard conventions for calibrating differences in BIC which can guide the final choice. When the differences are small, the smallest number of components that fit the data is chosen for the sake of parsimony.

\section{Thesis outline}
This thesis is organized in the following manner. In Chapter \ref{chap:misspecify}, I develop a series of simulations to study the impact of misspecifying the covariance structure in finite mixture models and present finite-sample and asymptotic bias results. In Chapter \ref{chap:motivate}, I motivate the rest of the thesis by discuss the general clustering methodology for longitudinal data and illustrate how these methods do not cluster based on the shape of change over time. Chapter \ref{chap:methods} presents three proposed methods to cluster based solely on the shape and compares them to standard methods through a simulation study in Chapter \ref{chap:sim}. Chapter \ref{chap:data} applies these methods to a data example. In Chapter \ref{chap:concl}, I will overview the results of the thesis, discuss the contributions, and present areas of future work. 

\section{Contributions}
This thesis contributes the following to the field:
\begin{itemize}
\item highlights the impact of misspecifying the covariance structure in finite mixture models in terms of bias and explore the behavior of this bias
\item presents practical advice to determine how to choose a covariance model
\item highlights the need for methodology to cluster longitudinal data based on the shape and patten of change
\item presents three proposed models to cluster trajectories on the basis of shape
\item compares these methods with standard clustering methods through a simulation study
\item apply these methods to body mass index trajectories
\end{itemize}
