\chapter{Three proposed shape-based clustering methods}
In this chapter, I present three new clustering techniques---derivative spline coefficient partitioning, the multilayer mixture model, and the vertically shifted mixture model---that attempt to answer the three research questions presented in the last chapter: Are there distinct shape patterns in the longitudinal data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? For each method, I discuss related work and then introduce necessary background, notation, and the model specification. I describe the implementation process and any foreseen issues and limitations. After introducing the three methods, I discuss the advantages and disadvantages of each. In the next chapter, a simulation study compares the proposed methods with those presented earlier in the thesis in addressing the three research questions above.

\section{Derivative spline coefficients partitioning}
The first method is based on the idea that calculating the derivative removes the level from a function and provides information about the shape. Unlike the quotient difference method described in Chapter 3,  the proposed method invokes smoothing out noise prior to calculating the derivative and does not require the data to be regularly sampled for all subjects. Using techniques from functional data analysis \cite{ramsay2002}, we project each individual's outcome data onto a functional basis to estimate a smooth curve over time and then differentiate the basis functions to get an indirect estimate of the derivative function. The estimated derivative functions then are used to calculate the dissimilarity between individuals. By first removing the level, this  method theoretically clusters individuals with similarly shaped trajectories.

\subsection{Related work}
In Chapter 3, we discuss the quotient difference distance measure separately suggested by D'Urso and M{\"o}ller-Levet et. al. \cite{d2000,moller2003}. M{\"o}ller-Levet et. al. calls this measure the short time-series distance and develops it to identify similar shapes in microarray data. In contrast, D'Urso takes a physics view of the data and refers to the quotient difference distance as the longitudinal-velocity dissimilarity measure and also uses a quotient difference of the velocities to calculate a measure inspired by acceleration. In the end, he combines the cross sectional and evolutive information of the trajectories into one compromise dissimilarity.

Similar to D'Urso, Zerbe presents three distance measures for growth curves based on position, velocity, and acceleration \cite{zerbe1979,schneiderman1993}. Rather than using quotient differencing, he suggests estimating each individual's growth curve by fitting a polynomial of degree $d$ using least squares. For individual $i$, we let $\B y_{i}$ be the vector of observed outcomes at times $\B t_{i}$. Then, the estimated curve is $\hat{f}_{i}(t) = [1\;\; t\;\;t^{2}\;\;...\;\;t^{d}]\;\hat{\bs\beta}_{i}$ where 
$$\hat{\bs\beta}_{i} = (\B X^{T}_{i}\B X_{i})^{-1}\B X^{T}_{i}\B y_{i}$$
and $\B X_{i}$ is the within-individual design matrix determined by the polynomial function evaluated at the observation times of individual $i$. Then, the estimated derivative is equal to
$\hat{f}_{i}^{'}(t) = [0\;\; 1\;\;2t\;\;...\;\;dt^{d-1}]\;\hat{\bs\beta}_{i}$. In other words, he projects the data onto a polynomial basis of degree $d$, differentiates the basis and calculates the estimated derivative function, which can be represented by another polynomial basis of degree $d-1$. Zerbe suggests using the $L_{2}$ distance on the derivative functions \cite{schneiderman1993}. The dissimilarity between the $i$th and $j$th individuals equals
$$d_{ij} =\left[ \int_{\mathcal{T}} [\hat{f}^{'}_{i}(t)-\hat{f}^{'}_{j}(t)]^{2}dt\right]^{1/2}$$
for a chosen time interval, $\mathcal{T}$.This is easy to calculate since the functions can be written with a polynomial basis.

At the end of their paper about clustering functional data, Tarpey and Kinateder make a few suggestions of ways to cluster data after getting 'rid of dominating variability in the intercept' \cite{tarpey2003}. One proposal is to cluster the derivatives of the estimated functions. In personal correspondence with one of the authors, he clarified the details of the implementation. After projecting individuals' data onto a Fourier basis, they differentiate the Fourier basis functions and use the K-means algorithm on the coefficients of the derivative functions. Thus, the estimated function for individual $i$ is $\hat{f}_{i}(t)= [1\;\; \sin(t)\;\;\cos(t)\;\;...\;\;\sin(wt)\;\;\cos(wt)]\;\hat{\bs\beta}_{i}$ where
$$\hat{\bs\beta}_{i} = (\B X^{T}_{i}\B X_{i})^{-1}\B X^{T}_{i}\B y_{i}$$
and $\B X_{i}$ is the within-individual design matrix determined by the Fourier expansion evaluated at the observation times of individual $i$. Since $\frac{d}{dt}\cos(wt) = -w\sin(wt)$ and $\frac{d}{dt}\sin(wt) = w\cos(wt)$, the estimated derivative function for individual $i$ equals the Fourier basis with new coefficients, $\hat{\bs\alpha}_{i}$, equal to a linear transformation of $\hat{\bs\beta}_{i}$. The dissimilarity between the $i$th and $j$th individuals equals
$$d_{ij} = (\hat{\bs\alpha}_{i}-\hat{\bs\alpha}_{j})^{T}(\hat{\bs\alpha}_{i}-\hat{\bs\alpha}_{j}).$$

These methods use the same general procedure. Individual trajectories are smoothed by projecting the data onto a chosen basis, the derivative of the estimated function is calculated by differentiating the basis functions, and the dissimilarity is based on the derivative functions. Now with any smoothing procedure, there is a fundamental bias-variance tradeoff. In this case, choosing including higher ordered terms or more sine and cosine functions in the basis decreases the bias but increases the variance. The basis needs to be determine in order to strike a balance between the two so as to not over fit the data. The choice of basis also affects the differentiation process. Polynomial and Fourier bases are computationally convenient in that differentiation results in a basis of the same type.

 The two methods differ in how the dissimilarity measure is defined between two individuals. Zerbe used the $L_{2}$ distance between two derivative functions; Tarpey and Kinateder calculated the squared Euclidean distance between the basis coefficient vectors for the estimated derivative function. This reflects the diversity in the general functional cluster analysis literature; some use the $L_{2}$ distance on functions \cite{hitchcock2007} while others use the linear coefficients of the basis function \cite{serban2005, tarpey2003, abraham2003}.
 
Tarpey reconciled these two approaches by showing that clustering functional data using the $L_{2}$ metric on function space can be achieved by clustering a suitable linear transformation of the regression coefficients \cite{tarpey2007}. If $y(t)$ is a functional observation represented as $y(t)=\sum_{j}\beta_{u}u_{j}(t)$and $\mu(t)$ is a functional cluster mean represented as $\mu(t) = \sum_{j}\gamma_{j}u_{j}(t)$, then the squared $L^{2}$ distance on interval $\mathcal{T}$ is
\begin{align*}
\int_{\mathcal{T}}(y(t)-\mu(t))^{2}dt &= \int_{\mathcal{T}}(\sum_{j}(\beta_{j}-\gamma_{j})u_{j}(t))^{2}dt\\
&= \sum_{j}\sum_{l}(\beta_{j}-\gamma_{j})(\beta_{l}-\gamma_{l}) \int_{\mathcal{T}}u_{j}(t)u_{l}(t)dt\\
&=(\bs\beta-\bs\gamma)^{T}\B W (\bs\beta-\bs\gamma)\\
&=(\B W^{1/2}(\bs\beta-\bs\gamma))^{T} (\B W^{1/2}(\bs\beta-\bs\gamma))
\end{align*}
where $\B W_{jl} = \int_{\mathcal{T}}u_{j}(t)u_{l}(t)dt$. Therefore, the clustering with $L^{2}$ distance is equivalent to plugging transformed coefficients, $\B W^{1/2}\bs\beta$, into the K-means algorithm. Consequently, when the functions are represented using an orthogonal basis such as the Fourier expansion, K-means on the coefficients is equivalent to the $L^{2}$ implementation.\\

Both the polynomial and Fourier bases are restrictive. The Fourier basis only works well when the data is periodic in nature and a polynomial basis does not provide a general structure to represent complex functions with few parameters. Another popular basis is the class of B-splines \cite{deboor1978, schumaker1981}, which extend the advantages of polynomials to include greater flexibility \cite{abraham2003}. To the author's knowledge, no other study have focused on clustering longitudinal data using the coefficients of the B-spline derivative estimate.

In this following sections, we introduce B-spline functions and demonstrate how they can be used to estimate derivative functions. Then, the implementation and practical decisions that need to be made in this clustering method are presented and discussed.
\subsection{B-spline background}
We fit a m-order spline function to each subject $i$ in order to estimate its underlying smooth, $f_i$. For the sake of being self-contained, we include some background on splines. Let $t\in[a,b]$ where $a,b\in\mathbb{R}$ and $\xi_0=a<\xi_{1}<\cdots<\xi_{L} < b = \xi_{L+1}$ be a subdivision of  the interval $[a,b]$ by $L$ distinct points, known as internal knots. We now define the augmented knot sequence, $\tau=[\tau_{1},...,\tau_{L+2m}]$ for $m\in\mathbb{N}$, such that 
\begin{align*}
\tau_{1}&=\tau_{2}=\cdots =\tau_{m} =\xi_{0}\\
\tau_{j+m}& = \xi_{j}, \quad\quad j=1,...,L\\
\xi_{L+1}&=\tau_{L+m+1}=\tau_{L+m+2}=\cdots =\tau_{L+2m} 
\end{align*}
The spline function, $f(t)$, is a polynomial of order $m$ on every interval $[\tau_{j-1},\tau_{j}]$ and has $m-2$ continuous derivatives on the interval $(a,b)$. The set of spline functions of order $m$ for a fixed sequence of knots, $\tau = [\tau_1,...,\tau_{L+2m}]$, is a linear space of functions with $L+m$ free parameters. A useful basis $B_{1,m}(t),...,B_{L+m,m}(t)$ for this linear space is given by Schoenbergs' B-splines \cite{curry1966, de1976} defined as
\begin{align*}
B_{j,1}(t) &= \begin{cases}
1 \text{ if }\tau_j\leq t < \tau_{j+1}\\
0\text{ otherwise}
\end{cases}\\
B_{j,l}(t) &= \frac{t-\tau_j}{\tau_{j+l-1}-\tau_j} B_{j,l-1}(t)+\frac{\tau_{j+l}-t}{\tau_{j+l}-\tau_{j+1}} B_{j+1,l-1}(t)
\end{align*}
where $l=2,...,m$ and $j=1,...,L+2m-l$.  If we adopt the convention that $B_{j,1}(t)=0$ for all $t\in\mathbb{R}$ if $\tau_{j}=\tau_{j+1}$, then by induction $B_{j,l}(t)=0$ if $\tau_{j}=\tau_{j+1}=\cdots=\tau_{j+l}$. Hence, $B_{1,l}(t)=0$ for $t\in\mathbb{R}$ and $l<m$ on the defined knot sequence. The B-spline function of order $m$ is defined by
$$f(t) = \sum^{L+m}_{j=1} \beta_j B_{j,m}(t).$$
Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote a vector of repeated observations for individual $i$ observed at times $\B t_{i}=(t_{i1},...,t_{im_{i}})  for $i=1,...,n$.$ We assume that $y_{ij} = f_{i}(t_{ij}) + \epsilon_{ij}$ such that $E(\epsilon_{ij}) = 0$ for all $j=1,...,m_{i}$ and $i=1,...,n$.  To estimate $f_{i}$, we fix the order of the B-spline, $m$, and internal knots. Then we estimate the coefficients, $\bs \beta^{(i)} = (\beta^{(i)}_1,...,\beta^{(i)}_{L+m})$ using least squares, 
$$\hat{\bs \beta}^{(i)} = (\B X_{i}^{T}\B X_{i})^{-1}\B X_{i}\B y_{i}$$
where $\B X_{i}$ is the within-individual design matrix based on the B-spline basis functions. Then, the estimated function is $\hat{f}_i(t)=\sum^{L+m}_{j=1} \hat{\beta}^{(i)}_j B_{j,m}(t)$. \\

To estimate $f_i'(t)$, we differentiate $\hat{f}_i(t)$ with respect to $t$ and we get
$$\hat{f}'_i(t)=\sum^{L+m}_{j=1} \hat{\beta}^{(i)}_j B'_{j,m}(t)$$
Prochazkova \cite{prochazkova2005} showed that this can be simplified to
$$\hat{f}'_i(t)=\sum^{L+m}_{j=2} \hat{\beta}^{(i)}_j \left[\frac{m-1}{\tau_{j+m-1}-\tau_j} B_{j,m-1}(t)-\frac{m-1}{\tau_{j+m}-\tau_{j+1}} B_{j+1,m-1}(t)\right].$$
However, we can go one step further and write this in terms of a B-spline basis of one order lower.
\begin{align*}
\hat{f}'_i(t)&=\sum^{L+m-1}_{j=1} (\hat{\beta}^{(i)}_{j+1} -\hat{\beta}^{(i)}_j)\frac{m-1}{\tau_{j+m}-\tau_{j+1} }B_{j+1,m-1}(t)
\end{align*}
If we adjust the knot sequence to only have $m-1$ replicates at the beginning and end, then $\hat{f}'_{i}(t)$ can be written as
$$\hat{f}'_i(t)=\sum^{L+m-1}_{j=1}\hat{\alpha}^{(i)}_jB_{j,m-1}(t)$$
where $\hat{\alpha}^{(i)}_j= (\hat{\beta}^{(i)}_{j+1} -\hat{\beta}^{(i)}_j)\frac{m-1}{\tau_{j+m}-\tau_{j+1} }$.
We use these derivative coefficients, $\hat{\bs \alpha}^{(i)} = [\hat{\alpha}^{(i)}_1,...,\hat{\alpha}^{(i)}_{ L+m-1}]$, to cluster trajectories with similar shape with the K-means algorithm \cite{macqueen1967, hartigan1979}. 
\subsection{Implementation}
To use this method in practice, decisions about the B-spline basis need to be made a priori. First, the order of the polynomials must be chosen. This together with the number of internal knots impacts the flexibility of the B-spline function. Cubic B-splines of order four have been shown to have good mathematical properties and are used frequently in practice \cite{james2003}. However, depending on the number of data points observed per subject, it may be necessary to use a quadratic polynomial ($m = 3$) due to the restriction that the sum of the order and the number of internal knots must be less than or equal to the minimum number of observation points per subject. 

The number of internal knots, $L$, plays a role in the flexibility of the class of spline functions and should be large enough to fit the features in the data. Just as with the order, the main limiting factor in choosing L is be the number of data points per subject. In longitudinal considered in this thesis, the number of data points per subjects is about five. It is important not to over fit the individual curves so for data with limited observation, it may only be possible to have a t most one internal knot. If there are many repeated measures, model selection information criteria or cross-validation can be used to choose $L$ \cite{rice2001}. The location of the internal knots is another issue of discussion. There are some suggested data-driven ways to select knot location \cite{shanggang2001}, but Ruppert \cite{ruppert2002} supports fixing the knots at sample quantiles. We follow his suggestion and adjust them as necessary to the areas with the most action. 

Once the order of the polynomials and number and location of knots are chosen, then smooth functions and their derivatives are estimated using Least Squares for every subject. The coefficients from the estimated derivative functions become the input vectors to the clustering procedure. The K-means algorithm is a standard clustering algorithm that given a fixed number of clusters, minimizes the within sum of squares after initially randomizing vectors to groups. This algorithm converges but there is no guarantee that it will find the grouping that globally minimizes the objective function. Therefore, in practice, the algorithm is run multiple times with different random initializations and the clustering that minimizes the objective function is chosen. We use 25 random starts.

However, in order to run K-means, you need to fix the number of clusters, $K$, which is usually unknown and of interest to researchers. While no perfect mathematical criterion exists, a number of heuristics (see \cite{tibshirani2001} and discussion therein) are available for choosing $K$. For this thesis, we choose the $K$ that maximizes the overall average silhouette width \cite{rousseeuw1987}.  For each subject $i$, the silhouette width, $s_{i}$, is calculated by
$$s_{i}=\frac{b_{i}-a_{i}}{\max\{a_{i},b_{i}\}}$$
where $a_{i}$ is the average dissimilarity of $i$ to all other trajectories in its cluster, $A$, $d_{i}(C)$ is the average dissimilarity of $i$ to all trajectories of cluster $C$ and $b_{i}$ is equal to $\min_{C\not= A} d_{i}(C)$. The overall average silhouette width, $\bar{s}$, is the average of $s_{i}$ over all subjects in the whole data set. By maximizing the overall average silhouette width, the dissimilarity between clusters is maximized and the dissimilarity within clusters is minimized resulting in distinct groups. In this case, the dissimilarity between two individuals is defined as the squared Euclidean distance between derivative coefficients. We choose $K$ that best separates the data so we have groups of individuals with similar shape patterns over time.

K-means is a partitioning algorithm, therefore, by definition every subject is `hard' clustered into only one of the groups. There is no stated uncertainty in the group memberships even if a subject is in the overlap between two clusters. In order to estimate the relationship between baseline variables and shape group membership, we assume the cluster labels are known. Given the subject grouping labels from the partition, $\{c_{i}\}$ such that $c_{i}\in\{1,2,...,K\}$ for all $i=1,...n$, we fit a multinomial logistic regression model, which is an extension of the logistic regression model, using the group labels as the outcome and baseline factors as explanatory variables such that
$$P(c_{i} = k|\B z_{i}) = \frac{\exp(\B z_{i}^{T}\bs\gamma_{k})}{\sum_{j=1}^{K}\exp(\B z_{i}^{T}\bs\gamma_{j})}$$
for all $k=1,...,K$ and $i=1,...,n$ with $\bs\gamma_{K}=0$ where $\B z_{i}$ is the design vector based on baseline factors. We then estimate the parameters $\bs\gamma_{1},...,\bs\gamma_{K-1}$ using maximum likelihood estimation. The estimated standard errors from a standard Hessian calculation do not include any group membership uncertainty  as we assume the hard clustering labels are known for the estimation process.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multilayer model}
The second method attempts to make up for the lack of uncertainty in groups memberships by using a model-based approach. The circumstances in which traditional clustering methods fail are when the level and shape are weakly dependent resulting in individuals with the same shape but at different levels. Therefore, it makes sense to model data as originating from shape clusters each having a mixture distribution of levels.
\subsection{Related work}
Model-based methods provides the probability framework that dissimilarity-based methods lack. One of the main benefits is the ability to take into account uncertainty---the uncertainty of the mean estimates as to make inferences about a larger population and the uncertainty of cluster membership when estimating relationships between baseline variables and clusters. As mentioned in Chapter 1, the main model-based method is the finite mixture model. However, a standard mixture of Gaussians fit to the raw data does not distinguish between the level and shape and will cluster subjects based on the dominant source of variability. Another limitation of Gaussian mixtures for grouping data is that the cluster densities may be non-normal. For these reasons, the mixture model has been extended into the multilayer mixture model, which is a generalization of finite mixture models in that each cluster is allowed to be a mixture itself \cite{li2005}. For example, imagine a data set with two groups with bimodal densities. One could fit a multilayer mixture of two clusters each with two components. (see Figure \ref{fig:dia} for diagram). A variation of this idea has been used in clustering non-normal groups by first fitting a mixture and then combining cluster components to make more meaningful clusters \cite{hennig2010}. 
\begin{figure}[h]
\begin{center}
\includegraphics[width=4in]{Chp4multilayer_diagram}
\end{center}
\label{fig:dia}
\caption{Diagram of multilayer mixture model showing that each cluster is comprised of potentially more than one component.}
\end{figure}

In our case, we want the clusters to be meaningful in terms of distinguishing between shape. Therefore, we can use this generalization in order to model $K$ non-normal shape clusters comprised of mixture components with the same mean shape at different levels.

\subsection{Model specification}
In a multilayer mixture model, let $K$ be the number of clusters and $J_{k}$ be the number of components in cluster $k=1,...,K$ such that $J=\sum_{k=1}^{K}J_{k}$ is the total number of components in the entire model. Let $j$ uniquely index all of the components such that $j=1,...,J$. For ease of explanation, we define a cluster assigning function, $c(j):\{1,...,J\}\rightarrow \{1,2,...,K\}$, to specify the cluster to which a component belongs. Let $f(\B y|\bs\mu,\bs\Sigma)$ be the probability density function of a multivariate normal distribution with mean $\bs\mu$ and covariance matrix $\bs\Sigma$. Then the probability density function for cluster $k$ is
$$f_{k}(\B y) = \sum_{j: c(j) = k} \pi_{j|c(j)}f(\B y|\mu_{j},\Sigma_{j})$$
where $\pi_{j|c(j)}$ is the probability of being in component $j$ given a sample is in cluster $c(j)$ and $\sum_{j: c(j) = k}\pi_{j|c(j)}=1$ for all $k=1,2,...,K$. Let the probability of cluster $k$ given baseline factors, $\B z$, be $\pi_{k}(\B z,\bs\gamma)=\exp(\B z^{T}\bs\gamma_{k})/\sum^{K}_{l=1}\exp(\B z^{T}\bs\gamma_{l})$ such that $\bs\gamma_{K}=0$ and $\bs\gamma=(\bs\gamma_{1},...,\bs\gamma_{K})$. Then the probability density function for the multilayer mixture can be written as
$$g(\B y|\B z) = \sum_{k=1}^{K}\pi_{k}(\B z,\bs \gamma)f_{k}(\B y) = \sum_{k=1}^{K}\pi_{k}(\B z,\bs\gamma)\sum_{j: c(j) = k} \pi_{j|c(j)}f( \B y| \bs\mu_{j},\bs\Sigma_{j})$$
Since every component belongs to one and only one shape cluster, the above equation reduces to a regular mixture model if we let $\bar{\pi}_{j}(\B z,\bs \gamma)=\pi_{c(j)}(\B z,\bs\gamma)\pi_{j|c(j)}$,
$$g(\B y|\B z) = \sum_{j=1}^{J}\bar{\pi}_{j}(\B z,\gamma)f(\B y|\bs \mu_{j},\bs\Sigma_{j})$$
In order to accommodate our goal of clustering on shape, we include a regression parameterization for the mean structure to model the smooth underlying group mean function. We use a B-spline basis functions for the design matrix as presented earlier in this chapter. However, we restrict the regression parameters to be the same within shape clusters and allow the intercept to differ in each component. Hence, $\mu_{j}=\alpha_{j}+\B x \bs\beta_{c(j)}$, where $\B x$ is a matrix of B-spline basis functions excluding the first basis function so as to allow for estimation of intercept terms for each component. This allows the clusters to be based on shape while the components can have different levels (see figure \ref{fig:diashape} for diagram). Additionally, we assume $\Sigma_{j}=\sigma^{2}_{j}I$ for all $j=1,...,J$  for simplicity and to allow for irregularly sampled data.

In order to estimate the shape cluster and component-specific parameters, we maximize the classification likelihood using a modified Expectation Maximization (EM) algorithm \cite{dempster1977} called the Classification EM algorithm \cite{mclachlan2000}. The details of the iterative algorithm can be found in Ji Lia's paper \cite{li2005}, but the main modification involves hard assigning clusters while soft assigning the components through the iterative estimation process.
\begin{figure}[h]
\begin{center}
\includegraphics[width=4in]{Chp4multilayer_diagram_shape}
\end{center}
\label{fig:diashape}
\caption{Diagram of multilayer mixture model showing that each shape cluster is comprised of potentially more than one level component.}
\end{figure}
\subsection{Implementation}
Denote the shape cluster identity of sample $i$ by $\eta_{i}$ where $\eta_{i}\in\{1,...,K\}$. Then, $\bs\eta = \{\eta_{i}; i=1,...,n\}$ and $\bs\theta=\{\bs \gamma_{k}, \pi_{j|c(j)},\alpha_{j}, \bs\beta_{k},\sigma^{2}_{j};\; j=1,...,J, k=1,...,K\}$ are the parameters for the model. In order to estimate the parameters in this multilayer mixture model, we maximize the classification likelihood function, \cite{mclachlan2000}
\begin{align}
 L(\theta,\eta) &= \sum^{n}_{i=1} \log \pi_{\eta_{i}}(\B z_{i},\bs\gamma) f_{\eta_{i}}(\B y_{i}|\B x_{i})\\
\label{ll}& =  \sum^{n}_{i=1} \log\left[ \pi_{\eta_{i}}(\B z_{i},\bs\gamma)  \sum_{j: c(j) = \eta_{i}} \pi_{j|c(j)}f(\B y_{i}|\B x_{i},\alpha_{j},\bs\beta_{\eta_{i}},\sigma^{2}_{j})\right]
\end{align} 
using a modified EM algorithm called the Classification Expectation Maximization algorithm (CEM) \ref{celeux1992;mclachlan2000}. The modification involves adding a classification step between the Expectation step and Maximization step where individuals are assigned to shape clusters. The algorithm continues to iterate between these three steps until convergence.

In order to start the iterative process, the individuals are initially assigned into shape clusters and component groups. Initialization can involve randomly partitioning individuals into components or strategically partitioning individuals into shape clusters using a quick procedure such as the first method, derivative coefficient K-means, and then randomly partitioning the individuals into components. 

Let $\bs\theta^{(t)}$ and $\bs\eta^{(t)}$ be the current estimates of the parameters at iteration $t$. The algorithm updates these estimates as follows:
\begin{enumerate}
\item Expectation step: For each individual $i$, compute the posterior probability of being in shape cluster $k$, $p_{i,k}$, such that
$$p_{i,k}= \pi_{k}(\B z_{i},\bs\gamma)f_{k}(\B y_{i}|\B x_{i})/\sum^{K}_{j=1} \pi_{j}(\B z_{i},\bs\gamma)f_{j}(\B y_{i}|\B x_{i})$$
for $i=1,...,n$ and $k=1,...,K$.
\item Classification step: Hard classify subjects to shape clusters according to $\eta^{(t+1)}_{i} = \arg\max_{k} p_{i,k}$.
\item Maximization step: For each shape cluster, use maximum likelihood estimation to update parameter vector $\theta$ by embedding an EM procedure initialized with the current parameter values.
\end{enumerate} 
This algorithm increases the log-likelihood at each iteration and the statement is below. The proof is in the appendix. 
\begin{theorem} The classification likelihood $L(\bs\theta,\bs\eta)$ defined in (\ref{ll}) is non-decreasing after each update of $\bs\eta$ and $\bs\theta$ by the CEM algorithm. That is, $L(\bs\theta^{(t+1)},\bs\eta^{(t+1)}) \geq L(\bs\theta^{(t)},\bs\eta^{(t)})$ for all $t$.
\end{theorem}

In the Maximization step of the algorithm, parameters are estimated by maximizing the likelihood for each shape cluster. However, we have constrained the shape parameters to be equal for all components within the cluster. To estimate both the component and cluster-specific parameters simultaneously, we use computational methods suggested by Gr{\"u}n \cite{grun2008}. For shape cluster $k$, the outcome vector for subject $i$ is temporarily replaced by a vector of $\B y_{i}$ repeated $J_{k}$ times. The design matrix  is set equal to $(I_{J_{k}}\otimes \bs 1_{m_{i}}, \bs 1_{J_{k}} \otimes \B x_{i})$ where $\otimes$ refers to the Kronecker product. Lastly, the covariance matrix structure is block diagonal with $\sigma^{2}_{j}I_{m_{i}}$ in each block for $j$ that satisfy $c(j)=k$. Assuming a multivariate Gaussian distribution and using profile likelihoods, we maximize the likelihood function with respect to the parameters for cluster $k$ and its components using a standard numerical optimization routine.

Now, as in the derivative coefficient method, we want to allow the mean structure to be flexible enough to accommodate shapes other than lower ordered polynomials. So we use a B-spline basis for the mean structure. The issues of the number and location of internal knots are dealt with in the same manner.

With this more complex mixture structure, it is necessary to select the number of shape clusters, $K$, as well as the number of components for each cluster $k$, $J_{k}$, $k=1,..,K$. It is recommended to fix $K$ and then use model selection criteria to select $J_{k}$ for each $k=1,...,K$. As suggested by Jia Li, we use BIC even though the regularity conditions do not hold as the criteria has been shown to be useful a informal guide in practice \cite{li2005}.

Derivations of robust standard error  were done following the same procedure as Boldea and Magnuson \cite{boldea2009}. These are available in the appendix.

\section{Vertical shifting mixture model}
The goal of cluster analysis is to partition a collection of individuals into homogeneous groups. In this thesis, we define similar individuals as those with the same outcome pattern or shape over time. We aim to develop an effective clustering method that focuses on grouping longitudinal data with a special focus on shape similarities. If two curves only differ in intercepts by vertical shifts, they are placed in the same group. Up until now, the two proposed methods use derivatives to implicitly remove the level or directly model the variability in the level with a mixture model.  In this method, we consider subtracting the mean outcome level. Each cluster comprises a density with a mean shape curve and a covariance function. The assumed finite mixture of densities is fit to the vertically shifted data to estimate parameters and group membership probabilities. 

\subsection{Recent work}
A mixture model is a standard method for clustering multivariate data \cite{everitt2009} and has been used for longitudinal applications \cite{muthen2010, jones2001}. However, for longitudinal data, the models are commonly used for the observed data without much regard to the real goal of clustering by shape. We suggest subtracting out the mean outcome level before modeling. Subtracting the level is not a novel idea in statistics or even cluster analysis. In most multivariate clustering applications, it is recommended that each variable is standardized by subtracting the mean of the variable measures and dividing by the standard deviation so that each standardized variable is in comparable units and equally contributes to the clustering process. This is not recommended for the longitudinal setting where each ``variable'' is a repeated measurement at a different time point. We want the variables to stay in the same units since the change in the original units is the characteristic of interest. Any transformation should only be additive to maintain the shape of the data over time. Rather than completing variable-specific normalization, we suggest subtracting the subject-specific level. This seems like an obvious pre-processing step; however, it is rarely used in practice and there are some consequences that need to be considered when modeling the transformed data as discussed later in the chapter. 

A partial version of this idea has been implemented in the functional data analysis literature. For processes in a Hilbert space of square integrable functions with respect to the Lebesgue measure, $dt$, on the interval $\mathcal{T}=[0,T]$, Chiou and Li \cite{chiou2008} propose using a mixture model and the Karhunen-Lo{\`e}ve expansion for centered stochastic processes within their correlation-based clustering algorithm. To center the processes, they subtract the integral of the process over interval $\mathcal{T}$ divided by $T$, the length of the interval. The resulting process integrates to zero. The integral of the process is the functional analogue to a mean in finite vector space and similarly, the resulting vector after subtraction has mean zero.

 Although centering a process and a vector stems from the same idea, there are distinct consequences of subtracting the estimated level of a noisy curve observed at a finite number of points that don't arise when centering a smooth function. The term centering is used in the stochastic processes literature, but we use the term vertically shifting rather than centering to refer to the procedure of subtracting the mean since it graphically describes the transformation of the noisy longitudinal data.
 
In this next section, we outline the model notation and specification for vertically shifted mixture models including the mean and covariance structure. Then we describe the implementation process of choosing a basis, the number of clusters, and estimating the parameters of the model. As mentioned there are some considerations and issues that we discuss in terms of the impact of transforming the data on the modeling process and final clustering results. 
\subsection{Model specification}
 Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote an outcome vector of repeated observations for individual $i$, $i=1,...,n$. As longitudinal data is collected over a period of time, the vector of corresponding times of observation for individual $i$ is denoted as $\B t_{i}=(t_{i1},...,t_{im_{i}})$ and $\B z_{i}$ is a $q$-length design vector based on time-fixed factors that are typically collected at or before time $t_{i1}$. Let $\bar{y}_{i}= m_{i}^{-1}\sum^{m_{i}}_{j=1} y_{ij}$ be the mean of the observed outcomes for individual $i$. This is one measure of the vertical level of a curve. To remove the level of the observations and leave the shape over time, we apply a linear transform to the vector of observations and let
\begin{align*}
\B y^{*}_{i} = \B A_{i}\B y_{i}\quad\text{ where }\quad \B A_{i} =\B I_{m_{i}} - m_{i}^{-1}\B 1_{m_{i}}\B 1_{m_{i}}^{T}
\end{align*}
where $\B 1_{m_{i}}$ is a m-length vector of 1's and $\B I_{m_{i}}$ is an $m_{i}\times m_{i}$ identity matrix. Multiplying the symmetric matrix $\B A_{i}$ to the vector $\B y_{i}$ subtracts the individual mean, $\bar{y}_{i}$, from each element $\B y_{i}$. The resulting vertically-shifted vector, $\B y^{*}_{i}$, is in units relative to the mean, $\bar{y}_{i}$, and is assumed to arise from one of $K$ shape groups characterized by probability densities that are centered around mean shape functions with dependence summarized by covariance matrices. We assume that, conditional on $\B t$ and $\B z$, $\B y^{*}$ is a realization from a finite multivariate Gaussian mixture model with density
\begin{align*}
 f(\B y^{*}|\B t,\B z,\bs\theta) =  \sum^{K}_{k=1}\pi_{k}(\B z, \bs\gamma)f_{k}( \B y^{*}|\B t,\bs\theta_{k})
\end{align*}
where $\pi_{k}(\B z,\gamma)>0$ for $k=1,...,K$ and $\sum^{K}_{k=1}\pi_{k}(\B z,\gamma)=1$ and $\bs\theta = (\bs\gamma,\bs\theta_{1},...,\bs\theta_{K})$. To allow baseline covariates to affect the probability of having a certain shape pattern over time, we parameterize the mixing proportions using the generalized logit function with the form
$$\pi_{k}(\B z,\bs\gamma)=\frac{\exp(\B z^{T}\bs\gamma_{k})}{\sum_{j=1}^{K}\exp(\B z^{T}\bs\gamma_{j})}$$ 
for $k=1,...,K$ where $\bs \gamma_{k}\in\mathbb{R}^{q}$, $\bs\gamma = (\bs\gamma_{1},...,\bs\gamma_{K})$, and we fix $\bs\gamma_{K}=\B 0$. We assume the component densities $f_{k}(\B y^{*}|\B t,\bs\theta_{k})$ are multivariate Gaussian with mean $\bs\mu_{k}(\B )$ and covariance matrix $\Sigma_{k}(\B t)$.

\subsubsection{Mean structure}
In the longitudinal setting, the main interest is the change over time; therefore, the mean shape is a function of time. To parameterize the mean pattern in the mixture model, we let the design matrix, $\B x_{i}$, be a matrix of values from a chosen functional basis taken at observation times $\B t_{i}$. It is common to use a quadratic or cubic basis by letting $\B x_{i} = (\B 1_{m_{i}}^{T},\B t_{i}^{T}, (\B t^{2}_{i})^{T})$. However, this limits the ability of the model to capture the complexity of a mean structure. We use the class of B-splines basis functions \cite{deboor1978, schumaker1981,curry1966, de1976}, which extend the advantages of polynomials to include greater flexibility. The B-spline function of order $m$ is defined by a linear combination of coefficients and basis functions
$$\mu(t) = \sum^{L+m}_{j=1} \beta_j B_{j,m}(t)$$
where the basis functions, $B_{j,m}(t)$, are defined earlier in this chapter. We assume the mean pattern for the $k$th shape cluster is approximated by the function $s_{k}(t)=\sum^{L+m}_{j=1} \beta_{jk} B_{j,m}(t)$.  Since we only observe measurements at discrete times, we let the design matrix include $L+m$ variables corresponding to the basis functions evaluated at the measurement times.  For each individual $i$ observed at times $t_{i1},...,t_{im_{i}}$, we let $$\B x_{i} = \left(\begin{array}{cccc} 
B_{1,m}(t_{i1})&B_{2,m}(t_{i1})&\cdots&B_{L+m,m}(t_{i1})\\
B_{1,m}(t_{i2})&B_{2,m}(t_{i2})&\cdots&B_{L+m,m}(t_{i2})\\
\vdots&\vdots&\ddots&\vdots\\
B_{1,m}(t_{im_{i}})&B_{2,m}(t_{im_{i}})&\cdots&B_{L+m,m}(t_{im_{i}})\end{array}\right)$$
and $\bs\beta_{k} = (\beta_{jk},...,\beta_{(L+m) k})$ so that the mean vector for individual $i$ is $\bs\mu_{k}(\B t_{i}) = \B x_{i}\bs\beta_{k}$.

\subsubsection{Covariance structure}
Various assumptions can be made about the covariance matrix, $\B \Sigma_{k}(\B t_{i})$. Since it is common for longitudinal data to have sparse, irregular time sampling, we must impose some structure on the covariance matrix to allow for parameter estimation as described by Jennrich and Schluchter in their seminal paper \cite{jennrich1986}. A common parameterization is conditional independence with constant variance where $\B \Sigma_{k}(\B t_{i}) = \sigma_{k}^{2}I_{m_{i}}$. This is an unrealistic assumption for longitudinal data since there is inherent dependence between repeated measures on the same unit. 

Compound symmetry, which is also known as exchangeable correlation, is a popular correlation structure in longitudinal analysis where all repeated measures are equally correlated, which is typically paired with constant variance, and $\B \Sigma_{k}(\B t_{i}) = \sigma_{k}^{2}(\rho_{k}\B1_{m_{i}}\B1_{m_{i}}^{T}+(1-\rho_{k})I_{m_{i}})$ where $-1\leq\rho_{k}\leq 1$ is the correlation. This dependence structure is not realistic in  many situations but improves upon the independence model. It is frequently used as a working correlation structure when the estimation of mean parameters is unbiased no matter the correlation specified. Estimating mixture models does not fall into this category as evidenced in Chapter 2.

A more general structure that provides a compromise between the two is the exponential correlation structure in which the dependence decreases as the time between observations increases---$\B \Sigma_{k}(\B t_{i})_{jl} = \sigma_{k}^{2}\exp(-| t_{ij}-t_{il}| / r_{k})$ where $r_{k}> 0$. This structure is similar to the correlation matrix generated from an autoregressive model of order one where $\B\Sigma_{k}(\B t_{i})_{jl} = \sigma^{2}\rho_{k}^{|t_{ij}-t_{il}|}$ where $\rho_{k}$ is the correlation for measurements observed one unit of time apart. If we set $\rho_{k} = \exp(-1/r_{k})$, then $0\leq\rho_{k}\leq1$ and the two parameterization result in the same structure when the correlation between two measures is constrained to be positive. This is a reasonable assumption for longitudinal data in the original form but it many not be acceptable for the transformed data as we discuss later in the chapter.

The covariation structures mentioned above are associated with weakly stationary processes in that they have constant variance and the correlation only depends on the time lag between observations. If the variance or correlation function is non-constant but varying continuously, it could be potentially modeled as a function, but estimation is more difficult.

As discovered in Chapter 2, the assumed covariance matrix can highly impact the results in terms of parameter estimates and the final clustering if the groups are not well-separated. A measure such as the RJ criteria can be useful to choose a covariance matrix structure in practice. The RJ criteria is equal to the mean of the diagonal elements of $\widehat{\B W}_{1}^{-1}\widehat{\B W}_{3})$ where $\widehat{\B W}_{1}$ and $\widehat{\B W}_{3}$ are the Hessian and robust sandwich estimate of the covariance matrix, respectively.  

\subsection{Implementation}
Given a collection of independent observed outcome vectors, $\B y_{1},...,\B y_{n}$, the first step of implementing this method is to calculate the mean for each subject, $\bar{y}_{i}$, $i=1,...,n$ and subtract the subject-specific mean from the observed outcome vector. This transformation leaves the updated independent vectors $\B y^{*}_{1},...,\B y^{*}_{n}$. Second, the order of the spline and the number and location of internal knots for the mean structure need to be decided prior to model fitting. 

The B-spline basis is kept constant for all components so the simplest way to select the number of knots is through visual inspection of the full data set considering the number of observations per subject. If the most complex shape patterns is a lower order polynomial, no internal knots are necessary. However, if the most complex curve is more varied, adding knots and increasing the order of the polynomials can flexibly accommodate the twists and turns of the mean patterns. In choosing both the order of the polynomials and the number of knots, it is important to balance the number of mean parameters with the sample size. Every unit increase in the order or in the number of knots increases the number of parameters by $K,$ the number of components. In terms of location of the knots, it has been suggested to place knots at sample quantiles based on the sampling times of all the observations \cite{ruppert2002}. However, this strategy may not work well if the median time is not the point at which the curve deviates from a typical polynomial. If possible, it is best to place knots at local maxima, minima, and inflection points of the overall trends to as to accommodate the differences from a polynomial function \cite{eubank1999}.  Once these are decided, then the design matrixes $\B x_{i}$ are calculated using widely available algorithms. 

Thirdly, the model assumes the number of clusters, $K$, is known. In practice, this is not the case and we must choose $K$. The most popular procedure is to chose a maximum value of $K$ such that $K_{max}<<n$, fit the model under all values of $K=2,...,K_{max}$, and choose the value that optimizes a chosen criteria. In this thesis, we use the Bayesian Information Criterion (BIC) \cite{schwarz1978} for choosing $K$. It is defined as
$$BIC = -2\log L(\hat{\bs\theta},K)- d\log(n)$$
where $d$ is the length of $\bs\theta$, the number of parameters in the mixture model, and $L(\bs\theta,K)$ is the log likelihood function for the parameter vector. The BIC has been widely used for model selection with mixture models since Roeder and Wasserman's use in 1997 \cite{roeder1997}. In particular, the criteria has been to select the number of clusters \cite{dasgupta1999,fraley1999} with good results in practice. For regular models, the BIC was derived as an approximation to twice the log integrated likelihood using the Laplace method \cite{tierney1986}, but the necessary regularity conditions do not hold for mixture models in general \cite{aitkin1985}. However, Roeder and Wasserman \cite{roeder1997} showed that the BIC leads to a consistent estimator of the mixture density, and Keribin \cite{keribin2000} showed that the BIC is consistent for choosing the number of components in a mixture model.

In order to fit the model and estimate the parameters, we use maximum likelihood estimation via the EM algorithm. Under the assumption that $\B y^{*}_{1},...,\B y^{*}_{n}$ are independent realizations from the mixture distribution, $f(\B y^{*} | \B t, \B z, \bs\theta)$, defined in \ref{mixdens}, the log likelihood function for the parameter vector, $\bs \theta$, is given by
$$\log L(\bs\theta,K)=\sum^{n}_{i=1}\log f(\B y^{*}_{i}|\B t_{i},\B z_{i},\bs \theta).$$
The ML estimate of $\bs\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\bs\theta)/\partial \bs\theta=\B 0.$ Solutions of this equation corresponding to local maxima can be found iteratively through the Expectation-Maximization (EM) algorithm \cite{dempster1977}. This algorithm is applied in the framework where given $(\B t_{i},\B z_{i})$ each $\B y^{*}_{i}$ is assumed to have stemmed from one of the components and the indicator denoting its originating component is missing. The complete-data log likelihood is based on these indicator variables as well as the observed data $\{(\B y^{*}_{i}, \B t_{i}, \B z_{i})\}$. The Expectation step (E-step) involves replacing the indicators by current values of the conditional expectation, which is the posterior probability of component membership, written as
$$\alpha_{ik}=\pi_{k}(\B z_{i},\bs\gamma)f_{k}(\B y^{*}_{i}|\B t_{i})/\sum_{j=1}^{K}\pi_{j}(\B z_{i},\bs\gamma)f_{j}(\B y^{*}_{i}|\B t_{i},\bs \theta_{j})$$
for $i=1,...,n$ and $k=1,...,K$ using current estimates of the parameters. In the Maximization step (M-step), the parameter estimates for the mixing proportions, regression effects, and covariance matrices are updated by maximizing the complete-data log likelihood using the posterior probabilities from the E-step in place of the indicator variables using numerical optimization. The E- and M-steps are alternated repeatedly until convergence. The EM algorithm guarantees convergence to a local maximum; global convergence may be attained through initializing the algorithm by randomly assigning individuals to initial components, running the algorithm multiple times and using the estimates associated with the highest log likelihood. Besides the parameter estimates, the algorithm returns the posterior probability estimates of component membership. These probabilities can be used to partition individuals into distinct clusters by selecting the cluster with the maximum posterior probability. However, unlike K-means, the posterior probability provides some measure of uncertainty in the hard clustering. 

\subsection{Potential issues}
There are issues of identifiability with Gaussian mixture models that can be mitigated through some minor constraints \cite{mclachlan2000}. In this section, we discuss some unique consequences of vertically shifting the data on the model and inference.

\subsubsection{Covariance of transformed data vectors}
We let $\B Y=(Y_{1},...,Y_{m})$ be a random vector observed at times $\B t=(t_{1},...,t_{m})$ such that
$\B Y = \lambda + \bs\mu(\B t) + \bs\epsilon(\B t)$
such that $\lambda\sim F$, $\bs\mu(\B t)$ is a vectorized deterministic function of time, and $\bs\epsilon(\B t)\sim(0,\bs\Sigma(\B t)$. We let $\B\Sigma(\B t) =\B V^{1/2}\B R(\rho)\B V^{1/2}$ where $\B R(\rho)$ is an $m\times m$ correlation matrix based on the parameter $\rho$ and potentially the associated observation times, $\B t$, and $\B V$ is a $m\times m$ matrix with variance parameters along the diagonal. If we linearly transform the data to subtract the mean of the elements according to the symmetric matrix $A$ previously defined, the covariance of the resulting random vector is
\begin{align*}
Cov(\B A\B Y) &= Cov(\B A\bs\mu(\B t)+\bs\epsilon(\B t)))\\
&= \B A^{T}Cov(\bs\mu(\B t)+\bs\epsilon(\B t))\B A
\end{align*}
by the properties of covariance. The random intercept, $\lambda$, disappears so we do not need to worry about its distribution. 
\subsubsection{Fixed observation times}
For the moment, let us assume that the observation times, $\B t$, are fixed. Then
\begin{align*}
Cov(\B A\B Y)&= \B A^{T}Cov(\bs\epsilon(\B t))\B A\\
&= \B A^{T}\B\Sigma \B A
\end{align*}
If the variance is constant over time, $\B V=\sigma^{2}I$, and the elements of the vector are independent, $\B R(\rho)=I$, then the covariance of the transformed vector is
\begin{align*} 
Cov(\B A\B Y) &= \sigma^{2}\B A\B A^{T} \\
&=\sigma^{2}\B A\\
&= \sigma^{2}(I - m^{-1}\B1_{m}\B1_{m}^{T})\\
&=\sigma^{2}\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)I)
\end{align*}
 where $a=\frac{-1}{m-1}$ since $\B A$ is an idempotent matrix. Therefore, if the data has independent errors, subtracting the estimated mean induces negative exchangeable correction between the observations of magnitude $\frac{-1}{m-1}$. Additionally, the variance decreases to $\sigma^{2}\frac{m-1}{m}$. If $m$ is large, the resulting correlation structure is approximately independence with variance $\sigma^{2}$.
 
 If the errors in the original data have constant variance, $\B V=\sigma^{2}I$, and are exchangeable with $\B R(\rho) = \rho\B 1_{m} \B 1_{m} + (1-\rho) I$, then the covariance of the transformed vector is
 \begin{align*}
 Cov(\B A\B Y) &= \sigma^{2}\B A\B R(\rho)\B A^{T}\\
 &= \sigma^{2}(I-m^{-1}\B1_{m}\B1_{m}^{T})(\rho\B1_{m}\B1_{m}^{T}+(1-\rho)I)(I-m^{-1}\B1_{m}\B1_{m}^{T})^{T}\\
 &= \sigma^{2}(1-\rho)(I-m^{-1}\B1_{m}\B1_{m}^{T})\\
 &=\sigma^{2}(1-\rho)\left(\frac{m-1}{m}\right)(a\B 1_{m}\B 1_{m}^{T}+ (1-a)I)
 \end{align*} 
 where $a=\frac{-1}{m-1}$ as before. This transformation maintains the exchangeable structure but with negative correlation on the off diagonal and decreased variance.  Again, if the number of observed data points is large, then the structure is approximately independent with variance $\sigma^{2}(1-\rho)$.

 On the other hand, if the original correlation is exponential such that the correlation decreases as time lags increases, $Cor(Y_{j},Y_{l}) = \exp(-|t_{j}-t_{l}|/\rho)$, the resulting covariance after transformation is not a recognizable structure. In fact, the covariance can no longer be written as a function of time lags. The covariance matrix is a linear combination of the original correlation matrix, column and row means, and the overall mean correlation:
   \begin{align*}
 Cov(\B A\B Y) &= \sigma^{2}\left[\B R(\alpha)-m^{-1}\B1_{m}\B1_{m}^{T}\B R(\alpha)-m^{-1}\B R(\alpha)\B1_{m}\B1_{m}^{T} + m^{-2}\B1_{m}\B1_{m}^{T}\B R(\alpha)\B1_{m}\B1_{m}^{T}\right]\\
  &= \sigma^{2}\left[\B R(\alpha)-\text{ column mean vector }-\text{ row mean vector } + \text{ overall mean}\right].
 \end{align*} 
 This non-stationary covariance matrix includes negative correlations when the mean of the correlations within each column and within each row are positive and substantial. For example, if $\sigma^{2}=1$, $\alpha = 2$ and $\B t=(1,2,3,4)$, then the symmetric covariance matrix of the transformed vector is
$$ Cov(\B A\B Y) = \left[ \begin{array}{cccc}
 0.499&  0.009& -0.229& -0.278\\
  0.009&  0.307& -0.087& -0.229\\
 -0.229& -0.087&  0.307&  0.009\\
 -0.278& -0.229&  0.009&  0.499
\end{array}\right]$$
The variance and covariance changes over time with the correlation becoming negative as the time lag increases.  If the number of observation times increase such that the observation period increases as well, the covariance of the transformed vector will be close to the original covariance as column, row, and overall means will decrease to zero as the number of large time lags increases. However, if the observation period remains fixed as the number of observations increases, the covariance after transformation continues to be non-stationary and have negative correlations.  

We have calculated the covariance of the transformed random vector when we know the original covariance structure. In practice, we need to estimate the covariance based on observed data vectors of length $m$. We showed that if prior to transformation, the errors are independent or exchangeable, the correlation of the resulting transformed data is exchangeable equal to $\frac{-1}{m-1}$. This magnitude has significant meaning as it is the smallest correlation allowed in an exchangeable matrix while still being a correlation matrix. This means that the true parameter value is on the boundary of the parameter space. Regularity conditions for maximum likelihood are not met; consistency and asymptotic normality no longer hold. Additionally, the determinant of the covariance matrix for the transformed vector is zero and thus not invertible. Even though we know the true structure is exchangeable, estimating a model with exchangeable structure is difficult. Since the value of the correlation is so small, it may be best to assume conditional independence in this situation. 
 
\subsubsection{Random observation times} 
Up until now, we have assumed that the observation times are fixed. However, in most cases, individuals are not observed at exactly the same times but rather at random times. When the times are random, the deterministic function is evaluated at different times resulting in a random average values of function values. Therefore, the transformed vector will not only have variability due to the errors but also an induced random intercept from the average function value. For simplicity, imagine if the outcome had no error such that
$\B Y =  \bs\mu(\B t). $
If we evaluate the function at different sets of observation times, we induce a random intercept by subtracting the means, the variance of which depends not only on the variability in the random times but also the derivative of the function. When the original covariance of the error is independent or exchangeable, the induced random intercept is independent of the transformed errors; therefore, the covariance of the transformed outcomes is the sum of the two resulting in an exchangeable structure. Fortunately, the magnitude of the correlation should be far enough from the boundary for possible estimation when the function has a large derivative in places of irregular sampling.

On the other hand, if the original covariance is dependent on the observation times such as the exponential structure, the random intercept and error structure have a complex relationship both being indirectly dependent on the times of observation. We explore the impact of transforming the data through empirical simulations. We assume that observation times are equal to random perturbations around specified goal times such that $\B t = \B T + \B \tau$ where $\tau\sim N(0,\sigma^{2}_{tau})$ and $\B T = (1,2,...,9,10)$. We generate $n=300$ data realizations such that
$$\B y_{i} = \mu(\B t_{i}) + \epsilon_{i}\quad\text{ where }\epsilon_{i}\sim N(0,\B R(\rho))$$
where $\B R(\rho)$ is an exponential correlation matrix with $\rho=2$ assuming different functions, $\mu(t)$, and variances of $\tau$. Figure \ref{fig:cov} shows the estimated autocorrelation functions under different conditions for observations times and shape functions.
\begin{figure}
\begin{center}
\includegraphics[height=3.3in]{Chp3Cov}
\end{center}
\caption{Estimated autocorrelation functions from data generated with an exponential correlation error structure and perturbed observation times under different mean functions, $\mu(t)$, and variance of the perturbation.}
\label{fig:cov}
\end{figure}
As the variance of the perturbation in observation time and the magnitude of the derivative mean function increases, the estimated correlation at large lags becomes more positive. Once the curve is fully above the x-axis, the function resembles a scaled down and shifted version of the exponential function with the asymptote greater than zero. The transformation can occur with an additive model combining a random intercept plus exponential correlation  \cite{diggle2002}.

In practice, if the data are regularly sampled, true negative correlations are problematic for estimation and an independence or exponential correlation structure may be the best option. If the data are irregularly sampled, an additive model the combines a random intercept with the exponential correlation may be appropriately flexible. 

\subsubsection{Unbalanced observation times}
In addition to the issues of fixed versus random sampling, having an unequal number of observations can impact the estimation of covariance of transformed vector. As shown above, the length of the vector impacts the resulting covariance. Suppose we have a sample of individuals observed a different number of times. Even if a sample shared the same mean shape and covariance structure, transforming the vectors by subtracting the mean induces different covariance structure for each individual. If there is quite a bit of variability in the number of observations, it may impact clustering to assume they share the same covariance structure during the estimation/clustering process. However, if the number of observation times is high for all subjects and the observation period is long, then the covariance matrices should be similar. 

Additionally, if the unbalanced nature of the data is due to drop outs during the follow up period, clustering based on the shape should be done with caution. If the general shape of the curve during the observation period is not measured adequately, it does not make sense to try and cluster those individuals with the rest who have fully observed curves. 
%\section{Example} for technical report
%Toy examples

\section{Discussion}
In this chapter, I have describe three different approaches to the problem of clustering irregularly sampled longitudinal data by shape. The first focuses on clustering derivative functions, which should contain all the information about the shape ignoring the level. The most difficult aspect of this approach is estimating individual derivative functions. Projecting the data onto a B-spline basis should remove noise and provide a smooth underlying function, which should be an improvement over quotient differences, which simply linearly interpolates the data points with no regard to error. One difficulty with this method is choosing the correct basis so as to not over fit the data when there are only a few data points. While this may improve upon other methods, this approach has limitations. Individuals cannot borrow strength in estimating their derivative function even though we hypothesize it to be a common factor. Also, partitioning methods do not lend themselves to analysis of baseline factors since by definition, there is no uncertainty in group membership.  

Rather than trying to ignore the level, the second approach attempts to directly model the variability in the level by assuming that for each shape group, there are subgroups that differ in level. Assuming a multilayered mixture model provides a probability framework to take into account uncertainty while estimating the relationship between baseline factors and group membership. However, this model assumes that level varies discretely rather than continuously, which is a strong assumption. It is not clear how robust this method is to violations of this assumption.

Lastly, the third approach directly removes the level by subtracting individual-specific means prior to modeling. This allows individuals to be compared without making specific assumptions about the distribution of the level while providing the probability framework.  There are two difficulties with this method. First, subtracting an observed mean impacts the covariance in a way that makes it harder to model with a known correlation structure. Second, care needs to be taken when there is sparse and irregularly sampling. 

We compare these methods in practice with a simulation study in the next chapter.
