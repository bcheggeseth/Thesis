\chapter{Proposed shape-based methods}
\label{chap:methods}
In this chapter, I present three new clustering techniques that attempt to answer the three research questions presented in the last chapter: Are there distinct shape patterns in the longitudinal data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? For each method, I discuss related work and then introduce necessary background, notation, and the model specification. I describe the implementation process and any foreseen issues and limitations. I then discuss the advantages and disadvantages of each. In the next chapter, a simulation study compares the proposed methods with those presented earlier in this thesis in addressing research questions about shape.

\section{Derivative spline coefficients partitioning}
The first method is based on the idea that differentiation removes the level from a function and provides information about the shape. Unlike the difference quotient method described in Chapter 3,  the proposed method smoothes out noise prior to calculating the derivative and does not require the data to be regularly sampled for all subjects. Using techniques from functional data analysis \cite{ramsay2002}, outcome data for each individual are projected onto a functional basis to estimate a smooth function of time. The estimated function is differentiated to get an indirect estimate of the derivative function. The dissimilarity between individuals is based on these estimates. By first removing the level, this  method theoretically clusters individuals with similarly shaped trajectories.

\subsection{Related work}
In Chapter \ref{chap:motivate}, I introduced the difference quotient dissimilarity measure separately suggested by \textcite{d2000} and \textcite{moller2003}. M{\"o}ller-Levet et al. referred to this measure as the short time-series distance and developed it to identify similar shapes in microarray data. In contrast, D'Urso took a physics view of the data and referred to the difference quotient measure as the longitudinal-velocity dissimilarity measure. Additionally, he used a difference quotient in velocities to calculate a measure inspired by acceleration and then combined the cross-sectional and evolutive information of the trajectories into one dissimilarity measure.

Similar to D'Urso, Zerbe presented three distance measures for growth curves based on position, velocity, and acceleration \cite{zerbe1979,schneiderman1993}. Rather than using difference quotients, he suggested estimating the velocity by first fitting a polynomial of degree $d$ to each individual's growth curve using least squares. For individual $i$, he let $\B y_{i}$ be the vector of observed outcomes at times $\B t_{i}$. Then, the estimated curve is $\hat{f}_{i}(t) = (1\;\; t\;\;t^{2}\;\;...\;\;t^{d})\;\hat{\BS\beta}_{i}$ where 
$$\hat{\BS\beta}_{i} = (\B X^{T}_{i}\B X_{i})^{-1}\B X^{T}_{i}\B y_{i}$$
and $\B X_{i}$ is the within-individual design matrix determined by the polynomial function evaluated at the observation times of individual $i$. The estimated derivative is equal to
$\hat{f}_{i}^{'}(t) = [0\;\; 1\;\;2t\;\;...\;\;dt^{d-1}]\;\hat{\BS\beta}_{i}$. In other words, he suggested projecting the data onto a polynomial basis of degree $d$, differentiating the basis, and calculating the estimated derivative function, which can be represented by a polynomial basis of degree $d-1$. The dissimilarity between the $i$th and $j$th individuals based on the velocity equals
$$d(\B y_{i},\B y_{j}) =\left[ \int_{\mathcal{T}} [\hat{f}^{'}_{i}(t)-\hat{f}^{'}_{j}(t)]^{2}dt\right]^{1/2}$$
for a chosen time interval, $\mathcal{T}$. This integral is easy to calculate since the estimated derivatives functions are represented using a polynomial basis.

\Textcite{tarpey2003}, at the end of their functional clustering paper, briefly mentioned a few suggestions to cluster data after getting `rid of dominating variability in the intercept.' One of their proposals was to cluster individuals based on the derivatives of the estimated functions. In personal correspondence with one of the authors, the details of the implementation were clarified. After projecting individuals' data onto a finite Fourier basis, they differentiated the Fourier basis functions and used the K-means algorithm on the coefficients of the derivative functions. Thus, the estimated function for individual $i$ is $\hat{f}_{i}(t)= (1\;\; \sin(t)\;\;\cos(t)\;\;...\;\;\sin(wt)\;\;\cos(wt))\;\hat{\BS\beta}_{i}$ where
$$\hat{\BS\beta}_{i} = (\B X^{T}_{i}\B X_{i})^{-1}\B X^{T}_{i}\B y_{i}$$
and $\B X_{i}$ is the within-individual design matrix determined by the Fourier expansion evaluated at the observation times of individual $i$. Since $\frac{d}{dt}\cos(wt) = -w\sin(wt)$ and $\frac{d}{dt}\sin(wt) = w\cos(wt)$, the estimated derivative function for individual $i$ is represented using the same Fourier expansion with new coefficients 
$$\hat{\BS\alpha}_{i} = \left(\begin{array}{cccccc}
0 & 0&0&\cdots&0&0\\
0&0&-1&\cdots&0&0\\
0&1&0&\ddots&0&0\\
\vdots&\vdots&\ddots&\ddots&\ddots&\vdots\\
0&0&0&\ddots&0&-w\\
0&0&0&\cdots&w&0\end{array}\right)\hat{\BS\beta}_{i}$$
that are transforms of the original coefficients. The dissimilarity between the $i$th and $j$th individuals based on the derivative coefficients equals
$$d(\B y_{i},\B y_{j}) = (\hat{\BS\alpha}_{i}-\hat{\BS\alpha}_{j})^{T}(\hat{\BS\alpha}_{i}-\hat{\BS\alpha}_{j}).$$

These two approaches use the same general procedure. Individual trajectories are smoothed by projecting the data onto a chosen basis, the derivative of the estimated function is calculated by differentiating the basis functions, and the dissimilarity between individuals is based on the estimated derivative functions. Now with any smoothing procedure, there is a fundamental bias-variance tradeoff. In this case, including higher ordered polynomial terms or more sine and cosine functions in the basis decreases the bias but increases the variance. The basis functions need to be selected so as to strike a balance between the two. The type of basis also affects the differentiation process. For example, polynomial and Fourier bases are computationally convenient in that differentiation results in a basis of the same type.

Despite the similarities, these two methods differ in how the dissimilarity measure is defined between two individuals. Zerbe used the $L_{2}$ distance between two derivative functions; Tarpey and Kinateder calculated the squared Euclidean distance between the basis coefficient vectors for the estimated derivative function. This reflects the diversity in the functional cluster analysis literature; some use the $L_{2}$ distance on functions \cite{hitchcock2007} while others calculate the Euclidean distance between the linear coefficients of the basis function \cite{abraham2003,serban2005, tarpey2003}.
 
\Textcite{tarpey2007} reconciled these two dissimilarity measures by showing that clustering functional data using the $L_{2}$ metric on function space can be achieved by running K-means on a suitable linear transformation of the basis coefficients. If $y(t)$ is a functional realization represented as $y(t)=\sum_{j}\beta_{u}u_{j}(t)$and $\mu(t)$ is a functional cluster mean represented as $\mu(t) = \sum_{j}\gamma_{j}u_{j}(t)$, then the squared $L^{2}$ distance between them on interval $\mathcal{T}$ is
\begin{align*}
\int_{\mathcal{T}}(y(t)-\mu(t))^{2}dt &= \int_{\mathcal{T}}(\sum_{j}(\beta_{j}-\gamma_{j})u_{j}(t))^{2}dt\\
&= \sum_{j}\sum_{l}(\beta_{j}-\gamma_{j})(\beta_{l}-\gamma_{l}) \int_{\mathcal{T}}u_{j}(t)u_{l}(t)dt\\
&=(\BS\beta-\BS\gamma)^{T}\B W (\BS\beta-\BS\gamma)\\
&=(\B W^{1/2}(\BS\beta-\BS\gamma))^{T} (\B W^{1/2}(\BS\beta-\BS\gamma))
\end{align*}
where $\B W_{jl} = \int_{\mathcal{T}}u_{j}(t)u_{l}(t)dt$. Therefore, clustering with the $L^{2}$ distance is equivalent to plugging transformed coefficients, $\B W^{1/2}\BS\beta$, into the K-means algorithm. Consequently, when the functions are represented using an orthogonal basis such as the Fourier expansion, K-means on the coefficients is equivalent to the $L^{2}$ implementation.

Both functional bases presented thus far are restrictive. The Fourier basis only works well when the data is periodic in nature and a polynomial basis does not provide a general structure to represent complex functions with few parameters. 
To allow for flexibility in the functional shape, the observation time interval, $[a,b]$, can be broken up into smaller interval using $L$ internal knots, $a<\tau_{1}<\cdots<\tau_{L}<b$, so that polynomials of order $p$ are fit in each subinterval. This piecewise polynomial can be expressed as a linear combination of truncated power functions and polynomials of order $p$. In other words,
$\{1,t,t^{2},...,t^{p-1},(t-\tau_{1})_{+}^{p-1},...,(t-\tau_{L})_{+}^{p-1}\}$
is a basis for a piecewise polynomial with knots at $\tau_{1},...,\tau_{L}$. However, the normal equations associated with the truncated power basis are highly ill-conditioned. 

A better conditioned basis for the same function space is the B-spline basis \cite{deboor1978, schumaker1981,curry1966, de1976}, which extends the advantages of polynomials to include greater flexibility \cite{abraham2003}. To my knowledge, no other study have focused on clustering longitudinal data using the coefficients of the B-spline derivative estimate.

In the following sections, I introduce B-spline functions and demonstrate how they can be used to estimate derivative functions. Then, the implementation and practical decisions that need to be made are presented and discussed.

\subsection{B-spline functions}\label{sec:bsplines}
I fit an $p$-order spline function to each subject $i$ in order to estimate its underlying smooth, $f_i$. For the sake of being self-contained, I include some background on splines. Let $t\in[a,b]$ where $a,b\in\mathbb{R}$ and $\xi_0=a<\xi_{1}<\cdots<\xi_{L} < b = \xi_{L+1}$ be a subdivision of  the interval $[a,b]$ by $L$ distinct points, termed internal knots. The knot sequence is augmented by adding replicates at the beginning and end, $\tau=[\tau_{1},...,\tau_{L+2p}]$ for $p\in\mathbb{N}$, such that 
\begin{align*}
\tau_{1}&=\tau_{2}=\cdots =\tau_{p} =\xi_{0}\\
\tau_{j+p}& = \xi_{j}, \quad\quad j=1,...,L\\
\xi_{L+1}&=\tau_{L+p+1}=\tau_{L+p+2}=\cdots =\tau_{L+2p} 
\end{align*}
The spline function, $f(t)$, is a polynomial of order $p$ on every interval $[\tau_{j-1},\tau_{j}]$ and has $p-2$ continuous derivatives on the interval $(a,b)$. The set of spline functions of order $p$ for a fixed sequence of knots, $\tau = [\tau_1,...,\tau_{L+2p}]$, is a linear space of functions with $L+p$ free parameters. A useful basis $B_{1,p}(t),...,B_{L+p,p}(t)$ for this linear space is given by Schoenberg's B-splines \cite{curry1966, de1976} defined as
\begin{align*}
B_{j,1}(t) &= \begin{cases}
1 \text{ if }\tau_j\leq t < \tau_{j+1}\\
0\text{ otherwise}
\end{cases}\\
B_{j,l}(t) &= \frac{t-\tau_j}{\tau_{j+l-1}-\tau_j} B_{j,l-1}(t)+\frac{\tau_{j+l}-t}{\tau_{j+l}-\tau_{j+1}} B_{j+1,l-1}(t)
\end{align*}
where $l=2,...,p$ and $j=1,...,L+2p-l$.  If I adopt the convention that $B_{j,1}(t)=0$ for all $t\in\mathbb{R}$ if $\tau_{j}=\tau_{j+1}$, then by induction $B_{j,l}(t)=0$ if $\tau_{j}=\tau_{j+1}=\cdots=\tau_{j+l}$. Hence, $B_{1,l}(t)=0$ for $t\in\mathbb{R}$ and $l<p$ on the defined knot sequence. The B-spline function of order $p$ is defined by
$$f(t) = \sum^{L+p}_{j=1} \beta_j B_{j,p}(t).$$

Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote a vector of repeated observations for individual $i$ observed at times $\B t_{i}=(t_{i1},...,t_{im_{i}})$  for $i=1,...,n$. I assume that $y_{ij} = f_{i}(t_{ij}) + \epsilon_{ij}$ such that $E(\epsilon_{ij}) = 0$ for all $j=1,...,m_{i}$ and $i=1,...,n$.  To estimate $f_{i}$, I fix the order of the B-spline to $p$ and the internal knots and then estimate the coefficients, $\BS \beta_{i} = (\beta_{i,1},...,\beta_{i, (L+p)})$ using least squares. The estimated function is $\hat{f}_i(t)=\sum^{L+p}_{j=1} \hat{\beta}_{i,j} B_{j,p}(t)$ where
$$\hat{\BS \beta}_{i} = (\B X_{i}^{T}\B X_{i})^{-1}\B X_{i}\B y_{i}$$
and $\B X_{i}$ is the within-individual design matrix based on the B-spline basis functions. 

To estimate $f_i'(t)$, the estimated function $\hat{f}_i(t)$ is differentiated with respect to $t$ such that
$$\hat{f}'_i(t)=\sum^{L+p}_{j=1} \hat{\beta}_{i,j} B'_{j,p}(t).$$
\Textcite{prochazkova2005} showed that this can be simplified to
$$\hat{f}'_i(t)=\sum^{L+p}_{j=1} \hat{\beta}_{i,j} \left[\frac{p-1}{\tau_{j+p-1}-\tau_j} B_{j,p-1}(t)-\frac{p-1}{\tau_{j+p}-\tau_{j+1}} B_{j+1,p-1}(t)\right].$$
However, this can be written in terms of a B-spline basis of one order lower,
\begin{align*}
\hat{f}'_i(t)&=\sum^{L+p-1}_{j=1} (\hat{\beta}_{i,j+1} -\hat{\beta}_{i,j})\frac{p-1}{\tau_{j+p}-\tau_{j+1} }B_{j+1,p-1}(t).
\end{align*}
Adjusting the knot sequence to only have $p-1$ replicates at the beginning and end results in
$$\hat{f}'_i(t)=\sum^{L+p-1}_{j=1}\hat{\alpha}_{i,j}B_{j,p-1}(t)$$
where $\hat{\alpha}_{i,j}= (\hat{\beta}_{i,j+1} -\hat{\beta}_{i,j})\frac{p-1}{\tau_{j+p}-\tau_{j+1} }$.
These derivative coefficients, $\hat{\BS \alpha}_{i} = [\hat{\alpha}_{i,1},...,\hat{\alpha}_{i,(L+p-1)}]$, can be used to cluster trajectories with similar shape with the K-means algorithm \cite{macqueen1967, hartigan1979}. 

\subsection{Implementation}
To use this method in practice, decisions about the B-spline basis need to be made. The order of the polynomials must be selected. This together with the number of internal knots impacts the flexibility of the B-spline function. Cubic B-splines of order four have been shown to have good mathematical properties and are used frequently in practice \cite{james2003}. However, depending on the number of data points observed per subject, it may be necessary to use a quadratic polynomial ($p = 3$) due to the restriction that the sum of the order and the number of internal knots must be less than or equal to the minimum number of observation points per subject for estimation to be possible. 

The number of internal knots $L$ plays a role in the flexibility of the class of spline functions and should be enough to fit the features in the data. As with the order, the main limiting factor in choosing $L$ is the number of data points per subject. The longitudinal data sets considered in this thesis have about five to ten data points per subjects. It is important not to over fit the individual curves so for data with limited observation, it may only be possible to have at most one internal knot. If there are many repeated measures, model selection information criteria or cross-validation can be used to choose $L$ \cite{rice2001}. The location of the internal knots is another issue of discussion. There are some suggested data-driven ways to select knot location \cite{shanggang2001}, but \textcite{ruppert2002} supports fixing the knots at sample quantiles. I generally follow this suggestion and adjust them as necessary to the areas with the most functional activity. 

Once the order of the polynomials and number and location of knots are chosen, then smooth functions and their derivatives are estimated using least squares separately for every subject. The coefficients from the estimated derivative functions become the input vectors to the K-means algorithm (see Chapter \ref{chap:intro} for more details). This algorithm converges but there is no guarantee that it will find the grouping that globally minimizes the objective function. Therefore, in practice, the algorithm is run multiple times with different random initializations and the clustering that minimizes the objective function is chosen. I use 25 random starts.

The number of clusters $K$ must be fixed in order to run the K-means algorithm. However, $K$ is unknown and of interest to researchers in practice. While no perfect mathematical criterion exists, a number of heuristics (see \cite{tibshirani2001} and discussion therein) are available for choosing $K$. For this thesis, $K$ is chosen for partition methods so as to maximize the overall average silhouette width \cite{rousseeuw1987}. See Chapter \ref{chap:intro} for technical details of the silhouette width. By maximizing the overall average silhouette width, the dissimilarity between clusters is maximized and the dissimilarity within clusters is minimized resulting in distinct groups. In this case, the dissimilarity between two individuals is defined as the squared Euclidean distance between derivative coefficients.

K-means is a partitioning algorithm, therefore, by definition every subject is hard clustered into one of the groups. There is no stated uncertainty in the group memberships even if a subject is between two clusters. In order to estimate the relationship between baseline variables and shape group membership, I assume the cluster labels are known. Given the subject grouping labels from the partition, $\{c_{i}\}$ such that $c_{i}\in\{1,2,...,K\}$ for all $i=1,...n$, I fit a multinomial logistic regression model, which is an extension of the logistic regression model, using the group labels as the outcome and baseline factors as explanatory variables such that
$$P(c_{i} = k|\B w_{i}) = \frac{\exp(\B w_{i}^{T}\BS\gamma_{k})}{\sum_{j=1}^{K}\exp(\B w_{i}^{T}\BS\gamma_{j})}$$
for all $k=1,...,K$ and $i=1,...,n$ with $\BS\gamma_{K}=0$ where $\B w_{i}$ is the design vector based on baseline factors. The parameters $\BS\gamma_{1},...,\BS\gamma_{K-1}$ are estimated via maximum likelihood estimation. However, it is important to note that the estimated standard errors from a Hessian calculation do not include any group membership uncertainty  as the hard clustering labels are assumed known for the estimation process.


\section{Multilayer mixture model}\label{sec:multi}
The second method attempts to make up for the lack of uncertainty in groups memberships by using a model-based approach. Traditional clustering methods fail when the level and shape are weakly dependent resulting in individuals with the same shaped trajectory at different levels. As mentioned previously, if the levels within a shape group are normally distributed, using an exchangeable correlation matrix in a finite mixture model takes that variability into account. However, if the levels do not satisfy those assumptions, a Gaussian mixture could be used to model the non-Gaussian distribution of levels within a shape group. 

\subsection{Related work}
Model-based methods provide the probability framework that dissimilarity-based methods lack. One benefit is the ability to simultaneously take into account uncertainty about of the parameter estimates and cluster membership. As mentioned in Chapter \ref{chap:intro}, the main model-based method is the finite mixture model. However, a standard mixture of Gaussians fit to the original data does not distinguish between the level and shape and clusters subjects based on the dominant source of variability. Another limitation of Gaussian mixtures for grouping data is that it cannot handle clusters that are non-normal. For this reason, the mixture model has been extended to include multiple layers such that each cluster is allowed to be a mixture \cite{li2005}.  For example, an obvious model for a data set with two groups each with bimodal densities would be a multilayer mixture of two clusters each with two components (see Figure \ref{fig:dia} for a diagram of the model structure). A variation of this idea has been used to cluster non-normal groups by fitting a mixture with many components and then systematically combining components to make more meaningful clusters \cite{hennig2010}. 
\begin{figure}[h]
\centering
\includegraphics[width=4in]{Chp4multilayer_diagram}
\caption{Diagram of multilayer mixture model showing that each cluster is composed of potentially more than one component.}
\label{fig:dia}
\end{figure}

In this thesis, the goal is to have clusters be meaningful in terms of distinguishing between shape. Therefore, the idea of multilayer mixtures can be used to model $K$ non-normal shape clusters composed of mixture components with the same mean shape at different levels.


\subsection{Model specification}
In a multilayer mixture model, $K$ is the number of clusters and $J_{k}$ is the number of components in the $k$th cluster ($k=1,...,K$) such that $J=\sum_{k=1}^{K}J_{k}$ is the total number of components in the entire model. Let $j$ uniquely index all of the components such that $j=1,...,J$. For ease of explanation, let $c(j):\{1,...,J\}\rightarrow \{1,2,...,K\}$ be a cluster assigning function that specifies the cluster to which a component belongs. Let $f(\B y|\BS\mu,\BS\Sigma)$ be the probability density function of a multivariate normal distribution with mean vector $\BS\mu$ and covariance matrix $\BS\Sigma$. Then, the probability density function for cluster $k$ is
$$f_{k}(\B y) = \sum_{j: c(j) = k} \pi_{j|c(j)}f(\B y|\BS\mu_{j},\BS \Sigma_{j})$$
where $\pi_{j|c(j)}$ is the probability of being in component $j$ given a subject is in cluster $c(j)$ and $\sum_{j: c(j) = k}\pi_{j|c(j)}=1$ for all $k=1,2,...,K$. Let the probability of cluster $k$ given baseline factors, $\B w$, be $\pi_{k}(\B w,\BS\gamma)=\exp(\B w^{T}\BS\gamma_{k})/\sum^{K}_{l=1}\exp(\B z^{T}\BS\gamma_{l})$ such that $\BS\gamma_{K}=0$ and $\BS\gamma=(\BS\gamma_{1},...,\BS\gamma_{K})$. The probability density function for the multilayer mixture is written as
$$g(\B y|\B w) = \sum_{k=1}^{K}\pi_{k}(\B w,\BS \gamma)f_{k}(\B y) = \sum_{k=1}^{K}\pi_{k}(\B w,\BS\gamma)\sum_{j: c(j) = k} \pi_{j|c(j)}f( \B y| \BS\mu_{j},\BS\Sigma_{j}).$$
Since every component belongs to one and only one cluster, the above equation reduces to a regular mixture model if $\bar{\pi}_{j}(\B w,\BS \gamma)=\pi_{c(j)}(\B w,\BS\gamma)\pi_{j|c(j)}$ with the density written as
$$g(\B y|\B w) = \sum_{j=1}^{J}\bar{\pi}_{j}(\B w,\gamma)f(\B y|\BS \mu_{j},\BS\Sigma_{j}).$$

To adjust this model to satisfy the goal of clustering on shape, individuals in the clusters are assumed to have the same shaped trajectory over time and the varying levels are modeled by a mixture of components. The component mean structure includes a regression parameterization to model the smooth underlying group mean shape function over time plus a component-specific level. The design matrix for the regression can easily include B-spline basis functions presented in Section \ref{sec:bsplines}. Then, the regression parameters are constrained to be the same within shape clusters and the intercepts are allowed to differ in each level component. Hence, $\BS\mu_{j}=\lambda_{j}\B 1+\B x \BS\beta_{c(j)}$, where $\B x$ is a design matrix of B-spline basis functions excluding the first basis function to allow for estimation of intercept terms for each component. This allows the clusters to be based on shape while the components can have different levels (see Figure \ref{fig:diashape} for a diagram of the model structure). Additionally, I assume conditional independence within components, $\BS\Sigma_{j}=\sigma^{2}_{j}\B I$ for all $j=1,...,J$,  for simplicity and to allow for irregularly sampled longitudinal data.
\begin{figure}[h]
\centering
\includegraphics[width=4in]{Chp4multilayer_diagram_shape}
\caption{Diagram of multilayer mixture model showing that each shape cluster is composed of potentially more than one level component.}
\label{fig:diashape}
\end{figure}

\subsection{Implementation}
Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote a vector of repeated observations for individual $i$ observed at times $\B t_{i}=(t_{i1},...,t_{im_{i}})$  for $i=1,...,n$. For each individual $i$, B-spline basis functions are evaluated at the observation times to create design matrices $\B x_{i}$ for the mean regression and baseline covariates are combined in design vectors $\B w_{i}$ for the group membership regression. Now, B-splines are used so the mean structure is flexible enough to accommodate complex shapes. The number and location of internal knots are dealt with in the same manner as in the first proposed method.

Denote the shape cluster identity of individual $i$ by $\eta_{i}$ where $\eta_{i}\in\{1,...,K\}$. Then, the parameters of the multilayer mixture model $\BS\eta=(\eta_{1},...,\eta_{n})$ and $\BS\theta=\{\BS \gamma_{k}, \pi_{j|c(j)},\lambda_{j}, \BS\beta_{k},\sigma^{2}_{j};\; j=1,...,J, k=1,...,K\}$  are estimated by maximizing the classification log-likelihood function,
\begin{align}
 L(\BS\theta,\BS\eta) &= \sum^{n}_{i=1} \log \pi_{\eta_{i}}(\B w_{i},\BS\gamma) f_{\eta_{i}}(\B y_{i}|\B x_{i})\nonumber\\
\label{ll}& =  \sum^{n}_{i=1} \log\left[ \pi_{\eta_{i}}(\B w_{i},\BS\gamma)  \sum_{j: c(j) = \eta_{i}} \pi_{j|c(j)}f(\B y_{i}|\lambda_{j}\B 1 + \B x_{i}\BS\beta_{\eta_{i}},\sigma^{2}_{j}\B I)\right]
\end{align} 
using a modified EM algorithm called the classification expectation maximization algorithm (CEM) \cite{celeux1992, mclachlan2000}. The modification involves adding a classification step between the expectation step and maximization step where individuals are assigned to shape clusters. 

In order to start the iterative algorithm, the individuals are initially assigned into shape clusters and component groups. Initialization can involve randomly partitioning individuals into components or strategically partitioning individuals into shape clusters using a computationally fast procedure such as the first proposed method of this thesis and then randomly partition the individuals into components. 

Let $\BS\theta^{(t)}$ and $\BS\eta^{(t)}$ be the current estimates of the parameters at the $t$th iteration of the algorithm. The modified EM algorithm updates these estimates as follows:
\begin{enumerate}
\item Expectation step: For each individual $i$, compute the posterior probability of being in shape cluster $k$
$$p_{i,k}= \pi_{k}(\B w_{i},\BS\gamma)f_{k}(\B y_{i}|\B x_{i})/\sum^{K}_{j=1} \pi_{j}(\B w_{i},\BS\gamma)f_{j}(\B y_{i}|\B x_{i})$$
for $i=1,...,n$ and $k=1,...,K$.
\item Classification step: Hard classify subjects to shape clusters according to $\eta^{(t+1)}_{i} = \arg\max_{k} p_{i,k}$.
\item Maximization step: For each shape cluster, use maximum likelihood estimation to update parameter vector $\BS\theta$ by embedding an EM procedure initialized with the current parameter values.
\end{enumerate} 

This algorithm increases the classification log-likelihood at each iteration. The statement is below and the proof is in Appendix \ref{append:2}. 
\begin{theorem}\label{Thm:CEM} The classification likelihood $L(\BS\theta,\BS\eta)$ defined in equation (\ref{ll}) is non-decreasing after each update of $\BS\eta$ and $\BS\theta$ by the CEM algorithm. That is, $L(\BS\theta^{(t+1)},\BS\eta^{(t+1)}) \geq L(\BS\theta^{(t)},\BS\eta^{(t)})$ for all $t$.
\end{theorem}

In the maximization step of the algorithm, parameters are estimated by maximizing the likelihood for each shape cluster. However, the shape parameters are constrained to be equal for all components within the cluster. To estimate both the component and cluster-specific parameters simultaneously, this thesis uses computational methods suggested by \textcite{grun2008}. For shape cluster $k$, the outcome vector for subject $i$ is temporarily replaced by a vector of $\B y_{i}$ repeated $J_{k}$ times. The new design matrix  is set equal to $(\B I_{J_{k}}\otimes \BS 1_{m_{i}}, \BS 1_{J_{k}} \otimes \B x_{i})$ where $\otimes$ refers to the Kronecker product. Lastly, the covariance matrix structure is block diagonal with $\sigma^{2}_{j}\B I_{m_{i}}$ in each block for $j$ that satisfy $c(j)=k$. The likelihood function is maximized with respect to the parameters for cluster $k$ and its components using profile likelihoods and a standard numerical optimization routine.

With this complex mixture structure, it is necessary to select the number of shape clusters as well as the number of components for each cluster $k$. It is recommended to fix $K$ and then use model selection criteria to select $J_{k}$ for each $k=1,...,K$. As suggested by \textcite{li2005}, the BIC is used to select $J_{k}$ even though the regularity conditions do not hold as the criteria has been shown to be a useful informal guide in practice.

Robust standard errors for the parameter estimates can be found following the same procedure as \textcite{boldea2009}.

\section{Vertical shifted mixture model}
The goal of cluster analysis is to partition a collection of individuals into homogeneous groups. In this thesis, similar individuals are those with the same outcome shape pattern over time. If two curves only differ in intercepts by vertical shifts, they are placed in the same group. The first proposed method uses derivatives to implicitly remove the level and the second method directly models the variability in the level with an additional layer of mixture models.  In this method, I consider subtracting the subject-specific mean from the outcome measurements to remove the level. A finite mixture of densities with a mean shape curve and a covariance function is fit to the vertically shifted data. 

\subsection{Recent work}
A finite mixture model is a standard method for clustering multivariate data \cite{everitt2009} and has been used for longitudinal applications \cite{muthen2010, jones2001}. See Chapter 1 for an extensive summary of finite mixture models. However, for longitudinal data, the models are commonly used for the observed data without much regard to the goal of clustering by shape. 

I suggest removing the level by subtracting out the mean outcome level prior to modeling. Subtracting the mean is not a novel idea in statistics or even cluster analysis. In fact, experimental data such as gene expression microarrays are often normalized to compensate for variability in the measurement device between samples. In cluster analysis of multivariate data, it is recommended that each variable is standardized by subtracting the mean and dividing by the standard deviation within each variable so the variables are in comparable units and equally contribute to the grouping process. This is not recommended for the longitudinal setting where each variable is a repeated measurement at a different time point. 

To compare shapes, we want to maintain the original scale since the relationship between measurements within individuals is of interest. A translation via centering \cite{chiou2008} or vertically shifting preserves the shape of the data over time. In general, pre-processing the data can provide a path to answering the research question but any transformation of the data should be carefully studied for potential unintended consequences.

\subsection{Model Specification}\label{sub:vmod}
Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote an outcome vector of repeated observations for individual $i$, $i=1,...,n$. The vector of corresponding times of observation for individual $i$ is denoted as $\B t_{i}=(t_{i1},...,t_{im_{i}})$ and $\B w_{i}$ is a $q$-length design vector based on time-fixed factors that are typically collected at or before time $t_{i1}$. I assume that there are $K$ mean shape functions $\mu_{k}(t)$ in the population such that the outcome vector for individual $i$ in shape group $k$ is
 $$\B y_{i} = \lambda_{i}\B 1_{m_{i}}+\BS\mu_{i}+\BS\epsilon_{i},\quad \lambda_{i}\sim F_{\lambda}, \quad \BS\epsilon_{i}\sim N(0,\BS\Sigma_{k})$$
 where $F_{\lambda}$ is a probability distribution, $\B 1_{m_{i}}$ is an $m_{i}$-length vector of 1's, and $\mu_{ij} = \mu_{k}(t_{ij})$ is the $j$th element of a $m_{i}$-length vector of mean values evaluated at the observation times, $\B t_{i}$. The outcome vector is determined by a mean shape function, a random intercept, and potentially correlated random errors. The probability of having a particular shape could depend on baseline covariates. Let $\bar{y}_{i}= m_{i}^{-1}\sum^{m_{i}}_{j=1} y_{ij} = \lambda_{i}+\bar{\mu}_{i}+\bar{\epsilon}_{i}$ be the mean of the outcome measurements for individual $i$. This measure of the vertical level of the data vector can be removed by applying a linear transformation, $\B A_{i} = \B I_{m_{i}} - m_{i}^{-1}\B 1_{m_{i}}\B 1_{m_{i}}^{T}$, to the vector of observations. The vertically shifted vector for individual $i$ equals 
\begin{align*}
\B y^{*}_{i} &= \B A_{i}\B y_{i}\\
&=\B A_{i}(\lambda_{i}\B 1_{m_{i}}+\BS\mu_{ik}+\BS\epsilon_{i})\\
&=\B A_{i}(\BS\mu_{i}+\BS\epsilon_{i})\\
&=\BS\mu_{i} - \bar{\mu}_{i}+\BS\epsilon_{i}-\bar{\epsilon}_{i}.
\end{align*}
Applying the symmetric matrix $\B A_{i}$ to the vector $\B y_{i}$ subtracts the individual mean $\bar{y}_{i}$ from each element $\B y_{i}$. This results in the removal of the random intercept $\lambda_{i}$, leaving the mean function evaluated at the observation times plus random error shifted by a random constant $\bar{\mu}_{i}+\bar{\epsilon}_{i}$. Clearly, we do not have to worry about $F_{\lambda}$, the distribution of the random intercept, or any other time-fixed factors that only impact the level of the outcome. 

Once the level is removed, I assume the vertically shifted data $\B y_{i}^{*}$ follow a Gaussian mixture of $K$ groups with mean shape functions and random errors. If the observation times are fixed, vertically shifted data generated from the specified model follows this Gaussian mixture. Thus, conditional on observation times $\B t$ and baseline covariates $\B w$, $\B y^{*}$ is assumed to be a realization from a finite mixture model with density
\begin{align*}
 f(\B y^{*}|\B t,\B w,\BS\theta) =  \sum^{K}_{k=1}\pi_{k}(\B w,\BS \gamma)f_{k}( \B y^{*}|\B t,\BS\theta_{k})
\end{align*}
where $\pi_{k}(\B w,\BS \gamma)$ is the prior probability of being in the $k$th shape component given baseline covariates, $\B w$. The full vector of parameters for the model is $\BS\theta = (\BS\gamma,\BS\theta_{1},...,\BS\theta_{K})$. To allow baseline covariates to impact the probability of having a certain shape pattern over time, the prior probabilities are parameterized using the generalized logit function of the form
$$\pi_{k}(\B w,\BS\gamma)=\frac{\exp(\B w^{T}\BS\gamma_{k})}{\sum_{j=1}^{K}\exp(\B w^{T}\BS\gamma_{j})}$$ 
for $k=1,...,K$ where $\BS \gamma_{k}\in\mathbb{R}^{q}$, $\BS\gamma = (\BS\gamma_{1},...,\BS\gamma_{K})$, and $\BS\gamma_{K}=\B 0$. For continuous outcome vectors, the component densities $f_{k}(\B y^{*}|\B t,\BS\theta_{k})$ are multivariate Gaussian densities with mean and covariance dependent  on time.

\subsubsection{Mean Structure}
Only focusing on shape, the mean is modeled as a smooth function of time represented by a chosen functional basis. If the shape is periodic in nature, a Fourier basis is appropriate. Another common basis is a polynomial basis such as a quadratic or cubic basis. However, this type of basis cannot capture complex shapes with local changes with only a few parameters. A B-spline basis is appropriate in many public health circumstances.

A B-spline function of order $p$ with $L$ internal knots, $\tau_{1},...,\tau_{L}$, is defined by a linear combination of coefficients and B-spline basis functions
$$\mu(t) = \sum^{L+p}_{j=1} \beta_j B_{j,p}(t)$$
where the basis functions $B_{j,p}(t)$ are defined iteratively \cite{deboor1972,cox1972}  (see Section \ref{sec:bsplines} for more details). Values from the $p$th order B-spline basis functions taken at observation times $\B t_{i}$ can be used in a design matrix $\B x_{i}$ to linearly model the mean vector. Thus, the mean of the $k$th shape cluster is approximated by the linear function $\mu_{k}(t) = \sum^{L+p}_{j=1} \beta_{k,j} B_{j,p}(t)$. In the multivariate form, the mean vector at observation times $\B t_{i}$ equals $\B x_{i}\BS\beta_{k}$ where $\BS\beta_{k}=(\beta_{k,1},...,\beta_{k, L+p}).$  

\subsubsection{Covariance Structure}
There are many potential assumptions to be made about the covariance matrix. Here, we allow the covariances to differ between clusters. Since it is common for longitudinal data to have sparse, irregular time sampling, we need to impose structure on the covariance matrix to allow for parameter estimation as described by \textcite{jennrich1986} in their seminal paper. A common structure is conditional independence with constant variance where $\B \Sigma_{k}= \sigma_{k}^{2}\B I_{m_{i}}$. This is typically an unrealistic assumption for longitudinal data since there is inherent dependence between repeated measures on the same unit. Compound symmetry, which is also known as exchangeable correlation, is a popular correlation structure in longitudinal analysis where all repeated measures are equally correlated. This is typically paired with constant variance such that $\B \Sigma_{k} = \sigma_{k}^{2}(\rho_{k}\B1_{m_{i}}\B1_{m_{i}}^{T}+(1-\rho_{k})\B I_{m_{i}})$ where $-1\leq\rho_{k}\leq 1$ is the correlation between any two distinct measurements within an individual. This dependence structure describes the resulting correlation matrix of a random intercept model.

Another structure that provides a compromise is the exponential correlation structure in which the dependence decays as the time between observations increases such that the element in the $j$th row and $l$th column of $\BS\Sigma_{k}$ is $\sigma_{k}^{2}\exp(-| t_{ij}-t_{il}| / r_{k})$ where $r_{k}> 0$ is the range of the dependence. If the range $r_{k}$ is small, the correlation decays quickly, but if $r_{k}$ is large, there is long range dependence between measurements within an individual. This structure is similar to the correlation matrix generated from a continuous autoregressive model of order one such that the element in the $j$th row and $l$th column of $\BS\Sigma_{k}$ is $\sigma^{2}\rho_{k}^{|t_{ij}-t_{ill}|}$ where $\rho_{k}$ is the correlation for measurements observed one unit of time apart. If $\rho_{k} = \exp(-1/r_{k})$, then the two parameterization result in the same structure as long as the correlation between two measures is constrained to be positive. This is a reasonable assumption for longitudinal data in the original form but it many not be acceptable for the transformed data as discussed later. It is important to model the covariance structure correctly as removing the vertical level increases the potential overlap between shape components (see Chapter \ref{chap:misspecify}). 

\subsection{Implementation}
Given a collection of independent observed outcome vectors from $n$ individuals, $\B y_{1},...,\B y_{n}$, we remove the level by subtracting the subject-specific mean from the observed outcome vector, $y^{*}_{i} = \B y_{i} - \bar{y}_{i}$ for $i=1,...,n$. After a visual inspection of the data, the order of the spline and the number and location of internal knots for the B-spline mean structure is chosen. Adding knots and increasing the order of the spline functions flexibly accommodates the twists and turns of the mean patterns but also increases the number of parameters. The knots can be placed at local maxima, minima, and inflection points of the overall trends \cite{eubank1999} or at sample quantiles based on the sampling times of all the observations \cite{ruppert2002}. Design matrices $\B x_{i}$ are calculated using widely available B-spline algorithms for $i=1,...,n$. 

Under the assumption that $\B y^{*}_{1},...,\B y^{*}_{n}$ are independent realizations from the mixture distribution $f(\B y^{*} | \B t, \B w, \BS\theta)$ defined in Section \ref{sub:vmod}, the log-likelihood function for the parameter vector $\BS \theta$ is given by
$$\log L(\BS\theta)=\sum^{n}_{i=1}\log f(\B y^{*}_{i}|\B t_{i},\B w_{i},\BS \theta).$$
The maximum likelihood estimate of $\BS\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\BS\theta)/\partial \BS\theta=\B 0.$ Solutions of this equation corresponding to local maxima can be found iteratively through the EM algorithm \cite{dempster1977} (see Section \ref{sec:em} for technical details about the EM algorithm).

Estimation requires the number of clusters, $K$, to be known. In practice, this is not the case and $K$ is chosen. The most popular way to choose $K$ is by setting a maximum value such that $K_{max}<n$, fitting the model under all values of $K=2,...,K_{max}$, and choosing the value that optimizes a chosen criteria. In this thesis, I use is the Bayesian Information Criterion (BIC) \cite{schwarz1978}, defined as
$$BIC = -2\log L(\hat{\BS\theta})- d\log(n)$$
where $d$ is the length of $\BS\theta$, the number of parameters in the mixture model, and $L(\BS\theta)$ is the likelihood function for the parameter vector.

There are issues of identifiability with Gaussian mixture models that can be mitigated through some minor constraints \cite{mclachlan2000}. Next, we explore some unique consequences of vertically shifting the data on modeling and estimation.

\subsubsection{Covariance of vertically shifted data}
Let $\B Y=(Y_{1},...,Y_{m})$ be a random vector observed at times $\B t=(t_{1},...,t_{m})$ such that
$\B Y = \lambda\B 1_{m} + \BS\mu + \BS\epsilon$
where $\lambda\sim F_{\lambda}$, $\BS\mu$ is a vector of evaluations of a function $\mu(t)$ at times $\B t$, and $\BS\epsilon\sim(0,\BS\Sigma)$. Let $\B\Sigma =\B V^{1/2}\B R(\rho)\B V^{1/2}$ where $\B R(\rho)$ is an $m\times m$ correlation matrix based on the parameter $\rho$ and potentially the associated observation times, and $\B V$ is a $m\times m$ matrix with variances along the diagonal. 

 One important property of this transformation is that it is non-invertible; once the mean is subtracted from the data, the original data cannot be recovered. This has an impact on the correlation structure of the data. The covariance of the transformed random vector after removing the mean vector equals
\begin{align*}
Cov(\B Y^{*} - \BS\mu) &= Cov(\B A(\lambda\B 1_{m} + \BS\mu + \BS\epsilon) - \BS\mu)\\
&= Cov((\B A-\B I_{m})\BS\mu + \B A \BS\epsilon).
\end{align*}
If the observation times are fixed, then $\BS \mu$ is not random and the covariance matrix $\B A Cov(\BS\epsilon)\B A^{T}$ is singular since $det(\B A) =0$. However, if the observation times are random, then $\BS \mu$ is a random vector and contributes to the overall variability. To better understand how to model the transformed data, we explore the covariance when the observation times are fixed and random. From this point on, $\B I_{m}$ will be written as $\B I$ and $\B 1_{m}$ as $\B 1$ for simplification.\\

\noindent \textbf{Fixed observation times}

If the observation times $\B t$ are fixed, then
\begin{align*}
Cov(\B Y^{*} - \BS\mu) &= \B A\BS\Sigma \B A^{T}
\end{align*}
where $\BS\Sigma$ is the covariance of the original random errors. If the variance is constant over time, $\B V=\sigma^{2}I$, and the elements of the original vector are independent, $\B R_{i}(\rho)=\B I$, then the covariance can be written as
\begin{align*} 
Cov(\B Y^{*}- \BS\mu) &=\sigma^{2}\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)\B I)
\end{align*}
 where $a=\frac{-1}{m-1}$. Therefore, if the observation times are fixed and the data has independent errors, subtracting the estimated mean induces negative exchangeable correction between the observations of magnitude $\frac{-1}{m-1}$. Additionally, the variance decreases to $\sigma^{2}\frac{m-1}{m}$.
 
 If the errors in the original data have constant variance, $\B V=\sigma^{2}\B I$, and are exchangeable with $\B R(\rho) = \rho\B 1 \B 1^{T} + (1-\rho)\B I$, then the covariance is written as
 \begin{align*}
 Cov(\B Y^{*}- \BS\mu) &=\sigma^{2}(1-\rho)\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)\B I)
 \end{align*} 
 where $a=\frac{-1}{m-1}$. This transformation maintains the exchangeable structure but with negative correlation on the off diagonal and decreased variance of $\sigma^{2}(1-\rho)\left(\frac{m-1}{m}\right)$.

 On the other hand, if the original correlation is exponential such that the correlation decreases as time lags increases, $Cor(Y_{j},Y_{l}) = \exp(-|t_{j}-t_{l}|/\rho)$, the resulting covariance after transformation is not a recognizable structure. In fact, the covariance can no longer be written as a function of time lags. The covariance matrix is a linear combination of the original correlation matrix, column and row means, and the overall mean correlation,
   \begin{align*}
 Cov(\B Y^{*}- \BS\mu)  &= \sigma^{2}\left[\B R(\rho)-m^{-1}\B1\B1^{T}\B R(\rho)-m^{-1}\B R(\rho)\B1\B1^{T} + m^{-2}\B1\B1^{T}\B R(\rho)\B1\B1^{T}\right]
 \end{align*} 
 This non-stationary covariance matrix includes negative correlations when the mean of the correlations within each column and within each row are positive and substantial.
 
We have calculated the covariance of the transformed random vector under three common covariance structures for the original data assuming fixed observation times. All of these covariance matrices are not invertible since $det(\B A) = 0$. In particular, if prior to transformation, the errors are independent or exchangeable, the correlation of the resulting transformed data is exchangeable equal to $\frac{-1}{m-1}$. This particular value has significant meaning as it is the lower bound for correlation in an exchangeable matrix. This means that the true parameter value of the correlation for the transformed vector is on the boundary of the parameter space. Therefore, even if the true structure is known, estimating parameters for the true model is difficult. Conditional independence or the exponential structure may be an adequate approximation to regularize the estimation, especially if $m$ is moderately large.\\

\noindent \textbf{Random observation times}

In practice, individuals in a longitudinal study are not typically observed at exactly the same times but rather at sporadic times. When the times are random, $\BS\mu$ is random because the elements are evaluations of the function $\mu(t)$ at random times. Therefore, the transformed vector has variability due to the random times in addition to the errors. 

If the covariance of the original errors $\BS \Sigma$ does not depend on time, then the covariance simplifies to
\begin{align*}
Cov(\B Y^{*} - \BS\mu)&=m^{-2}\left(\sum^{m}_{j=1}Var(t_{j})[\mu'(E(t_{j}))]^{2}\right) \B 1\B 1^{T}  + \B A\BS\Sigma\B A^{T}
\end{align*}
using the delta method assuming the random times $t_{1},...,t_{m}$ are independent. This matrix is the sum of two non-invertible matrices, which need not be non-invertible. In fact, if the variance of the times and/or the derivative of the deterministic function $\mu(t)$ is large, the positive magnitude of the first matrix may be large enough to counteract negative correlations in the second matrix. 

If the original covariance is dependent on the random times, the mean vector and error structure both depend on the time. We explore the impact of transforming the data through empirical simulations. Let the observation times equal random perturbations around specified goal times such that $\B t = \B T + \B \tau$ where $\B \tau\sim N(0,\sigma^{2}_{\tau}\B I)$ and $\B T = (1,2,...,9,10)$. We generate $n=500$ realizations of the model,
$$\B y_{i} = \BS\mu_{i} + \BS\epsilon_{i}\quad\text{ where }\BS\epsilon_{i}\sim N(0,\B R_{i}(\rho))$$
where the vector elements $\mu_{ij}=\mu(t_{ij})$. We repeat the simulation under different assumptions for the mean function and standard deviations of the observation times. Figure \ref{fig:cov} show the estimated autocorrelation functions of the deviations of the transformed data from the mean when  $\B R_{i}(\rho)$ is an exponential correlation matrix with range parameter $\rho=2$ under varying conditions for observations times and shape functions. As the variance of observation times and the magnitude of the derivative mean function increases, the estimated correlation between deviations from the mean becomes more positive. Thus, variability in the observations times can result in covariance structures that are no longer singular. 

In practice, if the data are regularly or very close to regularly sample, negative correlations are problematic for estimation and an independence or exponential correlation structure may be the best option. If the data are irregularly sampled, one potential covariance model is an additive model that combines a random intercept with the exponential correlation \cite{diggle2002}, which may be appropriately flexible to approximate the covariance of the deviations from the mean of the transformed data.\\

\begin{landscape}
\begin{figure}
\centering
\includegraphics[height=5.5in]{Chp4Cov}
\caption{Estimated autocorrelation functions of the deviations from the mean from data generated with an exponential correlation error structure and random observation times under different mean functions, $\mu(t)$, and standard deviations of the random time perturbations, $\sigma_{\tau}$.}
\label{fig:cov}
\end{figure}
\end{landscape}

\noindent \textbf{Unbalanced observation times}

In addition to the issues of fixed versus random sampling, having an unequal number of observations per subject can impact the estimation of the covariance of the transformed vector. As we saw above, the length of the vector impacts the covariance of the transformed vector. Suppose the outcome vectors for a sample of individuals have the same mean shape and covariance over time, but each individual is observed a different number of times because they were unavailable for an interview or two. Transforming the vectors by subtracting means based on a variety of number of observations induces a different covariance structure for each individual based on the length of outcome vector. If there is quite a bit of variability in the number of observations, it may impact clustering to assume they share the same covariance structure during the estimation/clustering process. However, if the number of observation times is large for all subjects and the observation period is long, then the covariance matrices should be similar. 

Additionally, if the unbalanced nature of the data is due to lost to follow-up during a longitudinal study, clustering based on the shape should be done with caution. If the general shape of the curve during the observation period is not measured adequately by the number of observations, it does not make sense to try and cluster those individuals with the rest who have more fully observed curves. 



\section{Discussion}
In this chapter, I described three different approaches to the problem of clustering irregularly sampled longitudinal data by shape. The first method focuses on clustering derivative functions. Projecting the data onto a B-spline basis removes noise and provides an estimate of the smooth underlying function. The derivative can then be calculated from the estimated function. This should be an improvement over difference quotients, which simply linearly interpolates the data points with no regard to error. One difficulty with this method is choosing the correct order and knots for the basis so as to not over fit the data when there are only a few data points. While this may improve upon other methods, this approach has limitations. There is no direct way to borrow strength between individuals when estimating their derivative function even if shape is hypothesized to be a common factor. Also, partitioning methods do not lend themselves to analysis of baseline factors since by definition, the algorithm does not provide any uncertainty estimates in group membership.  

Rather than ignoring the level, the second approach attempts to directly model the variability in the level by assuming that for each shape group, the distribution for the level can be approximated with a Gaussian mixture model. Assuming a multilayered mixture model provides a probability framework to take into account uncertainty while estimating the relationship between baseline factors and group membership. However, this model requires a large number of individuals in each shape group for the mixture to model the distribution well. It is not clear how robust this method is for small sample and group sizes.

Lastly, the third approach directly removes the level by subtracting individual-specific means prior to modeling. This allows individuals to be compared without making specific assumptions about the distribution of the level while providing the probability framework.  There are two difficulties with this method. First, subtracting an observed mean impacts the covariance in a way that makes it harder to model with a known correlation structure. Second, care needs to be taken when there is sparse and irregularly sampling. 

I compare these methods in practice with a simulation study in the next chapter.
