\chapter{Proposed shape-based methods}
\label{chap:methods}
In this chapter, I present three new clustering techniques that attempt to answer the three research questions presented in the last chapter: Are there distinct shape patterns in the longitudinal data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? For each method, I discuss related work and then introduce necessary background, notation, and the model specification. I describe the implementation process and any foreseen issues and limitations. After introducing the three methods, I discuss the advantages and disadvantages of each. In the next chapter, a simulation study compares the proposed methods with those presented earlier in the thesis in addressing the three research questions above.

\section{Derivative spline coefficients partitioning}
The first method is based on the idea that calculating the derivative removes the level from a function and provides information about the shape. Unlike the quotient difference method described in Chapter 3,  the proposed method invokes smoothing out noise prior to calculating the derivative and does not require the data to be regularly sampled for all subjects. Using techniques from functional data analysis \cite{ramsay2002}, individual's outcome data are projected onto a functional basis to estimate a smooth curve over time and then differentiate the basis functions to get an indirect estimate of the derivative function. The estimated derivative functions then are used to calculate the dissimilarity between individuals. By first removing the level, this  method theoretically clusters individuals with similarly shaped trajectories.

\subsection{Related work}
In Chapter 3, I introduced the quotient difference dissimilarity measure separately suggested by \textcite{d2000,moller2003}. M{\"o}ller-Levet et al. refer to this measure as the short time-series distance and developed it to identify similar shapes in microarray data. In contrast, D'Urso takes a physics view of the data and refers to the quotient difference measure as the longitudinal-velocity dissimilarity measure and also uses a quotient difference in velocities to calculate a measure inspired by acceleration. In the end, he combines the cross-sectional and evolutive information of the trajectories into one dissimilarity measure.

Similar to D'Urso, Zerbe presents three distance measures for growth curves based on position, velocity, and acceleration \cite{zerbe1979,schneiderman1993}. Rather than using quotient differencing, he suggests estimating the velocity by first fitting a polynomial of degree $d$ to each individual's growth curve using least squares. For individual $i$, let $\B y_{i}$ be the vector of observed outcomes at times $\B t_{i}$. Then, the estimated curve is $\hat{f}_{i}(t) = (1\;\; t\;\;t^{2}\;\;...\;\;t^{d})\;\hat{\BS\beta}_{i}$ where 
$$\hat{\BS\beta}_{i} = (\B X^{T}_{i}\B X_{i})^{-1}\B X^{T}_{i}\B y_{i}$$
and $\B X_{i}$ is the within-individual design matrix determined by the polynomial function evaluated at the observation times of individual $i$. The estimated derivative is equal to
$\hat{f}_{i}^{'}(t) = [0\;\; 1\;\;2t\;\;...\;\;dt^{d-1}]\;\hat{\BS\beta}_{i}$. In other words, he suggests projecting the data onto a polynomial basis of degree $d$, differentiating the basis, and calculating the estimated derivative function, which can be represented by another polynomial basis of degree $d-1$. The dissimilarity between the $i$th and $j$th individuals based on the velocity equals
$$d_{ij} =\left[ \int_{\mathcal{T}} [\hat{f}^{'}_{i}(t)-\hat{f}^{'}_{j}(t)]^{2}dt\right]^{1/2}$$
for a chosen time interval, $\mathcal{T}$.This integral is easy to calculate since the estimated derivatives functions are represented with a polynomial basis.

\textcite{tarpey2003}, at the end of their functional clustering paper, make a few suggestions to cluster data after getting 'rid of dominating variability in the intercept.' One proposal is to cluster individuals based on the derivatives of the estimated functions. In personal correspondence with one of the authors, the details of the implementation were clarified. After projecting individuals' data onto a Fourier basis, they differentiated the Fourier basis functions and used the K-means algorithm on the coefficients of the derivative functions. Thus, the estimated function for individual $i$ is $\hat{f}_{i}(t)= (1\;\; \sin(t)\;\;\cos(t)\;\;...\;\;\sin(wt)\;\;\cos(wt))\;\hat{\BS\beta}_{i}$ where
$$\hat{\BS\beta}_{i} = (\B X^{T}_{i}\B X_{i})^{-1}\B X^{T}_{i}\B y_{i}$$
and $\B X_{i}$ is the within-individual design matrix determined by the Fourier expansion evaluated at the observation times of individual $i$. Since $\frac{d}{dt}\cos(wt) = -w\sin(wt)$ and $\frac{d}{dt}\sin(wt) = w\cos(wt)$, the estimated derivative function for individual $i$ is represented using the same Fourier expansion with new coefficients, 
$$\hat{\BS\alpha}_{i} = \left(\begin{array}{cccccc}
0 & 0&0&\cdots&0&0\\
0&\cos(t)&0&\cdots&0&0\\
0&0&-\sin(t)&\cdots&0&0\\
\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&0&\cdots&w\cos(wt)&0\\
0&0&0&\cdots&0&-w\sin(wt)\end{array}\right)\hat{\BS\beta}_{i}.$$
The dissimilarity between the $i$th and $j$th individuals based on the derivative coefficients equals
$$d_{ij} = (\hat{\BS\alpha}_{i}-\hat{\BS\alpha}_{j})^{T}(\hat{\BS\alpha}_{i}-\hat{\BS\alpha}_{j}).$$

These suggested methods use the same general procedure. Individual trajectories are smoothed by projecting the data onto a chosen basis, the derivative of the estimated function is calculated by differentiating the basis functions, and the dissimilarity between individuals is based on the estimated derivative functions. Now with any smoothing procedure, there is a fundamental bias-variance tradeoff. In this case, including higher ordered polynomial terms or more sine and cosine functions in the basis decreases the bias but increases the variance. Characteristics of the basis need to be selected to strike a balance between the two so as to not over fit the data. The type of basis also affects the differentiation process. For example, polynomial and Fourier bases are computationally convenient in that differentiation results in a basis of the same type.

They differ in how the dissimilarity measure is defined between two individuals. Zerbe used the $L_{2}$ distance between two derivative functions; Tarpey and Kinateder calculated the squared Euclidean distance between the basis coefficient vectors for the estimated derivative function. This reflects the diversity in the functional cluster analysis literature; some use the $L_{2}$ distance on functions \cite{hitchcock2007} while others calculate the Euclidean distance between the linear coefficients of the basis function \cite{serban2005, tarpey2003, abraham2003}.
 
\textcite{tarpey2007} reconciled these two approaches by showing that clustering functional data using the $L_{2}$ metric on function space can be achieved by clustering a suitable linear transformation of the basis coefficients. If $y(t)$ is a functional realization represented as $y(t)=\sum_{j}\beta_{u}u_{j}(t)$and $\mu(t)$ is a functional cluster mean represented as $\mu(t) = \sum_{j}\gamma_{j}u_{j}(t)$, then the squared $L^{2}$ distance on interval $\mathcal{T}$ is
\begin{align*}
\int_{\mathcal{T}}(y(t)-\mu(t))^{2}dt &= \int_{\mathcal{T}}(\sum_{j}(\beta_{j}-\gamma_{j})u_{j}(t))^{2}dt\\
&= \sum_{j}\sum_{l}(\beta_{j}-\gamma_{j})(\beta_{l}-\gamma_{l}) \int_{\mathcal{T}}u_{j}(t)u_{l}(t)dt\\
&=(\BS\beta-\BS\gamma)^{T}\B W (\BS\beta-\BS\gamma)\\
&=(\B W^{1/2}(\BS\beta-\BS\gamma))^{T} (\B W^{1/2}(\BS\beta-\BS\gamma))
\end{align*}
where $\B W_{jl} = \int_{\mathcal{T}}u_{j}(t)u_{l}(t)dt$. Therefore, clustering with the $L^{2}$ distance is equivalent to plugging transformed coefficients, $\B W^{1/2}\BS\beta$, into the K-means algorithm. Consequently, when the functions are represented using an orthogonal basis such as the Fourier expansion, K-means on the coefficients is equivalent to the $L^{2}$ implementation.

Both bases present thus far are restrictive. The Fourier basis only works well when the data is periodic in nature and a polynomial basis does not provide a general structure to represent complex functions with few parameters. Another popular basis is the class of B-splines \cite{deboor1978, schumaker1981}, which extend the advantages of polynomials to include greater flexibility \cite{abraham2003}. To the author's knowledge, no other study have focused on clustering longitudinal data using the coefficients of the B-spline derivative estimate.

In the following sections, I introduce B-spline functions and demonstrate how they can be used to estimate derivative functions. Then, the implementation and practical decisions that need to be made are presented and discussed.

\subsection{B-spline functions}\label{sec:bsplines}
I fit a m-order spline function to each subject $i$ in order to estimate its underlying smooth, $f_i$. For the sake of being self-contained, I include some background on splines. Let $t\in[a,b]$ where $a,b\in\mathbb{R}$ and $\xi_0=a<\xi_{1}<\cdots<\xi_{L} < b = \xi_{L+1}$ be a subdivision of  the interval $[a,b]$ by $L$ distinct points, termed internal knots. The knot sequence is augmented by adding replicates at the beginning and end, $\tau=[\tau_{1},...,\tau_{L+2m}]$ for $m\in\mathbb{N}$, such that 
\begin{align*}
\tau_{1}&=\tau_{2}=\cdots =\tau_{m} =\xi_{0}\\
\tau_{j+m}& = \xi_{j}, \quad\quad j=1,...,L\\
\xi_{L+1}&=\tau_{L+m+1}=\tau_{L+m+2}=\cdots =\tau_{L+2m} 
\end{align*}
The spline function, $f(t)$, is a polynomial of order $m$ on every interval $[\tau_{j-1},\tau_{j}]$ and has $m-2$ continuous derivatives on the interval $(a,b)$. The set of spline functions of order $m$ for a fixed sequence of knots, $\tau = [\tau_1,...,\tau_{L+2m}]$, is a linear space of functions with $L+m$ free parameters. A useful basis $B_{1,m}(t),...,B_{L+m,m}(t)$ for this linear space is given by Schoenbergs' B-splines \cite{curry1966, de1976} defined as
\begin{align*}
B_{j,1}(t) &= \begin{cases}
1 \text{ if }\tau_j\leq t < \tau_{j+1}\\
0\text{ otherwise}
\end{cases}\\
B_{j,l}(t) &= \frac{t-\tau_j}{\tau_{j+l-1}-\tau_j} B_{j,l-1}(t)+\frac{\tau_{j+l}-t}{\tau_{j+l}-\tau_{j+1}} B_{j+1,l-1}(t)
\end{align*}
where $l=2,...,m$ and $j=1,...,L+2m-l$.  If I adopt the convention that $B_{j,1}(t)=0$ for all $t\in\mathbb{R}$ if $\tau_{j}=\tau_{j+1}$, then by induction $B_{j,l}(t)=0$ if $\tau_{j}=\tau_{j+1}=\cdots=\tau_{j+l}$. Hence, $B_{1,l}(t)=0$ for $t\in\mathbb{R}$ and $l<m$ on the defined knot sequence. The B-spline function of order $m$ is defined by
$$f(t) = \sum^{L+m}_{j=1} \beta_j B_{j,m}(t).$$

Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote a vector of repeated observations for individual $i$ observed at times $\B t_{i}=(t_{i1},...,t_{im_{i}})$  for $i=1,...,n$. I assume that $y_{ij} = f_{i}(t_{ij}) + \epsilon_{ij}$ such that $E(\epsilon_{ij}) = 0$ for all $j=1,...,m_{i}$ and $i=1,...,n$.  To estimate $f_{i}$, I fix the order of the B-spline, $m$, and internal knots and then estimate the coefficients, $\BS \beta_{i} = (\beta_{i,1},...,\beta_{i, (L+m)})$ using least squares. The estimated function is $\hat{f}_i(t)=\sum^{L+m}_{j=1} \hat{\beta}_{i,j} B_{j,m}(t)$ where
$$\hat{\BS \beta}_{i} = (\B X_{i}^{T}\B X_{i})^{-1}\B X_{i}\B y_{i}$$
and $\B X_{i}$ is the within-individual design matrix based on the B-spline basis functions. 

To estimate $f_i'(t)$, the estimated function $\hat{f}_i(t)$ is differentiated with respect to $t$ such that
$$\hat{f}'_i(t)=\sum^{L+m}_{j=1} \hat{\beta}_{i,j} B'_{j,m}(t).$$
\Textcite{prochazkova2005} showed that this can be simplified to
$$\hat{f}'_i(t)=\sum^{L+m}_{j=2} \hat{\beta}_{i,j} \left[\frac{m-1}{\tau_{j+m-1}-\tau_j} B_{j,m-1}(t)-\frac{m-1}{\tau_{j+m}-\tau_{j+1}} B_{j+1,m-1}(t)\right].$$
However, going one step further, this can be written in terms of a B-spline basis of one order lower,
\begin{align*}
\hat{f}'_i(t)&=\sum^{L+m-1}_{j=1} (\hat{\beta}_{i,j+1} -\hat{\beta}_{i,j})\frac{m-1}{\tau_{j+m}-\tau_{j+1} }B_{j+1,m-1}(t).
\end{align*}
Adjusting the knot sequence to only have $m-1$ replicates at the beginning and end results in
$$\hat{f}'_i(t)=\sum^{L+m-1}_{j=1}\hat{\alpha}_{i,j}B_{j,m-1}(t)$$
where $\hat{\alpha}_{i,j}= (\hat{\beta}_{i,j+1} -\hat{\beta}_{i,j})\frac{m-1}{\tau_{j+m}-\tau_{j+1} }$.
These derivative coefficients, $\hat{\BS \alpha}_{i} = [\hat{\alpha}_{i,1},...,\hat{\alpha}_{i,(L+m-1)}]$, can be used to cluster trajectories with similar shape with the K-means algorithm \cite{macqueen1967, hartigan1979}. 

\subsection{Implementation}
To use this method in practice, decisions about the B-spline basis need to be made. The order of the polynomials must be chosen. This together with the number of internal knots impacts the flexibility of the B-spline function. Cubic B-splines of order four have been shown to have good mathematical properties and are used frequently in practice \cite{james2003}. However, depending on the number of data points observed per subject, it may be necessary to use a quadratic polynomial ($m = 3$) due to the restriction that the sum of the order and the number of internal knots must be less than or equal to the minimum number of observation points per subject. 

The number of internal knots, $L$, plays a role in the flexibility of the class of spline functions and should be enough to fit the features in the data. As with the order, the main limiting factor in choosing $L$ is the number of data points per subject. The longitudinal data sets considered in this thesis have about five to ten data points per subjects. It is important not to over fit the individual curves so for data with limited observation, it may only be possible to have a at most one internal knot. If there are many repeated measures, model selection information criteria or cross-validation can be used to choose $L$ \cite{rice2001}. The location of the internal knots is another issue of discussion. There are some suggested data-driven ways to select knot location \cite{shanggang2001}, but \textcite{ruppert2002} supports fixing the knots at sample quantiles. I follow this suggestion and adjust them as necessary to the areas with the most functional action. 

Once the order of the polynomials and number and location of knots are chosen, then smooth functions and their derivatives are estimated using least squares separately for every subject. The coefficients from the estimated derivative functions become the input vectors to the clustering procedure. The K-means algorithm is a standard clustering algorithm that given a fixed number of clusters, minimizes the within sum of squares after initially randomizing vectors to groups. This algorithm converges but there is no guarantee that it will find the grouping that globally minimizes the objective function. Therefore, in practice, the algorithm is run multiple times with different random initializations and the clustering that minimizes the objective function is chosen. I use 25 random starts.

The number of clusters $K$ must be fixed in order to run the K-means algorithm. However, $K$ is unknown and of interest to researchers in practice. While no perfect mathematical criterion exists, a number of heuristics (see \cite{tibshirani2001} and discussion therein) are available for choosing $K$. For this thesis, $K$ is chosen for partition methods so as to maximizes the overall average silhouette width \cite{rousseeuw1987}. See Chapter 1 for technical details of the silhouette width. By maximizing the overall average silhouette width, the dissimilarity between clusters is maximized and the dissimilarity within clusters is minimized resulting in distinct groups. In this case, the dissimilarity between two individuals is defined as the squared Euclidean distance between derivative coefficients. I choose $K$ that best separates the data so we have groups of individuals with similar shape patterns over time.

K-means is a partitioning algorithm, therefore, by definition every subject is `hard' clustered into one of the groups. There is no stated uncertainty in the group memberships even if a subject is between two clusters. In order to estimate the relationship between baseline variables and shape group membership, I assume the cluster labels are known. Given the subject grouping labels from the partition, $\{c_{i}\}$ such that $c_{i}\in\{1,2,...,K\}$ for all $i=1,...n$, we fit a multinomial logistic regression model, which is an extension of the logistic regression model, using the group labels as the outcome and baseline factors as explanatory variables such that
$$P(c_{i} = k|\B w_{i}) = \frac{\exp(\B w_{i}^{T}\BS\gamma_{k})}{\sum_{j=1}^{K}\exp(\B w_{i}^{T}\BS\gamma_{j})}$$
for all $k=1,...,K$ and $i=1,...,n$ with $\BS\gamma_{K}=0$ where $\B w_{i}$ is the design vector based on baseline factors. The parameters $\BS\gamma_{1},...,\BS\gamma_{K-1}$ are estimated via maximum likelihood estimation. However, it is important to note that the estimated standard errors from a Hessian calculation do not include any group membership uncertainty  as the hard clustering labels are assumed known for the estimation process.


\section{Multilayer mixture model}\label{sec:multi}
The second method attempts to make up for the lack of uncertainty in groups memberships by using a model-based approach. The circumstance in which traditional clustering methods fail is when the level and shape are weakly dependent resulting in individuals with the same shaped trajectory at different levels. As mentioned previously, if the levels within a shape group are normally distributed, using an exchangeable correlation matrix in a finite mixture model takes that variability into account. However, if the levels do not satisfy those assumptions, a Gaussian mixture could be used to model the non-Gaussian distribution of levels within a shape group. 

\subsection{Related work}
Model-based methods provide the probability framework that dissimilarity-based methods lack. One benefit is the ability to simultaneously take into account uncertainty about of the parameter estimates and cluster membership. As mentioned in Chapter 1, the main model-based method is the finite mixture model. However, a standard mixture of Gaussians fit to the data does not distinguish between the level and shape and clusters subjects based on the dominant source of variability. Another limitation of Gaussian mixtures for grouping data is that it cannot handle clusters that are non-normal. For this reason, the mixture model has been extended to include multiple layers such that each cluster is allowed to be a mixture \cite{li2005}.  For example, imagine a data set with two groups with bimodal densities. An obvious model would be a multilayer mixture of two clusters each with two components (see Figure \ref{fig:dia} for diagram of the model structure). A variation of this idea has been used to cluster non-normal groups by fitting a mixture with many components and then systematically combining components to make more meaningful clusters \cite{hennig2010}. 
\begin{figure}[h]
\centering
\includegraphics[width=4in]{Chp4multilayer_diagram}
\label{fig:dia}
\caption{Diagram of multilayer mixture model showing that each cluster is comprised of potentially more than one component.}
\end{figure}

In this thesis, the goal is to have clusters be meaningful in terms of distinguishing between shape. Therefore, the idea of multilayer mixtures can be used to model $K$ non-normal shape clusters comprised of mixture components with the same mean shape at different levels.


\subsection{Model specification}
In a multilayer mixture model, $K$ is the number of clusters and $J_{k}$ is the number of components in the $k$th cluster ($k=1,...,K$) such that $J=\sum_{k=1}^{K}J_{k}$ is the total number of components in the entire model. Let $j$ uniquely index all of the components such that $j=1,...,J$. For ease of explanation, let $c(j):\{1,...,J\}\rightarrow \{1,2,...,K\}$ be a cluster assigning function that specifies the cluster to which a component belongs. Let $f(\B y|\BS\mu,\BS\Sigma)$ be the probability density function of a multivariate normal distribution with mean $\BS\mu$ and covariance matrix $\BS\Sigma$. Then, the probability density function for cluster $k$ is
$$f_{k}(\B y) = \sum_{j: c(j) = k} \pi_{j|c(j)}f(\B y|\mu_{j},\Sigma_{j})$$
where $\pi_{j|c(j)}$ is the probability of being in component $j$ given a subject is in cluster $c(j)$ and $\sum_{j: c(j) = k}\pi_{j|c(j)}=1$ for all $k=1,2,...,K$. Let the probability of cluster $k$ given baseline factors, $\B w$, be $\pi_{k}(\B w,\BS\gamma)=\exp(\B w^{T}\BS\gamma_{k})/\sum^{K}_{l=1}\exp(\B z^{T}\BS\gamma_{l})$ such that $\BS\gamma_{K}=0$ and $\BS\gamma=(\BS\gamma_{1},...,\BS\gamma_{K})$. The probability density function for the multilayer mixture is written as
$$g(\B y|\B w) = \sum_{k=1}^{K}\pi_{k}(\B w,\BS \gamma)f_{k}(\B y) = \sum_{k=1}^{K}\pi_{k}(\B w,\BS\gamma)\sum_{j: c(j) = k} \pi_{j|c(j)}f( \B y| \BS\mu_{j},\BS\Sigma_{j}).$$
Since every component belongs to one and only one cluster, the above equation reduces to a regular mixture model if $\bar{\pi}_{j}(\B w,\BS \gamma)=\pi_{c(j)}(\B w,\BS\gamma)\pi_{j|c(j)}$ with the density written as
$$g(\B y|\B w) = \sum_{j=1}^{J}\bar{\pi}_{j}(\B w,\gamma)f(\B y|\BS \mu_{j},\BS\Sigma_{j}).$$

To adjust this model to satisfy the goal of clustering on shape, individuals in the clusters are assumed to have the same shaped trajectory over time and the varying levels are modeled by a mixture of components. The component mean structure includes a regression parameterization to model the smooth underlying group mean shape function over time. The design matrix for the regression can easily include B-spline basis functions presented in Section \ref{sec:bsplines}. Then, the regression parameters are constrained to be the same within shape clusters and the intercept are allowed differ in each level component. Hence, $\mu_{j}=\lambda_{j}+\B x \BS\beta_{c(j)}$, where $\B x$ is a matrix of B-spline basis functions excluding the first basis function to allow for estimation of intercept terms for each component. This allows the clusters to be based on shape while the components can have different levels (see figure \ref{fig:diashape} for diagram for model structure). Additionally, I assume conditional independence within components, $\Sigma_{j}=\sigma^{2}_{j}I$ for all $j=1,...,J$,  for simplicity and to allow for irregularly sampled longitudinal data.
\begin{figure}[h]
\centering
\includegraphics[width=4in]{Chp4multilayer_diagram_shape}
\label{fig:diashape}
\caption{Diagram of multilayer mixture model showing that each shape cluster is comprised of potentially more than one level component.}
\end{figure}

\subsection{Implementation}
Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote a vector of repeated observations for individual $i$ observed at times $\B t_{i}=(t_{i1},...,t_{im_{i}})$  for $i=1,...,n$. For each individual $i$, B-splines basis functions are evaluated at the observation times to create design matrices $\B x_{i}$ for the mean regression and baseline covariates are combined in design matrices $\B w_{i}$ for the group membership regression. Now, B-splines are used so the mean structure is flexible enough to accommodate complex shapes. The issues of the number and location of internal knots are dealt with in the same manner as in the first proposed method.

Denote the shape cluster identity of individual $i$ by $\eta_{i}$ where $\eta_{i}\in\{1,...,K\}$. Then, the parameters $\BS\eta = \{\eta_{i}; i=1,...,n\}$ and $\BS\theta=\{\BS \gamma_{k}, \pi_{j|c(j)},\lambda_{j}, \BS\beta_{k},\sigma^{2}_{j};\; j=1,...,J, k=1,...,K\}$ of the multilayer mixture model are estimated by maximizing the classification log-likelihood function,
\begin{align}
 L(\theta,\eta) &= \sum^{n}_{i=1} \log \pi_{\eta_{i}}(\B w_{i},\BS\gamma) f_{\eta_{i}}(\B y_{i}|\B x_{i})\\
\label{ll}& =  \sum^{n}_{i=1} \log\left[ \pi_{\eta_{i}}(\B w_{i},\BS\gamma)  \sum_{j: c(j) = \eta_{i}} \pi_{j|c(j)}f(\B y_{i}|\B x_{i},\lambda_{j},\BS\beta_{\eta_{i}},\sigma^{2}_{j})\right]
\end{align} 
using a modified EM algorithm called the classification expectation maximization algorithm (CEM) \cite{celeux1992, mclachlan2000}. The modification to the EM algorithm involves adding a classification step between the expectation step and maximization step where individuals are assigned to shape clusters. 

In order to start the iterative algorithm, the individuals are initially assigned into shape clusters and component groups. Initialization can involve randomly partitioning individuals into components or strategically partitioning individuals into shape clusters using a computationally fast procedure such as the first proposed method of this thesis and then randomly partition the individuals into components. 

Let $\BS\theta^{(t)}$ and $\BS\eta^{(t)}$ be the current estimates of the parameters at the $t$th iteration. The modified EM algorithm updates these estimates as follows:
\begin{enumerate}
\item Expectation step: For each individual $i$, compute the posterior probability of being in shape cluster $k$, $p_{i,k}$, such that
$$p_{i,k}= \pi_{k}(\B w_{i},\BS\gamma)f_{k}(\B y_{i}|\B x_{i})/\sum^{K}_{j=1} \pi_{j}(\B w_{i},\BS\gamma)f_{j}(\B y_{i}|\B x_{i})$$
for $i=1,...,n$ and $k=1,...,K$.
\item Classification step: Hard classify subjects to shape clusters according to $\eta^{(t+1)}_{i} = \arg\max_{k} p_{i,k}$.
\item Maximization step: For each shape cluster, use maximum likelihood estimation to update parameter vector $\theta$ by embedding an EM procedure initialized with the current parameter values.
\end{enumerate} 

This algorithm increases the classification log-likelihood at each iteration. The statement is below and the proof is in the Appendix \ref{append:2}. 
\begin{theorem}\label{Thm:CEM} The classification likelihood $L(\BS\theta,\BS\eta)$ defined in (\ref{ll}) is non-decreasing after each update of $\BS\eta$ and $\BS\theta$ by the CEM algorithm. That is, $L(\BS\theta^{(t+1)},\BS\eta^{(t+1)}) \geq L(\BS\theta^{(t)},\BS\eta^{(t)})$ for all $t$.
\end{theorem}

In the maximization step of the algorithm, parameters are estimated by maximizing the likelihood for each shape cluster. However, the shape parameters are constrained to be equal for all components within the cluster. To estimate both the component and cluster-specific parameters simultaneously, this thesis uses computational methods suggested by \textcite{grun2008}. For shape cluster $k$, the outcome vector for subject $i$ is temporarily replaced by a vector of $\B y_{i}$ repeated $J_{k}$ times. The new design matrix  is set equal to $(I_{J_{k}}\otimes \BS 1_{m_{i}}, \BS 1_{J_{k}} \otimes \B x_{i})$ where $\otimes$ refers to the Kronecker product. Lastly, the covariance matrix structure is block diagonal with $\sigma^{2}_{j}I_{m_{i}}$ in each block for $j$ that satisfy $c(j)=k$. Assuming a multivariate Gaussian distribution, the likelihood function is maximized with respect to the parameters for cluster $k$ and its components using profile likelihoods and a standard numerical optimization routine.

With this more complex mixture structure, it is necessary to select the number of shape clusters, $K$, as well as the number of components for each cluster $k$, $J_{k}$, $k=1,..,K$. It is recommended to fix $K$ and then use model selection criteria to select $J_{k}$ for each $k=1,...,K$. As suggested by \textcite{li2005}, the BIC is used to select $J_{k}$ even though the regularity conditions do not hold as the criteria has been shown to be useful a informal guide in practice.

Robust standard errors for the parameter estimates can be found following the same procedure as \textcite{boldea2009}.

\section{Vertical shifted mixture model}
The goal of cluster analysis is to partition a collection of individuals into homogeneous groups. In this thesis, similar individuals are those with the same outcome shape pattern over time. If two curves only differ in intercepts by vertical shifts, they are placed in the same group. The first proposed methods uses derivatives to implicitly remove the level and the second method directly models the variability in the level with an additional layer of mixture models.  In this method, I consider subtracting the subject-specific mean from the outcome measurements to remove the level. A finite mixture of densities with a mean shape curve and a covariance function is fit to the vertically shifted data. 

\subsection{Recent work}
A finite mixture model is a standard method for clustering multivariate data \cite{everitt2009} and has been used for longitudinal applications \cite{muthen2010, jones2001}. See Chapter 1 for an extensive summary of finite mixture models. However, for longitudinal data, the models are commonly used for the observed data without much regard to the goal of clustering by shape. 

I suggest removing the level by subtracting out the mean outcome level prior to modeling. Subtracting the mean is not a novel idea in statistics or even cluster analysis. In fact, experimental data such as gene expression microarrays are often normalized prior to analysis to remove variability between samples from the measurement device. In cluster analysis of multivariate data, it is recommended that each variable is standardized by subtracting the mean of the variable measures and dividing by the standard deviation so that each standardized variable is in comparable units and equally contributes to the grouping process. This, however, is not recommended for use in the longitudinal setting where each variable is a repeated measurement at a different time point. To compare the shape of the change patterns over time, the original scale of the outcome measurements needs to be maintained. Therefore, any transformation performed on the data should be additive in nature to preserve the original shape of the data over time. Pre-processing the data can assist in answering the research question but any transformation of the data should be carefully studied for potential unintended consequences.

A version of this idea has been implemented in the functional data analysis literature. For processes in a Hilbert space of square integrable functions with respect to the Lebesgue measure, $dt$, on the interval $\mathcal{T}$, \textcite{chiou2008} propose using a mixture model and the Karhunen-Lo{\`e}ve expansion for centered stochastic processes within their correlation-based clustering algorithm. The integral of the random function over interval $\mathcal{T}$ divided by the length of the interval is subtracted to center the process; the resulting process integrates to zero. This integral is the functional analogue to a mean vector in vector space; similarly, the resulting vector has mean zero after subtraction.

 Although centering a process and a shifting a vector stems from the same idea, there are distinct consequences of subtracting the estimated level of a noisy curve observed at a finite number of points that do not arise when centering a smooth function. The term centering is used in the stochastic processes literature, but I use the term vertically shifting to refer to the procedure of subtracting the mean since it graphically describes the transformation of the noisy longitudinal data.

\subsection{Model Specification}
 Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote an outcome vector of repeated observations for individual $i$, $i=1,...,n$. The vector of corresponding times of observation for individual $i$ is denoted as $\B t_{i}=(t_{i1},...,t_{im_{i}})$ and $\B w_{i}$ is a $q$-length design vector based on time-fixed factors that are typically collected at or before time $t_{i1}$. We assume that there are $K$ mean shape functions, $\mu_{k}(t)$, in the population such that the outcome vector for individual $i$ in shape group $k$ is
 $$\B y_{i} = \lambda_{i}\B 1_{m_{i}}+\BS\mu_{i}+\BS\epsilon_{i},\quad \lambda_{i}\sim F, \quad \BS\epsilon_{i}\sim N(0,\BS\Sigma_{k})$$
 where $F$ is a probability distribution, $\B 1_{m_{i}}$ is a m-length vector of 1's, and $\mu_{ij} = \mu_{k}(t_{ij})$ is the $j$th element of a $m_{i}$-length vector of mean values evaluated at the observation times, $\B t_{i}$. The outcome vector is determined by a mean shape function, a random intercept, and potentially correlated random errors. The probability of being in a particular shape group could depend on baseline covariates in the vector $\B w_{i}$. Let $\bar{y}_{i}= m_{i}^{-1}\sum^{m_{i}}_{j=1} y_{ij} = \lambda_{i}+\bar{\mu}_{i}+\bar{\epsilon}_{i}$ be the mean of the outcome measurements for individual $i$. This measure of the vertical level of the data vector can be removed by applying a linear transformation, $\B A_{i} = \B I_{m_{i}} - m_{i}^{-1}\B 1_{m_{i}}\B 1_{m_{i}}^{T}$, to the vector of observations. The vertically shifted vector for individual $i$ equals 
\begin{align*}
\B y^{*}_{i} &= \B A_{i}\B y_{i}\\
&=\B A_{i}(\lambda_{i}\B 1_{m_{i}}+\BS\mu_{ik}+\BS\epsilon_{i})\\
&=\B A_{i}(\BS\mu_{i}+\BS\epsilon_{i})\\
&=\BS\mu_{i} - \bar{\mu}_{i}+\epsilon_{i}-\bar{\epsilon}_{i}.
\end{align*}
Applying the symmetric matrix $\B A_{i}$ to the vector $\B y_{i}$ subtracts the individual mean, $\bar{y}_{i}$, from each element $\B y_{i}$. This results in the removal of the random intercept, $\lambda_{i}$, leaving the mean function evaluated at the observation times plus random error shifted by a random constant, $\bar{\mu}_{i}+\bar{\epsilon}_{i}$. Clearly, we do not have to worry about $F$, the distribution of the random intercept, or any other time-fixed factors that only impact the level of the outcome. 

Once the level is removed, we assume the vertically shifted data, $\B y_{i}^{*}$, follow a Gaussian mixture of $K$ groups with mean shape functions and random errors. If the observation times are fixed, vertically shifted data generated from the model above would exactly follow this Gaussian mixture. Thus, conditional on observation times $\B t$ and baseline covariates $\B w$, $\B y^{*}$ is assumed to be a realization from a finite mixture model with density
\begin{align*}
 f(\B y^{*}|\B t,\B w,\BS\theta) =  \sum^{K}_{k=1}\pi_{k}(\B w,\BS \gamma)f_{k}( \B y^{*}|\B t,\BS\theta_{k})
\end{align*}
where $\pi_{k}(\B w,\BS \gamma)$ is the prior probability of being in the $k$th component given baseline covariates, $\B w$. The full vector of parameters for the model is $\BS\theta = (\BS\gamma,\BS\theta_{1},...,\BS\theta_{K})$. To allow baseline covariates to impact the probability of having a certain shape pattern over time, the prior probabilities are parameterized using the generalized logit function of the form
$$\pi_{k}(\B w,\BS\gamma)=\frac{\exp(\B w^{T}\BS\gamma_{k})}{\sum_{j=1}^{K}\exp(\B w^{T}\BS\gamma_{j})}$$ 
for $k=1,...,K$ where $\BS \gamma_{k}\in\mathbb{R}^{q}$, $\BS\gamma = (\BS\gamma_{1},...,\BS\gamma_{K})$, and $\BS\gamma_{K}=\B 0$. For continuous outcome vectors, the component densities $f_{k}(\B y^{*}|\B t,\BS\theta_{k})$ are multivariate Gaussian densities with mean and covariance dependent  on time.

\subsubsection{Mean Structure}
To focus on shape, the mean is modeled as a smooth function of time represented by a chosen functional basis. If the shape is periodic in nature, a Fourier basis is appropriate. Another common basis is a lower order polynomial basis such as $\{1, t, t^{2}\}$. However, this basis cannot capture complex shapes with drastic, local changes. 

To allow for flexibility in the functional shape, the observation time interval, $[a,b]$, is broken up into smaller interval using $L$ knots, $a<\tau_{1}<\cdots<\tau_{L}<b$, and polynomials of order $p$ are fit in each subinterval. This piecewise polynomial can be expressed as a linear combination of truncated power functions and polynomials of order $p$. In other words,
$\{1,t,t^{2},...,t^{p-1},(t-\tau_{1})_{+}^{p-1},...,(t-\tau_{L})_{+}^{p-1}\}$
is a basis for a piecewise polynomial with knots at $\tau_{1},...,\tau_{L}$. However, the normal equations associated with the truncated power basis are highly ill-conditioned. 

A better conditioned basis for the same function space is the B-spline basis \cite{deboor1978, schumaker1981,curry1966, de1976}. A B-spline function of order $p$ with $L$ internal knots, $\tau_{1},...,\tau_{L}$, is defined by a linear combination of coefficients and B-spline basis functions
$$\mu(t) = \sum^{L+p}_{j=1} \beta_j B_{j,p}(t)$$
where the basis functions, $B_{j,p}(t)$, are defined iteratively \cite{deboor1972,cox1972}  (see Section \ref{sec:bsplines}). Values from the $p$th order B-spline basis functions taken at observation times $\B t_{i}$ can be used in a design matrix, $\B x_{i}$, to linearly model the mean vector. Thus, the mean of the $k$th shape cluster is approximated by the linear function $\BS\mu_{k}(t) = \sum^{L+p}_{j=1} \beta_j B_{j,p}(t)$. In the multivariate form, the mean vector at observation times $\B t_{i}$ equals $\B x_{i}\BS\beta_{k}$ where $\BS\beta_{k}=(\beta_{k,1},...,\beta_{k, L+p}).$  

\subsubsection{Covariance Structure}
There are many potential assumptions to be made about the covariance of the random deviations from the mean. Here, we allow the covariances to differ between clusters. Since it is common for longitudinal data to have sparse, irregular time sampling, the covariance matrix needs structure to allow for parameter estimation as described by \textcite{jennrich1986} in their seminal paper. A common parameterization is conditional independence with constant variance where $\B \Sigma_{k}= \sigma_{k}^{2}\B I_{m_{i}}$. This is typically an unrealistic assumption for longitudinal data since there is inherent dependence between repeated measures on the same unit. 

Compound symmetry, which is also known as exchangeable correlation, is a popular correlation structure in longitudinal analysis where all repeated measures are equally correlated. This is typically paired with constant variance such that $\B \Sigma_{k} = \sigma_{k}^{2}(\rho_{k}\B1_{m_{i}}\B1_{m_{i}}^{T}+(1-\rho_{k})\B I_{m_{i}})$ where $-1\leq\rho_{k}\leq 1$ is the correlation between any two distinct measurements within an individual. This dependence structure describes the resulting correlation matrix of a random intercept model.

Another structure that provides a compromise between the two is the exponential correlation structure in which the dependence decays as the time between observations increases such that $[\B \Sigma_{k}]_{jl} = \sigma_{k}^{2}\exp(-| t_{ij}-t_{il}| / r_{k})$ where $r_{k}> 0$ is the range of the dependence. If the range, $r_{k}$, is small, the correlation decays quickly, but if the $r_{k}$ is large, there is long range dependence between measurements within an individual. This structure is similar to the correlation matrix generated from a continuous autoregressive model of order one such that $[\B\Sigma_{k}]_{jl} = \sigma^{2}\rho_{k}^{|t_{ij}-t_{ill}|}$ where $\rho_{k}$ is the correlation for measurements observed one unit of time apart. If $\rho_{k} = \exp(-1/r_{k})$, then the two parameterization result in the same structure if the correlation between two measures is constrained to be positive. This is a reasonable assumption for longitudinal data in the original form but it many not be acceptable for the transformed data as discussed later.

All of the covariance structures mentioned above are associated with weakly stationary processes with constant variance and correlation dependent only on the time lag between observations. If the variance or correlation function is non-constant but varying continuously, it could be potentially modeled as a function, but estimation is more difficult.

It is important to model the covariance structure correctly as misspecification can highly impact mixture model results in terms of parameter estimates and the final clustering if the groups are not well separated (see Chapter \ref{chap:misspecify}). Transforming the data brings individuals with similar shapes closer but also brings others closer as well. In general, this decreases the separation between groups, which may force us to accurately model the correlation. 

\subsection{Implementation}
Given a collection of independent observed outcome vectors from $n$ individuals, $\B y_{1},...,\B y_{n}$. The first step is to calculate the mean for each subject, $\bar{y}_{i}$,  and subtract the subject-specific mean from the observed outcome vector for $i=1,...,n$. This transformation results in independent vertically shifted vectors, $\B y^{*}_{1},...,\B y^{*}_{n}$.  

Then, the order of the spline and the number and location of internal knots for the mean structure is chosen. The B-spline basis should be kept constant for all shape groups, so the simplest way to select the number of knots is through visual inspection of the full data set. If the most complex shape patterns is a lower order polynomial, no internal knots are necessary. However, if the most complex function has local activity, adding knots and increasing the order of the spline functions flexibly accommodates the twists and turns of the mean patterns. In choosing both the order of the polynomials and the number of knots, it is important to balance the number of mean parameters with the sample size. Every unit increase in the order or in the number of knots increases the number of parameters by $K,$ the number of groups. In terms of location of the knots, one suggestion is to place knots at sample quantiles based on the sampling times of all the observations \cite{ruppert2002}. However, this strategy may not work well if the median time is not the point of deviation from a regular polynomial. If possible, it is best to place knots at local maxima, minima, and inflection points of the overall trends as to accommodate the differences from a polynomial function \cite{eubank1999}.  Once these are decided, the design matrices, $\B x_{i}$, are calculated using widely available B-spline algorithms for $i=1,...,n$. 


Parameters are estimated using maximum likelihood estimation via the EM algorithm. Under the assumption that $\B y^{*}_{1},...,\B y^{*}_{n}$ are independent realizations from the mixture distribution, $f(\B y^{*} | \B t, \B w, \BS\theta)$, defined in Section 3, the log-likelihood function for the parameter vector, $\BS \theta^{*}$, is given by
$$\log L(\BS\theta)=\sum^{n}_{i=1}\log f(\B y^{*}_{i}|\B t_{i},\B w_{i},\BS \theta).$$
The maximum likelihood estimate of $\BS\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\BS\theta)/\partial \BS\theta=\B 0.$ Solutions of this equation corresponding to local maxima can be found iteratively through the expectation-maximization (EM) algorithm \cite{dempster1977}. See Section \ref{sec:em} for technical details about the EM algorithm.

Estimation requires the number of clusters, $K$, to be known. In practice, this is not the case and $K$ is chosen. The most popular way to choose $K$ is by setting a maximum value such that $K_{max}<n$, fitting the model under all values of $K=2,...,K_{max}$, and choosing the value that optimizes a chosen criteria. In this article, the criteria is the Bayesian Information Criterion (BIC) \cite{schwarz1978}, defined as
$$BIC = -2\log L(\hat{\BS\theta})- d\log(n)$$
where $d$ is the length of $\BS\theta$, the number of parameters in the mixture model, and $L(\BS\theta)$ is the likelihood function for the parameter vector.

There are issues of identifiability with Gaussian mixture models that can be mitigated through some minor constraints \cite{mclachlan2000}. In this section, we discuss some unique consequences of vertically shifting the data on the model and estimation.

\subsubsection{Covariance of transformed data vectors}
Let $\B Y=(Y_{1},...,Y_{m})$ be a random vector observed at times $\B t=(t_{1},...,t_{m})$ such that
$\B Y = \lambda\B 1_{m} + \BS\mu + \BS\epsilon$
such that $\lambda\sim F$, $\BS\mu$ is a vector of evaluations of a known deterministic function, $\mu(t)$, at times $\B t$, and $\BS\epsilon\sim(0,\BS\Sigma)$. Let $\B\Sigma =\B V^{1/2}\B R(\rho)\B V^{1/2}$ where $\B R(\rho)$ is an $m\times m$ correlation matrix based on the parameter $\rho$ and potentially the associated observation times, and $\B V$ is a $m\times m$ matrix with variances along the diagonal. 

Subtracting the mean of the elements of the vector by applying the transformation matrix $\B A = \B I_{m}-m^{-1}\B 1_{m}\B 1_{m}^{T}$ changes the correlation structure of the data. The covariance of the transformed random vector, $\B Y^{*}$, equals
\begin{align*}
Cov(\B Y^{*})&=Cov(\B A\B Y)\\
&=Cov( \B A(\lambda\B 1_{m}+ \BS\mu + \BS\epsilon))\\
&=Cov( \B A( \BS\mu + \BS\epsilon))\\
&= \B ACov( \BS\mu + \BS\epsilon)\B A^{T}
\end{align*}
by the properties of covariance. One important property of this transformation is that it is non-invertible; once the mean is subtracted from the data, the original data cannot be recovered. This has tremendous consequences on the covariance matrix. Since $det(\B A) = 0$, the determinant of $Cov(\B Y^{*})$ is always zero and the covariance matrix is singular. Intuitively, the matrix has to be singular because the removal of the mean constrains the data to sum to zero. In other words, the transformation projects the data onto the $(m-1)$-dimensional subspace orthogonal to the nullspace $\B 1$. However, this presents challenges when trying to model the covariance of the transformed data. 

Rather than focusing on the covariance of the transformed vector, we work with the covariance of the transformed vectors after removing the relationship with time. Therefore, the covariance of the deviations of the transformed data from the known mean shape is
\begin{align*}
Cov(\B Y^{*} - \BS\mu) &= Cov(\B A\B Y - \BS\mu)\\
&= Cov(\B A(\lambda\B 1_{m} + \BS\mu + \BS\epsilon) - \BS\mu)\\
&= Cov(\B A(\BS\mu + \BS\epsilon) - \BS\mu)\\
&= Cov((\B A-\B I_{m})\BS\mu + \B A \BS\epsilon).
\end{align*}
If the observation times are fixed, then $\BS \mu$ is not random and the covariance simplifies to $\B A Cov(\BS\epsilon)\B A^{T}$. However, if the observation times are random, then $\BS \mu$ is a random vector and contributes variability. To better understand how best to model the transformed data, we explore this covariance matrix when the observation times are fixed and random. From this point on, $\B I_{m}$ will be written as $\B I$ and $\B 1_{m}$ as $\B 1$ for simplification.

For the moment, assume that the observation times, $\B t$, are fixed. Then
\begin{align*}
Cov(\B Y^{*} - \BS\mu)&= \B ACov(\BS\epsilon)\B A^{T}\\
&= \B A\BS\Sigma \B A^{T}
\end{align*}
where $\BS\Sigma$ is the covariance of the original random errors. If the variance is constant over time, $\B V=\sigma^{2}I$, and the elements of the original vector are independent, $\B R_{i}(\rho)=\B I$, then the covariance of the deviations of the transformed data from the known mean shape is
\begin{align*} 
Cov(\B Y^{*}- \BS\mu) &= \sigma^{2}\B A\B A^{T} \\
&=\sigma^{2}\B A\\
&= \sigma^{2}(\B I - m^{-1}\B1\B1^{T})\\
&=\sigma^{2}\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)\B I)
\end{align*}
 where $a=\frac{-1}{m-1}$. Therefore, if the observation times are fixed and the data has independent errors, subtracting the estimated mean induces negative exchangeable correction between the observations of magnitude $\frac{-1}{m-1}$. Additionally, the variance decreases to $\sigma^{2}\frac{m-1}{m}$. If $m$ is large, the resulting correlation structure is approximately independence with variance $\sigma^{2}$.
 
 If the errors in the original data have constant variance, $\B V=\sigma^{2}\B I$, and are exchangeable with $\B R(\rho) = \rho\B 1 \B 1^{T} + (1-\rho)\B I$, then the covariance of the deviations of the transformed data from the known mean shape is
 \begin{align*}
 Cov(\B Y^{*}- \BS\mu) &= \sigma^{2}\B A\B R(\rho)\B A^{T}\\
 &= \sigma^{2}(\B I-m^{-1}\B1\B1^{T})(\rho\B1\B1^{T}+(1-\rho)\B I)(\B I-m^{-1}\B1\B1^{T})^{T}\\
 &= \sigma^{2}(1-\rho)(\B I-m^{-1}\B1\B1^{T})\\
 &=\sigma^{2}(1-\rho)\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)\B I)
 \end{align*} 
 where $a=\frac{-1}{m-1}$. This transformation maintains the exchangeable structure but with negative correlation on the off diagonal and decreased variance of $\sigma^{2}(1-\rho)\left(\frac{m-1}{m}\right)$.  Again, if the number of observed data points is large, then the structure is approximately independent with variance $\sigma^{2}(1-\rho)$.

 On the other hand, if the original correlation is exponential such that the correlation decreases as time lags increases, $Cor(Y_{j},Y_{l}) = \exp(-|t_{j}-t_{l}|/\rho)$, the resulting covariance after transformation is not a recognizable structure. In fact, the covariance can no longer be written as a function of time lags. The covariance matrix is a linear combination of the original correlation matrix, column and row means, and the overall mean correlation,
   \begin{align*}
 Cov(\B Y^{*}- \BS\mu) &= \sigma^{2}\B A\B R(\rho)\B A^{T}\\
 &= \sigma^{2}\left[\B R(\rho)-m^{-1}\B1\B1^{T}\B R(\rho)-m^{-1}\B R(\rho)\B1\B1^{T} + m^{-2}\B1\B1^{T}\B R(\rho)\B1\B1^{T}\right]\\
  &= \sigma^{2}\left[\B R(\rho)-\text{ column mean vector }-\text{ row mean vector } + \text{ overall mean}\right].
 \end{align*} 
 This non-stationary covariance matrix includes negative correlations when the mean of the correlations within each column and within each row are positive and substantial. For example, if $\sigma^{2}=1$, $\rho = 2$ and $\B t=(1,2,3,4)$, then the covariance matrix of the deviations of the transformed data from the known mean shape is
$$ Cov(\B Y^{*}- \BS\mu) = \left[ \begin{array}{cccc}
 0.499&  0.009& -0.229& -0.278\\
  0.009&  0.307& -0.087& -0.229\\
 -0.229& -0.087&  0.307&  0.009\\
 -0.278& -0.229&  0.009&  0.499
\end{array}\right].$$
The variance and covariance changes over time with the covariance becoming negative as the time lag increases.  If the number of observation times increase such that the observation period expands, the covariance of the transformed vector becomes close to the original covariance as column, row, and overall means decrease to zero when the number of pairs of measurements with large time lags increases. However, if the observation period remains fixed as the number of observations increases, the covariance after transformation continues to be non-stationary and has negative correlations.  

We have calculated the covariance of the transformed random vector under three common covariance structures for the original data assume fixed observation times. All of these covariance matrices are not invertible since $det(\B A) = 0$. In particular, if prior to transformation, the errors are independent or exchangeable, the correlation of the resulting transformed data is exchangeable equal to $\frac{-1}{m-1}$. This particular value has significant meaning as it is the lower bound for correlation in an exchangeable matrix. This means that the true parameter value of the correlation for the transformed vector is on the boundary of the parameter space. Therefore, even if the true structure is known, estimating parameters for the true model is difficult. Conditional independence or the exponential structure may be an adequate approximation to regularize the estimation, especially if $m$ is moderately large.

In practice, individuals in a longitudinal study are not typically observed at exactly the same times but rather at random times. When the times are random, the vector $\BS\mu$ is random because the elements are evaluations of the deterministic function, $\mu(t)$, at random times. Therefore, the transformed vector has variability due to the random times in addition to the errors. 

If the covariance of the original errors, $\BS \Sigma$, does not depend on time such as in the case of conditional independence or exchangeable, then the covariance simplifies to
\begin{align*}
Cov(\B Y^{*} - \BS\mu) &= Cov((\B A-\B I)\BS\mu + \B A \BS\epsilon)\\
&=(\B A-\B I) Cov(\BS\mu)(\B A-\B I)^{T} + \B A Cov(\BS\epsilon)\B A^{T}\\
&=m^{-2}\B 1\B 1^{T} Cov(\BS\mu)\B 1\B 1^{T} + \B A\BS\Sigma\B A^{T}.
\end{align*}
Let the random times, $t_{1},...,t_{m}$ be independent with potentially different expected values and variances. The covariance of $\BS\mu$ equals a diagonal matrix with the $j$th diagonal entry approximately equal to $Var(t_{j})[\mu'(E(t_{j}))]^{2}$ by the delta method. Then, the covariance matrix of $\B Y^{*}$ is the sum of two non-invertible matrices,
\begin{align*}
Cov(\B Y^{*} - \BS\mu) &=m^{-2}\left(\sum^{m}_{j=1}Var(t_{j})[\mu'(E(t_{j}))]^{2}\right) \B 1\B 1^{T}  + \B A\BS\Sigma\B A^{T},
\end{align*}
which need not be non-invertible. In fact, if the variance of the times and/or the derivative of the deterministic function, $\mu(t)$, is large, the positive magnitude of the first matrix may be large enough to counteract negative correlations in the second matrix. 

If the original covariance is dependent on the random times through an exponential function of time lags, the mean vector and error structure both depend on the times of observation. We explore the impact of transforming the data through empirical simulations. Let the observation times equal random perturbations around specified goal times such that $\B t = \B T + \B \tau$ where $\B \tau\sim N(0,\sigma^{2}_{\tau}\B I)$ and $\B T = (1,2,...,9,10)$. Therefore, $E(\B t) = \B T$ and $Cov(\B t ) = \sigma^{2}_{\tau}\B I$. We generate $n=500$ realizations of the model,
$$\B y_{i} = \BS\mu_{i} + \epsilon_{i}\quad\text{ where }\epsilon_{i}\sim N(0,\B R_{i}(\rho))$$
where the mean elements $\mu_{ij}=\mu(t_{ij})$. We repeat the simulation under different assumptions for the mean function, $\mu(t)$, and standard deviations of the observation times, $\sigma_{\tau}$. Figure \ref{fig:cov1} and Figure \ref{fig:cov} show the estimated autocorrelation functions of the deviations of the transformed data from the mean when $\B R_{i}(\rho)$ is an exchangeable correlation matrix with correlation parameter $\rho=0.5$ and when  $\B R_{i}(\rho)$ is an exponential correlation matrix with range parameter $\rho=2$, respectively, under varying conditions for observations times and shape functions.

As the variance of observation times and the magnitude of the derivative mean function increases, the estimated correlation between deviations from the mean becomes more positive. Thus, variability in the observations times can result in covariance structures that are no longer singular. 

In practice, if the data are regularly or very close to regularly sample, negative correlations are problematic for estimation and an independence or exponential correlation structure may be the best option. If the data are irregularly sampled, one potential covariance model is an additive model that combines a random intercept with the exponential correlation \cite{diggle2002}, which may be appropriately flexible to approximate the covariance of the deviations from the mean of the transformed data.

In addition to the issues of fixed versus random sampling, having an unequal number of observations per subject can impact the estimation of covariance of transformed vector. As we saw above, the length of the vector, $m$, impacts the covariance of the transformed vector. Suppose the outcome vectors for a sample of individuals has the same mean shape and covariance over time, but each individual is observed a different number of times because they were unavailable for an interview or two. Transforming the vectors by subtracting means based on a variety of number of observations induces a different covariance structure for each individual based on the length of outcome vector. If there is quite a bit of variability in the number of observations, it may impact clustering to assume they share the same covariance structure during the estimation/clustering process. However, if the number of observation times is large for all subjects and the observation period is long, then the covariance matrices should be similar. 

Additionally, if the unbalanced nature of the data is due to lost to follow up during a longitudinal study, clustering based on the shape should be done with caution. If the general shape of the curve during the observation period is not measured adequately by the number of observations, it does not make sense to try and cluster those individuals with the rest who have more fully observed curves. 

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[height=5.5in]{Chp4Cov2}
\end{center}
\caption{Smoothed sample autocorrelation of the deviations from the mean from data generated with an exchangeable correlation error structure and random observation times under different mean functions, $\mu(t)$, and standard deviations of the random time perturbations, $\sigma_{\tau}$.}
\label{fig:cov1}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=5.5in]{Chp4Cov}
\end{center}
\caption{Smoothed sample autocorrelation of the deviations from the mean from data generated with an exponential correlation error structure and random observation times under different mean functions, $\mu(t)$, and standard deviations of the random time perturbations, $\sigma_{\tau}$.}
\label{fig:cov}
\end{figure}
\end{landscape}



\section{Discussion}
In this chapter, I described three different approaches to the problem of clustering irregularly sampled longitudinal data by shape. The first method focuses on clustering derivative functions. Projecting the data onto a B-spline basis removes noise and provides an estimate of the smooth underlying function. The derivative can then be calculated based on the estimated function. This should be an improvement over quotient differences, which simply linearly interpolates the data points with no regard to error. One difficulty with this method is choosing the correct basis so as to not over fit the data when there are only a few data points. While this may improve upon other methods, this approach has limitations. Individuals cannot borrow strength in estimating their derivative function even though shape is hypothesized to be a common factor between individuals. Also, partitioning methods do not lend themselves to analysis of baseline factors since by definition, there is no uncertainty in group membership.  

Rather than ignoring the level, the second approach attempts to directly model the variability in the level by assuming that for each shape group, the distribution for the level can be approximated with a Gaussian mixture model. Assuming a multilayered mixture model provides a probability framework to take into account uncertainty while estimating the relationship between baseline factors and group membership. However, this model requires a large number of individuals in each shape group for the mixture to model the distribution well. It is not clear how robust this method is for smaller sample sizes.

Lastly, the third approach directly removes the level by subtracting individual-specific means prior to modeling. This allows individuals to be compared without making specific assumptions about the distribution of the level while providing the probability framework.  There are two difficulties with this method. First, subtracting an observed mean impacts the covariance in a way that makes it harder to model with a known correlation structure. Second, care needs to be taken when there is sparse and irregularly sampling. 

We compare these methods in practice with a simulation study in the next chapter.
