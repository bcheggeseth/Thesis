\chapter{Proposed shape-based methods}
In this chapter, I present three new clustering techniques---derivative spline coefficient partitioning, the multilayer mixture model, and the vertically shifted mixture model---that attempt to answer the three research questions presented in the last chapter: Are there distinct shape patterns in the longitudinal data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? For each method, I discuss related work and then introduce necessary background, notation, and the model specification. I describe the implementation process and any foreseen issues and limitations. After introducing the three methods, I discuss the advantages and disadvantages of each. In the next chapter, a simulation study compares the proposed methods with those presented earlier in the thesis in addressing the three research questions above.

\section{Derivative spline coefficients partitioning}
The first method is based on the idea that calculating the derivative removes the level from a function and provides information about the shape. Unlike the quotient difference method described in Chapter 3,  the proposed method invokes smoothing out noise prior to calculating the derivative and does not require the data to be regularly sampled for all subjects. Using techniques from functional data analysis \cite{ramsay2002}, we project each individual's outcome data onto a functional basis to estimate a smooth curve over time and then differentiate the basis functions to get an indirect estimate of the derivative function. The estimated derivative functions then are used to calculate the dissimilarity between individuals. By first removing the level, this  method theoretically clusters individuals with similarly shaped trajectories.

\subsection{Related work}
In Chapter 3, we discuss the quotient difference distance measure separately suggested by D'Urso and M{\"o}ller-Levet et. al. \cite{d2000,moller2003}. M{\"o}ller-Levet et. al. calls this measure the short time-series distance and develops it to identify similar shapes in microarray data. In contrast, D'Urso takes a physics view of the data and refers to the quotient difference distance as the longitudinal-velocity dissimilarity measure and also uses a quotient difference of the velocities to calculate a measure inspired by acceleration. In the end, he combines the cross sectional and evolutive information of the trajectories into one compromise dissimilarity.

Similar to D'Urso, Zerbe presents three distance measures for growth curves based on position, velocity, and acceleration \cite{zerbe1979,schneiderman1993}. Rather than using quotient differencing, he suggests estimating each individual's growth curve by fitting a polynomial of degree $d$ using least squares. For individual $i$, we let $\B y_{i}$ be the vector of observed outcomes at times $\B t_{i}$. Then, the estimated curve is $\hat{f}_{i}(t) = [1\;\; t\;\;t^{2}\;\;...\;\;t^{d}]\;\hat{\BS\beta}_{i}$ where 
$$\hat{\BS\beta}_{i} = (\B X^{T}_{i}\B X_{i})^{-1}\B X^{T}_{i}\B y_{i}$$
and $\B X_{i}$ is the within-individual design matrix determined by the polynomial function evaluated at the observation times of individual $i$. Then, the estimated derivative is equal to
$\hat{f}_{i}^{'}(t) = [0\;\; 1\;\;2t\;\;...\;\;dt^{d-1}]\;\hat{\BS\beta}_{i}$. In other words, he projects the data onto a polynomial basis of degree $d$, differentiates the basis and calculates the estimated derivative function, which can be represented by another polynomial basis of degree $d-1$. Zerbe suggests using the $L_{2}$ distance on the derivative functions \cite{schneiderman1993}. The dissimilarity between the $i$th and $j$th individuals equals
$$d_{ij} =\left[ \int_{\mathcal{T}} [\hat{f}^{'}_{i}(t)-\hat{f}^{'}_{j}(t)]^{2}dt\right]^{1/2}$$
for a chosen time interval, $\mathcal{T}$.This is easy to calculate since the functions can be written with a polynomial basis.

At the end of their paper about clustering functional data, Tarpey and Kinateder make a few suggestions of ways to cluster data after getting 'rid of dominating variability in the intercept' \cite{tarpey2003}. One proposal is to cluster the derivatives of the estimated functions. In personal correspondence with one of the authors, he clarified the details of the implementation. After projecting individuals' data onto a Fourier basis, they differentiate the Fourier basis functions and use the K-means algorithm on the coefficients of the derivative functions. Thus, the estimated function for individual $i$ is $\hat{f}_{i}(t)= [1\;\; \sin(t)\;\;\cos(t)\;\;...\;\;\sin(wt)\;\;\cos(wt)]\;\hat{\BS\beta}_{i}$ where
$$\hat{\BS\beta}_{i} = (\B X^{T}_{i}\B X_{i})^{-1}\B X^{T}_{i}\B y_{i}$$
and $\B X_{i}$ is the within-individual design matrix determined by the Fourier expansion evaluated at the observation times of individual $i$. Since $\frac{d}{dt}\cos(wt) = -w\sin(wt)$ and $\frac{d}{dt}\sin(wt) = w\cos(wt)$, the estimated derivative function for individual $i$ equals the Fourier basis with new coefficients, $\hat{\BS\alpha}_{i}$, equal to a linear transformation of $\hat{\BS\beta}_{i}$. The dissimilarity between the $i$th and $j$th individuals equals
$$d_{ij} = (\hat{\BS\alpha}_{i}-\hat{\BS\alpha}_{j})^{T}(\hat{\BS\alpha}_{i}-\hat{\BS\alpha}_{j}).$$

These methods use the same general procedure. Individual trajectories are smoothed by projecting the data onto a chosen basis, the derivative of the estimated function is calculated by differentiating the basis functions, and the dissimilarity is based on the derivative functions. Now with any smoothing procedure, there is a fundamental bias-variance tradeoff. In this case, choosing including higher ordered terms or more sine and cosine functions in the basis decreases the bias but increases the variance. The basis needs to be determine in order to strike a balance between the two so as to not over fit the data. The choice of basis also affects the differentiation process. Polynomial and Fourier bases are computationally convenient in that differentiation results in a basis of the same type.

 The two methods differ in how the dissimilarity measure is defined between two individuals. Zerbe used the $L_{2}$ distance between two derivative functions; Tarpey and Kinateder calculated the squared Euclidean distance between the basis coefficient vectors for the estimated derivative function. This reflects the diversity in the general functional cluster analysis literature; some use the $L_{2}$ distance on functions \cite{hitchcock2007} while others use the linear coefficients of the basis function \cite{serban2005, tarpey2003, abraham2003}.
 
Tarpey reconciled these two approaches by showing that clustering functional data using the $L_{2}$ metric on function space can be achieved by clustering a suitable linear transformation of the regression coefficients \cite{tarpey2007}. If $y(t)$ is a functional observation represented as $y(t)=\sum_{j}\beta_{u}u_{j}(t)$and $\mu(t)$ is a functional cluster mean represented as $\mu(t) = \sum_{j}\gamma_{j}u_{j}(t)$, then the squared $L^{2}$ distance on interval $\mathcal{T}$ is
\begin{align*}
\int_{\mathcal{T}}(y(t)-\mu(t))^{2}dt &= \int_{\mathcal{T}}(\sum_{j}(\beta_{j}-\gamma_{j})u_{j}(t))^{2}dt\\
&= \sum_{j}\sum_{l}(\beta_{j}-\gamma_{j})(\beta_{l}-\gamma_{l}) \int_{\mathcal{T}}u_{j}(t)u_{l}(t)dt\\
&=(\BS\beta-\BS\gamma)^{T}\B W (\BS\beta-\BS\gamma)\\
&=(\B W^{1/2}(\BS\beta-\BS\gamma))^{T} (\B W^{1/2}(\BS\beta-\BS\gamma))
\end{align*}
where $\B W_{jl} = \int_{\mathcal{T}}u_{j}(t)u_{l}(t)dt$. Therefore, the clustering with $L^{2}$ distance is equivalent to plugging transformed coefficients, $\B W^{1/2}\BS\beta$, into the K-means algorithm. Consequently, when the functions are represented using an orthogonal basis such as the Fourier expansion, K-means on the coefficients is equivalent to the $L^{2}$ implementation.\\

Both the polynomial and Fourier bases are restrictive. The Fourier basis only works well when the data is periodic in nature and a polynomial basis does not provide a general structure to represent complex functions with few parameters. Another popular basis is the class of B-splines \cite{deboor1978, schumaker1981}, which extend the advantages of polynomials to include greater flexibility \cite{abraham2003}. To the author's knowledge, no other study have focused on clustering longitudinal data using the coefficients of the B-spline derivative estimate.

In this following sections, we introduce B-spline functions and demonstrate how they can be used to estimate derivative functions. Then, the implementation and practical decisions that need to be made in this clustering method are presented and discussed.
\subsection{B-spline background}
We fit a m-order spline function to each subject $i$ in order to estimate its underlying smooth, $f_i$. For the sake of being self-contained, we include some background on splines. Let $t\in[a,b]$ where $a,b\in\mathbb{R}$ and $\xi_0=a<\xi_{1}<\cdots<\xi_{L} < b = \xi_{L+1}$ be a subdivision of  the interval $[a,b]$ by $L$ distinct points, known as internal knots. We now define the augmented knot sequence, $\tau=[\tau_{1},...,\tau_{L+2m}]$ for $m\in\mathbb{N}$, such that 
\begin{align*}
\tau_{1}&=\tau_{2}=\cdots =\tau_{m} =\xi_{0}\\
\tau_{j+m}& = \xi_{j}, \quad\quad j=1,...,L\\
\xi_{L+1}&=\tau_{L+m+1}=\tau_{L+m+2}=\cdots =\tau_{L+2m} 
\end{align*}
The spline function, $f(t)$, is a polynomial of order $m$ on every interval $[\tau_{j-1},\tau_{j}]$ and has $m-2$ continuous derivatives on the interval $(a,b)$. The set of spline functions of order $m$ for a fixed sequence of knots, $\tau = [\tau_1,...,\tau_{L+2m}]$, is a linear space of functions with $L+m$ free parameters. A useful basis $B_{1,m}(t),...,B_{L+m,m}(t)$ for this linear space is given by Schoenbergs' B-splines \cite{curry1966, de1976} defined as
\begin{align*}
B_{j,1}(t) &= \begin{cases}
1 \text{ if }\tau_j\leq t < \tau_{j+1}\\
0\text{ otherwise}
\end{cases}\\
B_{j,l}(t) &= \frac{t-\tau_j}{\tau_{j+l-1}-\tau_j} B_{j,l-1}(t)+\frac{\tau_{j+l}-t}{\tau_{j+l}-\tau_{j+1}} B_{j+1,l-1}(t)
\end{align*}
where $l=2,...,m$ and $j=1,...,L+2m-l$.  If we adopt the convention that $B_{j,1}(t)=0$ for all $t\in\mathbb{R}$ if $\tau_{j}=\tau_{j+1}$, then by induction $B_{j,l}(t)=0$ if $\tau_{j}=\tau_{j+1}=\cdots=\tau_{j+l}$. Hence, $B_{1,l}(t)=0$ for $t\in\mathbb{R}$ and $l<m$ on the defined knot sequence. The B-spline function of order $m$ is defined by
$$f(t) = \sum^{L+m}_{j=1} \beta_j B_{j,m}(t).$$
Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote a vector of repeated observations for individual $i$ observed at times $\B t_{i}=(t_{i1},...,t_{im_{i}})  for $i=1,...,n$.$ We assume that $y_{ij} = f_{i}(t_{ij}) + \epsilon_{ij}$ such that $E(\epsilon_{ij}) = 0$ for all $j=1,...,m_{i}$ and $i=1,...,n$.  To estimate $f_{i}$, we fix the order of the B-spline, $m$, and internal knots. Then we estimate the coefficients, $\BS \beta^{(i)} = (\beta^{(i)}_1,...,\beta^{(i)}_{L+m})$ using least squares, 
$$\hat{\BS \beta}^{(i)} = (\B X_{i}^{T}\B X_{i})^{-1}\B X_{i}\B y_{i}$$
where $\B X_{i}$ is the within-individual design matrix based on the B-spline basis functions. Then, the estimated function is $\hat{f}_i(t)=\sum^{L+m}_{j=1} \hat{\beta}^{(i)}_j B_{j,m}(t)$. \\

To estimate $f_i'(t)$, we differentiate $\hat{f}_i(t)$ with respect to $t$ and we get
$$\hat{f}'_i(t)=\sum^{L+m}_{j=1} \hat{\beta}^{(i)}_j B'_{j,m}(t)$$
Prochazkova \cite{prochazkova2005} showed that this can be simplified to
$$\hat{f}'_i(t)=\sum^{L+m}_{j=2} \hat{\beta}^{(i)}_j \left[\frac{m-1}{\tau_{j+m-1}-\tau_j} B_{j,m-1}(t)-\frac{m-1}{\tau_{j+m}-\tau_{j+1}} B_{j+1,m-1}(t)\right].$$
However, we can go one step further and write this in terms of a B-spline basis of one order lower.
\begin{align*}
\hat{f}'_i(t)&=\sum^{L+m-1}_{j=1} (\hat{\beta}^{(i)}_{j+1} -\hat{\beta}^{(i)}_j)\frac{m-1}{\tau_{j+m}-\tau_{j+1} }B_{j+1,m-1}(t)
\end{align*}
If we adjust the knot sequence to only have $m-1$ replicates at the beginning and end, then $\hat{f}'_{i}(t)$ can be written as
$$\hat{f}'_i(t)=\sum^{L+m-1}_{j=1}\hat{\alpha}^{(i)}_jB_{j,m-1}(t)$$
where $\hat{\alpha}^{(i)}_j= (\hat{\beta}^{(i)}_{j+1} -\hat{\beta}^{(i)}_j)\frac{m-1}{\tau_{j+m}-\tau_{j+1} }$.
We use these derivative coefficients, $\hat{\BS \alpha}^{(i)} = [\hat{\alpha}^{(i)}_1,...,\hat{\alpha}^{(i)}_{ L+m-1}]$, to cluster trajectories with similar shape with the K-means algorithm \cite{macqueen1967, hartigan1979}. 
\subsection{Implementation}
To use this method in practice, decisions about the B-spline basis need to be made a priori. First, the order of the polynomials must be chosen. This together with the number of internal knots impacts the flexibility of the B-spline function. Cubic B-splines of order four have been shown to have good mathematical properties and are used frequently in practice \cite{james2003}. However, depending on the number of data points observed per subject, it may be necessary to use a quadratic polynomial ($m = 3$) due to the restriction that the sum of the order and the number of internal knots must be less than or equal to the minimum number of observation points per subject. 

The number of internal knots, $L$, plays a role in the flexibility of the class of spline functions and should be large enough to fit the features in the data. Just as with the order, the main limiting factor in choosing L is be the number of data points per subject. In longitudinal considered in this thesis, the number of data points per subjects is about five. It is important not to over fit the individual curves so for data with limited observation, it may only be possible to have a t most one internal knot. If there are many repeated measures, model selection information criteria or cross-validation can be used to choose $L$ \cite{rice2001}. The location of the internal knots is another issue of discussion. There are some suggested data-driven ways to select knot location \cite{shanggang2001}, but Ruppert \cite{ruppert2002} supports fixing the knots at sample quantiles. We follow his suggestion and adjust them as necessary to the areas with the most action. 

Once the order of the polynomials and number and location of knots are chosen, then smooth functions and their derivatives are estimated using Least Squares for every subject. The coefficients from the estimated derivative functions become the input vectors to the clustering procedure. The K-means algorithm is a standard clustering algorithm that given a fixed number of clusters, minimizes the within sum of squares after initially randomizing vectors to groups. This algorithm converges but there is no guarantee that it will find the grouping that globally minimizes the objective function. Therefore, in practice, the algorithm is run multiple times with different random initializations and the clustering that minimizes the objective function is chosen. We use 25 random starts.

However, in order to run K-means, you need to fix the number of clusters, $K$, which is usually unknown and of interest to researchers. While no perfect mathematical criterion exists, a number of heuristics (see \cite{tibshirani2001} and discussion therein) are available for choosing $K$. For this thesis, we choose the $K$ that maximizes the overall average silhouette width \cite{rousseeuw1987}.  For each subject $i$, the silhouette width, $s_{i}$, is calculated by
$$s_{i}=\frac{b_{i}-a_{i}}{\max\{a_{i},b_{i}\}}$$
where $a_{i}$ is the average dissimilarity of $i$ to all other trajectories in its cluster, $A$, $d_{i}(C)$ is the average dissimilarity of $i$ to all trajectories of cluster $C$ and $b_{i}$ is equal to $\min_{C\not= A} d_{i}(C)$. The overall average silhouette width, $\bar{s}$, is the average of $s_{i}$ over all subjects in the whole data set. By maximizing the overall average silhouette width, the dissimilarity between clusters is maximized and the dissimilarity within clusters is minimized resulting in distinct groups. In this case, the dissimilarity between two individuals is defined as the squared Euclidean distance between derivative coefficients. We choose $K$ that best separates the data so we have groups of individuals with similar shape patterns over time.

K-means is a partitioning algorithm, therefore, by definition every subject is `hard' clustered into only one of the groups. There is no stated uncertainty in the group memberships even if a subject is in the overlap between two clusters. In order to estimate the relationship between baseline variables and shape group membership, we assume the cluster labels are known. Given the subject grouping labels from the partition, $\{c_{i}\}$ such that $c_{i}\in\{1,2,...,K\}$ for all $i=1,...n$, we fit a multinomial logistic regression model, which is an extension of the logistic regression model, using the group labels as the outcome and baseline factors as explanatory variables such that
$$P(c_{i} = k|\B z_{i}) = \frac{\exp(\B z_{i}^{T}\BS\gamma_{k})}{\sum_{j=1}^{K}\exp(\B z_{i}^{T}\BS\gamma_{j})}$$
for all $k=1,...,K$ and $i=1,...,n$ with $\BS\gamma_{K}=0$ where $\B z_{i}$ is the design vector based on baseline factors. We then estimate the parameters $\BS\gamma_{1},...,\BS\gamma_{K-1}$ using maximum likelihood estimation. The estimated standard errors from a standard Hessian calculation do not include any group membership uncertainty  as we assume the hard clustering labels are known for the estimation process.

\section{Multilayer mixture model}\label{chp4:multi}
The second method attempts to make up for the lack of uncertainty in groups memberships by using a model-based approach. The circumstances in which traditional clustering methods fail are when the level and shape are weakly dependent resulting in individuals with the same shape but at different levels. As mentioned previously if the levels within a shape group are normally distributed, using an exchangeable correlation matrix takes that variability into account within the standard mixture model. However, if the levels do not satisfy those assumptions, a mixture of Normals could be used to model the non-Gaussian distribution of levels. 
\subsection{Related work}
Model-based methods provide the probability framework that dissimilarity-based methods lack. One of the main benefits is the ability to take into account uncertainty about of the mean estimates and cluster membership. As mentioned in Chapter 1, the main model-based method is the finite mixture model. However, a standard mixture of Gaussians fit to the data does not distinguish between the level and shape and will cluster subjects based on the dominant source of variability. Another limitation of Gaussian mixtures for grouping data is that the cluster densities may be non-normal. For these reasons, the mixture model has been extended to the multilayer mixture model in which each cluster is allowed to be a mixture \cite{li2005}.  A variation of this idea has been used in clustering non-normal groups by first fitting a mixture and then combining cluster components to make more meaningful clusters \cite{hennig2010}. For example, imagine a data set with two groups with bimodal densities. An obvious model would be a multilayer mixture of two clusters each with two components. (see Figure \ref{fig:dia} for diagram).
\begin{figure}[h]
\begin{center}
\includegraphics[width=4in]{Chp4multilayer_diagram}
\end{center}
\label{fig:dia}
\caption{Diagram of multilayer mixture model showing that each cluster is comprised of potentially more than one component.}
\end{figure}

In this thesis, the goal is to have clusters be meaningful in terms of distinguishing between shape. Therefore, the idea of multilayer mixtures can be used to model $K$ non-normal shape clusters comprised of mixture components with the same mean shape at different levels.


\subsection{Model specification}
In a multilayer mixture model, let $K$ be the number of clusters and $J_{k}$ be the number of components in cluster $k=1,...,K$ such that $J=\sum_{k=1}^{K}J_{k}$ is the total number of components in the entire model. Let $j$ uniquely index all of the components such that $j=1,...,J$. For ease of explanation, let $c(j):\{1,...,J\}\rightarrow \{1,2,...,K\}$ be a cluster assigning function to specify the cluster to which a component belongs. Let $f(\B y|\BS\mu,\BS\Sigma)$ be the probability density function of a multivariate normal distribution with mean $\BS\mu$ and covariance matrix $\BS\Sigma$. Then the probability density function for cluster $k$ is
$$f_{k}(\B y) = \sum_{j: c(j) = k} \pi_{j|c(j)}f(\B y|\mu_{j},\Sigma_{j})$$
where $\pi_{j|c(j)}$ is the probability of being in component $j$ given a sample is in cluster $c(j)$ and $\sum_{j: c(j) = k}\pi_{j|c(j)}=1$ for all $k=1,2,...,K$. Let the probability of cluster $k$ given baseline factors, $\B w$, be $\pi_{k}(\B w,\BS\gamma)=\exp(\B w^{T}\BS\gamma_{k})/\sum^{K}_{l=1}\exp(\B z^{T}\BS\gamma_{l})$ such that $\BS\gamma_{K}=0$ and $\BS\gamma=(\BS\gamma_{1},...,\BS\gamma_{K})$. Then the probability density function for the multilayer mixture can be written as
$$g(\B y|\B w) = \sum_{k=1}^{K}\pi_{k}(\B w,\BS \gamma)f_{k}(\B y) = \sum_{k=1}^{K}\pi_{k}(\B w,\BS\gamma)\sum_{j: c(j) = k} \pi_{j|c(j)}f( \B y| \BS\mu_{j},\BS\Sigma_{j})$$
Since every component belongs to one and only one shape cluster, the above equation reduces to a regular mixture model if $\bar{\pi}_{j}(\B z,\BS \gamma)=\pi_{c(j)}(\B z,\BS\gamma)\pi_{j|c(j)}$,
$$g(\B y|\B w) = \sum_{j=1}^{J}\bar{\pi}_{j}(\B w,\gamma)f(\B y|\BS \mu_{j},\BS\Sigma_{j})$$
In order to accommodate the goal of clustering on shape, the mean structure includes a regression parameterization to model the smooth underlying group mean shape function. The design matrix for the regression includes B-spline basis functions presented earlier in this chapter. However, the regression parameters are restricted to be the same within shape clusters and the intercept are allowed differ in each level component. Hence, $\mu_{j}=\alpha_{j}+\B x \BS\beta_{c(j)}$, where $\B x$ is a matrix of B-spline basis functions excluding the first basis function to allow for estimation of intercept terms for each component. This allows the clusters to be based on shape while the components can have different levels (see figure \ref{fig:diashape} for diagram). Additionally, I assume $\Sigma_{j}=\sigma^{2}_{j}I$ for all $j=1,...,J$  for simplicity and to allow for irregularly sampled data.
\begin{figure}[h]
\begin{center}
\includegraphics[width=4in]{Chp4multilayer_diagram_shape}
\end{center}
\label{fig:diashape}
\caption{Diagram of multilayer mixture model showing that each shape cluster is comprised of potentially more than one level component.}
\end{figure}

\subsection{Implementation}
Denote the shape cluster identity of sample $i$ by $\eta_{i}$ where $\eta_{i}\in\{1,...,K\}$. Then, $\BS\eta = \{\eta_{i}; i=1,...,n\}$ and $\BS\theta=\{\BS \gamma_{k}, \pi_{j|c(j)},\alpha_{j}, \BS\beta_{k},\sigma^{2}_{j};\; j=1,...,J, k=1,...,K\}$ are the parameters for the model. In order to estimate the parameters in this multilayer mixture model, we maximize the classification likelihood function,
\begin{align}
 L(\theta,\eta) &= \sum^{n}_{i=1} \log \pi_{\eta_{i}}(\B w_{i},\BS\gamma) f_{\eta_{i}}(\B y_{i}|\B x_{i})\\
\label{ll}& =  \sum^{n}_{i=1} \log\left[ \pi_{\eta_{i}}(\B w_{i},\BS\gamma)  \sum_{j: c(j) = \eta_{i}} \pi_{j|c(j)}f(\B y_{i}|\B x_{i},\alpha_{j},\BS\beta_{\eta_{i}},\sigma^{2}_{j})\right]
\end{align} 
using a modified EM algorithm called the classification expectation maximization algorithm (CEM) \ref{celeux1992;mclachlan2000}. The modification involves adding a classification step between the expectation step and maximization step where individuals are assigned to shape clusters. The algorithm continues to iterate between these three steps until convergence. 

In order to start the iterative process, the individuals are initially assigned into shape clusters and component groups. Initialization can involve randomly partitioning individuals into components or strategically partitioning individuals into shape clusters using a quick procedure such as the first proposed method, derivative coefficient K-means, and then randomly partitioning the individuals into components. 

Let $\BS\theta^{(t)}$ and $\BS\eta^{(t)}$ be the current estimates of the parameters at iteration $t$. The algorithm updates these estimates as follows:
\begin{enumerate}
\item Expectation step: For each individual $i$, compute the posterior probability of being in shape cluster $k$, $p_{i,k}$, such that
$$p_{i,k}= \pi_{k}(\B z_{i},\BS\gamma)f_{k}(\B y_{i}|\B x_{i})/\sum^{K}_{j=1} \pi_{j}(\B z_{i},\BS\gamma)f_{j}(\B y_{i}|\B x_{i})$$
for $i=1,...,n$ and $k=1,...,K$.
\item Classification step: Hard classify subjects to shape clusters according to $\eta^{(t+1)}_{i} = \arg\max_{k} p_{i,k}$.
\item Maximization step: For each shape cluster, use maximum likelihood estimation to update parameter vector $\theta$ by embedding an EM procedure initialized with the current parameter values.
\end{enumerate} 
This algorithm increases the log-likelihood at each iteration and the statement is below. The proof is in the Appendix \ref{append:2}. 
\begin{theorem} The classification likelihood $L(\BS\theta,\BS\eta)$ defined in (\ref{ll}) is non-decreasing after each update of $\BS\eta$ and $\BS\theta$ by the CEM algorithm. That is, $L(\BS\theta^{(t+1)},\BS\eta^{(t+1)}) \geq L(\BS\theta^{(t)},\BS\eta^{(t)})$ for all $t$.
\end{theorem}



In the maximization step of the algorithm, parameters are estimated by maximizing the likelihood for each shape cluster. However, the shape parameters are constrained to be equal for all components within the cluster. To estimate both the component and cluster-specific parameters simultaneously, this thesis uses computational methods suggested by \textcite{grun2008}. For shape cluster $k$, the outcome vector for subject $i$ is temporarily replaced by a vector of $\B y_{i}$ repeated $J_{k}$ times. The new design matrix  is set equal to $(I_{J_{k}}\otimes \BS 1_{m_{i}}, \BS 1_{J_{k}} \otimes \B x_{i})$ where $\otimes$ refers to the Kronecker product. Lastly, the covariance matrix structure is block diagonal with $\sigma^{2}_{j}I_{m_{i}}$ in each block for $j$ that satisfy $c(j)=k$. Assuming a multivariate Gaussian distribution, the likelihood function is maximized with respect to the parameters for cluster $k$ and its components using profile likelihoods and a standard numerical optimization routine.

Now, as in the derivative coefficient method, B-splines are used so the mean structure needs is flexible enough to accommodate shapes.The issues of the number and location of internal knots are dealt with in the same manner.

With this more complex mixture structure, it is necessary to select the number of shape clusters, $K$, as well as the number of components for each cluster $k$, $J_{k}$, $k=1,..,K$. It is recommended to fix $K$ and then use model selection criteria to select $J_{k}$ for each $k=1,...,K$. As suggested by \textcite{li2005}, the BIC is used to select $J_{k}$ even though the regularity conditions do not hold as the criteria has been shown to be useful a informal guide in practice.

Derivations of robust standard error were done following the same procedure as \textcite{boldea2009}. These are available in Appendix \ref{append:3}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Vertical shifted mixture model}
The goal of cluster analysis is to partition a collection of individuals into homogeneous groups. In this thesis, similar individuals are those with the same outcome shape pattern over time. If two curves only differ in intercepts by vertical shifts, they are placed in the same group. Up until now, the two proposed methods use derivatives to implicitly remove the level or directly model the variability in the level with an additional layer of mixture models.  In this method, I consider subtracting the mean outcome level. Each cluster comprises a density with a mean shape curve and a covariance function. The assumed finite mixture of densities is fit to the vertically shifted data to estimate parameters and group membership probabilities. 

\subsection{Recent work}
A finite mixture model is a standard method for clustering multivariate data \cite{everitt2009} and has been used for longitudinal applications \cite{muthen2010, jones2001}. See \cite{mclachlan2000} for an extensive summary of finite mixture models. However, for longitudinal data, the models are commonly used for the observed data without much regard to the goal of clustering by shape. 

We suggest removing the level by subtracting out the mean outcome level prior to modeling. Subtracting the mean is not a novel idea in statistics or even cluster analysis. In fact, experimental data such as gene expression microarrays are often normalized to compensate for variability in the measurement device between samples. In cluster analysis of multivariate data, it is recommended that each variable is standardized by subtracting the mean of the variable measures and dividing by the standard deviation so that each standardized variable is in comparable units and equally contributes to the grouping process. This is not recommended for the longitudinal setting where each variable is a repeated measurement at a different time point. To compare shapes, the original scale needs to be maintained since the relationship between measurements within individuals is of interest. Therefore, any transformation performed should only be additive in nature to preserve the original shape of the data over time. In general, pre-processing the data can provide the path to answering the research question but any transformation of the data should be carefully studied for potential unintended consequences.

A version of this idea has been implemented in the functional data analysis literature. For processes in a Hilbert space of square integrable functions with respect to the Lebesgue measure, $dt$, on the interval $\mathcal{T}=[0,T]$, \textcite{chiou2008} propose using a mixture model and the Karhunen-Lo{\`e}ve expansion for centered stochastic processes within their correlation-based clustering algorithm. The integral of the random function over interval $\mathcal{T}$ divided by $T$, the length of the interval, is subtracted to center the process; the resulting process integrates to zero. The integral of the process is the functional analogue to a mean vector in vector space; similarly, the resulting vector has mean zero after subtraction.

 Although centering a process and a shifting a vector stems from the same idea, there are distinct consequences of subtracting the estimated level of a noisy curve observed at a finite number of points that do not arise when centering a smooth function. The term centering is used in the stochastic processes literature, but I use the term vertically shifting to refer to the procedure of subtracting the mean since it graphically describes the transformation of the noisy longitudinal data.

\section{Model Specification}
 Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote an outcome vector of repeated observations for individual $i$, $i=1,...,n$. The vector of corresponding times of observation for individual $i$ is denoted as $\B t_{i}=(t_{i1},...,t_{im_{i}})$ and $\B w_{i}$ is a $q$-length design vector based on time-fixed factors that are typically collected at or before time $t_{i1}$. We assume that there are $K$ mean shape functions, $\mu_{k}(t)$, in the population such that the outcome vector for individual $i$ in shape group $k$ is
 $$\B y_{i} = \lambda_{i}\B 1_{m_{i}}+\BS\mu_{i}+\BS\epsilon_{i},\quad \lambda_{i}\sim F, \quad \BS\epsilon_{i}\sim N(0,\BS\Sigma_{k})$$
 where $F$ is a probability distribution, $\B 1_{m_{i}}$ is a m-length vector of 1's, and $\mu_{ij} = \mu_{k}(t_{ij})$ is the $j$th element of a $m_{i}$-length vector of mean values evaluated at the observation times, $\B t_{i}$. Therefore, this outcome vector is determined by a mean shape, a random intercept, and potentially correlated errors. The probability of being in a particular shape group could depend on baseline covariates $\B w_{i}$. Let $\bar{y}_{i}= m_{i}^{-1}\sum^{m_{i}}_{j=1} y_{ij} = \lambda_{i}+\bar{\mu}_{i}+\bar{\epsilon}_{i}$ be the mean of the outcomes for individual $i$. This is one measure of the vertical level of the data vector and it can be removed by applying a linear transform, $\B A_{i} = \B I_{m_{i}} - m_{i}^{-1}\B 1_{m_{i}}\B 1_{m_{i}}^{T}$, to the vector of observations. The vertically shifted vector equals 
\begin{align*}
\B y^{*}_{i} &= \B A_{i}\B y_{i}\\
&=\B A_{i}(\lambda_{i}\B 1_{m_{i}}+\BS\mu_{ik}+\BS\epsilon_{i})\\
&=\B A_{i}(\BS\mu_{i}+\BS\epsilon_{i})\\
&=\BS\mu_{ik} - \bar{\mu}_{i}+\epsilon_{i}-\bar{\epsilon}_{i}.
\end{align*}
Applying the symmetric matrix $\B A_{i}$ to the vector $\B y_{i}$ subtracts the individual mean, $\bar{y}_{i}$, from each element $\B y_{i}$. This results in the removal of the random intercept, $\lambda_{i}$, leaving the mean function and error shifted by a random constant. Hence, we do not have to worry about the distribution of the random intercept, $\lambda_{i}$.

Once the level is removed, I assume the vertically shifted data, $\B y_{i}^{*}$, follow a Gaussian mixture of $K$ groups with mean shape functions and random noise. We differentiate the parameters of the model on the original data and the transformed data with an asterisk. If the observation times are fixed, vertically shifted data generated from the model above would exactly follow this Gaussian mixture. We assume that, conditional on observation times $\B t$ and baseline covariates $\B w$, $\B y^{*}$ is a realization from a finite mixture model with density
\begin{align*}
 f(\B y^{*}|\B t,\B w,\BS\theta^{*}) =  \sum^{K}_{k=1}\pi^{*}_{k}(\B w,\BS \gamma^{*})f_{k}( \B y^{*}|\B t,\BS\theta^{*}_{k})
\end{align*}
where $\pi^{*}_{k}$ is the prior probability of being in the $k$th component and $\BS\theta^{*} = (\BS\gamma^{*},\BS\theta^{*}_{1},...,\BS\theta^{*}_{K})$. To allow baseline covariates to impact the probability of having a certain shape pattern over time, the prior probabilities are parameterized using the generalized logit function of the form
$$\pi^{*}_{k}(\B w,\BS\gamma^{*})=\frac{\exp(\B w^{T}\BS\gamma^{*}_{k})}{\sum_{j=1}^{K}\exp(\B w^{T}\BS\gamma^{*}_{j})}$$ 
for $k=1,...,K$ where $\BS \gamma^{*}_{k}\in\mathbb{R}^{q}$, $\BS\gamma^{*} = (\BS\gamma^{*}_{1},...,\BS\gamma^{*}_{K})$, and fix $\BS\gamma^{*}_{K}=\B 0$. We assume the component densities $f_{k}(\B y^{*}|\B t,\BS\theta^{*}_{k})$ are multivariate Gaussian densities with mean and covariance dependent  on time.

\subsection{Mean Structure}
The mean shape is modeled as a smooth function of time represented using a chosen functional basis. If the shape is periodic in nature, a Fourier basis is appropriate. Another common basis is a lower order polynomial basis such as $\{1, t, t^{2}\}$. However, this basis cannot capture complex shapes with drastic changes. 

To allow for local changes, the time interval, $[a,b]$, is broken up into smaller interval using $L$ knots, $a<\tau_{1}<\cdots<\tau_{L}<b$, and fit polynomials of order $p$ in each subinterval. This piecewise polynomial can be expressed as a linear combination of truncated power functions and polynomials of order $p$. In other words,
$\{1,t,t^{2},...,t^{p-1},(t-\tau_{1})_{+}^{p-1},...,(t-\tau_{L})_{+}^{p-1}\}$
is a basis for a piecewise polynomial with knots at $\tau_{1},...,\tau_{L}$. However, the normal equations associated with the truncated power basis are highly ill-conditioned. 

A better conditioned basis for the same function space is the B-spline basis \cite{deboor1978, schumaker1981,curry1966, de1976}. A B-spline function of order $p$ with $L$ internal knots, $\tau_{1},...,\tau_{L}$, is defined by a linear combination of coefficients and B-spline basis functions
$$\mu(t) = \sum^{L+p}_{j=1} \beta_j B_{j,p}(t)$$
where the basis functions, $B_{j,p}(t)$, are defined iteratively \cite{deboor1972,cox1972}. We construct a matrix  of values from the $p$th order B-spline basis functions taken at observation times $\B t_{i}$ to model the mean values. We assume the mean pattern for the $k$th shape cluster is approximated by the function $\BS\mu^{*}_{k}=\B x_{i}\BS\beta^{*}_{k}$ where $\BS\beta^{*}_{k}=(\beta^{*}_{k,1},...,\beta^{*}_{k, L+p}).$  

\subsection{Covariance Structure}
There are many potential structure that could be assumed for the covariance matrix. Here, I allow the covariance to differ between clusters. Since it is common for longitudinal data to have sparse, irregular time sampling, the covariance matrix needs structure to allow for parameter estimation as described by \textcite{jennrich1986} in their seminal paper. A common parameterization is conditional independence with constant variance where $\B \Sigma^{*}_{k}= \sigma_{k}^{*2}I_{m_{i}}$. This is an unrealistic assumption for longitudinal data since there is inherent dependence between repeated measures on the same unit. 

Compound symmetry, which is also known as exchangeable correlation, is a popular correlation structure in longitudinal analysis where all repeated measures are equally correlated. This is typically paired with constant variance such that $\B \Sigma^{*}_{k} = \sigma^{*}_{k}^{2}(\rho^{*}_{k}\B1_{m_{i}}\B1_{m_{i}}^{T}+(1-\rho_{k})I_{m_{i}})$ where $-1\leq\rho_{k}\leq 1$ is the correlation between any two distinct measurements within an individual. This dependence structure describes the resulting correlation matrix of a random intercept model.

Another structure that provides a compromise between the two is the exponential correlation structure in which the dependence decays as the time between observations increases such that $[\B \Sigma^{*}_{k}]_{jl} = \sigma_{k}^{*2}\exp(-| t_{ij}-t_{il}| / r^{*}_{k})$ where $r_{k}> 0$ is the range of the dependence. If the range, $r_{k}$, is small, the correlation decays quickly, but if the $r_{k}$ is large, there is long range dependence between measurements within an individual. This structure is similar to the correlation matrix generated from a continuous autoregressive model of order one such that $[\B\Sigma^{*}_{k}]_{jl} = \sigma^{*2}\rho_{k}^{*|t_{ij}-t_{ill}|}$ where $\rho^{*}_{k}$ is the correlation for measurements observed one unit of time apart. If $\rho^{*}_{k} = \exp(-1/r^{*}_{k})$, then the two parameterization result in the same structure if the correlation between two measures is constrained to be positive. This is a reasonable assumption for longitudinal data in the original form but it many not be acceptable for the transformed data as discussed later.

All of the covariance structures mentioned above are associated with weakly stationary processes with constant variance and correlation dependent only on the time lag between observations. If the variance or correlation function is non-constant but varying continuously, it could be potentially modeled as a function, but estimation is more difficult.

It is important to model the covariance structure correctly as misspecification can highly impact mixture model results in terms of parameter estimates and the final clustering if the groups are not well-separated \cite{heggeseth2013}. Transforming the data brings individuals with similar shapes closer but also brings others closer as well. In general, this decreases the separation between groups, which may force us to accurately model the correlation. 

\section{Implementation}
Given a collection of independent observed outcome vectors, $\B y_{1},...,\B y_{n}$. The first step is to calculate the mean for each subject, $\bar{y}_{i}$, $i=1,...,n$ and subtract the subject-specific mean from the observed outcome vector. This transformation leaves vertically shifted independent vectors, $\B y^{*}_{1},...,\B y^{*}_{n}$.  

Then, the order of the spline and the number and location of internal knots for the mean structure is chosen. The B-spline basis should be kept constant for all shape groups so the simplest way to select the number of knots is through visual inspection of the full data set. If the most complex shape patterns is a lower order polynomial, no internal knots are necessary. However, if the most complex curve has localized activity, adding knots and increasing the order of the spline functions can flexibly accommodate the twists and turns of the mean patterns. In choosing both the order of the polynomials and the number of knots, it is important to balance the number of mean parameters with the sample size. Every unit increase in the order or in the number of knots increases the number of parameters by $K,$ the number of groups. In terms of location of the knots, one suggestion is to place knots at sample quantiles based on the sampling times of all the observations \cite{ruppert2002}. However, this strategy may not work well if the median time is not the point of deviation. If possible, it is best to place knots at local maxima, minima, and inflection points of the overall trends as to accommodate the differences from a polynomial function \cite{eubank1999}.  Once these are decided, the design matrixes $\B x_{i}$ are calculated using widely available B-spline algorithms. 

Parameters are estimated using maximum likelihood estimation via the EM algorithm. Under the assumption that $\B y^{*}_{1},...,\B y^{*}_{n}$ are independent realizations from the mixture distribution, $f(\B y^{*} | \B t, \B w, \BS\theta)$, defined above, the log likelihood function for the parameter vector, $\BS \theta^{*}$, is given by
$$\log L(\BS\theta^{*})=\sum^{n}_{i=1}\log f(\B y^{*}_{i}|\B t_{i},\B w_{i},\BS \theta^{*}).$$
The maximum likelihood estimate of $\BS\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\BS\theta^{*})/\partial \BS\theta^{*}=\B 0.$ Solutions of this equation corresponding to local maxima can be found iteratively through the expectation-maximization (EM) algorithm \cite{dempster1977}. This algorithm is applied in the framework where given $(\B t_{i},\B w_{i})$ each $\B y^{*}_{i}$ is assumed to have stemmed from one of the components and the indicator denoting its originating component is missing. The complete-data log likelihood is based on these indicator variables as well as the observed data $\{(\B y^{*}_{i}, \B t_{i}, \B w_{i})\}$. The expectation step (E-step) involves replacing the indicators by current values of their conditional expectation, which is the posterior probability of component membership, written as
$$\alpha_{ik}=\pi^{*}_{k}(\B w_{i},\BS\gamma^{*})f_{k}(\B y^{*}_{i}|\B t_{i},\BS\theta_{k}^{*})/\sum_{j=1}^{K}\pi^{*}_{j}(\B w_{i},\BS\gamma^{*})f_{j}(\B y^{*}_{i}|\B t_{i},\BS \theta_{j}^{*})$$
for $i=1,...,n$ and $k=1,...,K$ using current estimates of the parameters. In the maximization step (M-step), the parameter estimates for the prior probabilities, linear mean, and covariance matrices are updated by maximizing the complete-data log likelihood using the posterior probabilities from the E-step in place of the indicator variables using numerical optimization. The E- and M-steps are alternated repeatedly until convergence. The EM algorithm guarantees convergence to a local maximum; global convergence may be attained through initializing the algorithm by randomly assigning individuals to initial components, running the algorithm multiple times and using the estimates associated with the highest log likelihood. Besides the parameter estimates, the algorithm returns the posterior probability estimates of component membership. These probabilities can be used to partition individuals into distinct clusters by selecting the cluster with the maximum posterior probability. However, unlike K-means, the posterior probability provides some measure of uncertainty in the hard clustering. 

The model requires the number of clusters, $K$, to be known. In practice, this is not the case and $K$ is chosen. The most popular procedure is to chose a maximum value of $K$ such that $K_{max}<<n$, fit the model under all values of $K=2,...,K_{max}$, and choose the value that optimizes a chosen criteria. In this thesis, the BIC \cite{schwarz1978} is used to choose $K$. It is defined as
$$BIC = -2\log L(\hat{\BS\theta})- d\log(n)$$
where $d$ is the length of $\BS\theta$, the number of parameters in the mixture model, and $L(\BS\theta,K)$ is the log likelihood function for the parameter vector. The BIC has been widely used for model selection with mixture models since Roeder and Wasserman's use in 1997 \cite{roeder1997}. In particular, the criteria has been to select the number of clusters \cite{dasgupta1999,fraley1999} with good results in practice. For regular models, the BIC was derived as an approximation to twice the log integrated likelihood using the Laplace method \cite{tierney1986}, but the necessary regularity conditions do not hold for mixture models in general \cite{aitkin1985}. However, Roeder and Wasserman \cite{roeder1997} showed that the BIC leads to a consistent estimator of the mixture density, and Keribin \cite{keribin2000} showed that the BIC is consistent for choosing the number of components in a mixture model.

\section{Potential modeling challenges}
There are issues of identifiability with Gaussian mixture models that can be mitigated through some minor constraints \cite{mclachlan2000}. In this section, we discuss some unique consequences of vertically shifting the data on the model and inference.

\subsection{Covariance of transformed data vectors}
We let $\B Y_{i}=(Y_{i1},...,Y_{im})$ be a random vector observed at times $\B t_{i}=(t_{i1},...,t_{im})$ such that
$\B Y_{i} = \lambda_{i} + \BS\mu_{i} + \BS\epsilon_{i}$
such that $\lambda_{i}\sim F$, $\BS\mu_{i}$ is a vector based on a deterministic function of time, and $\BS\epsilon_{i}\sim(0,\BS\Sigma_{i})$. We let $\B\Sigma_{i} =\B V^{1/2}\B R_{i}(\rho)\B V^{1/2}$ where $\B R_{i}(\rho)$ is an $m\times m$ correlation matrix based on the parameter $\rho$ and potentially the associated observation times, and $\B V$ is a $m\times m$ matrix with variance parameters along the diagonal. If we linearly transform the data to subtract the mean of the elements according to the symmetric matrix $\B A = I_{m}-m^{-1}\B 1_{m}\B 1_{m}^{T}$, the covariance of the resulting random vector is
\begin{align*}
Cov(\B A\B Y_{i}) &= \B A^{T}Cov(\B Y_{i})\B A
\end{align*}
by the properties of covariance. This transformation changes the correlation structure and actually induces correlation between random variables are were originally independent. One important property of this transformation is that it is non-invertible; once the mean is subtracted from the data, the original data cannot be recovered. This has tremendous consequences on the covariance matrix. Since $det(\B A) = 0$, the determinant of $\B A\B Y$ will always be zero. Thus, the matrix is singular. This makes sense since the removal of the mean constrains the data to sum to zero. In other words, the transformation projects the data onto the $(m-1)$-dimensional subspace orthogonal to the nullspace $\B 1$. This presents challenges when trying to fit a Gaussian mixture model. To better understand how best to model the transformed data, I explore the covariance matrices when the observation times are fixed and random. From now on, $I_{m}$ will be written as $I$ and $\B 1_{m}$ as $\B 1$ for simplification.

\subsubsection{Fixed observation times}
For the moment, let us assume that the observation times are fixed, $\B t_{i}=\B t$. Then
\begin{align*}
Cov(\B A\B Y_{i})&= \B A^{T}Cov(\BS\epsilon_{i}\B A\\
&= \B A^{T}\B\Sigma_{i} \B A
\end{align*}
If the variance is constant over time, $\B V=\sigma^{2}I$, and the elements of the vector are independent, $\B R_{i}(\rho)=I$, then the covariance of the transformed vector is
\begin{align*} 
Cov(\B A\B Y_{i}) &= \sigma^{2}\B A\B A^{T} \\
&=\sigma^{2}\B A\\
&= \sigma^{2}(I - m^{-1}\B1\B1^{T})\\
&=\sigma^{2}\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)I)
\end{align*}
 where $a=\frac{-1}{m-1}$. Therefore, if the data has independent errors, subtracting the estimated mean induces negative exchangeable correction between the observations of magnitude $\frac{-1}{m-1}$. Additionally, the variance decreases to $\sigma^{2}\frac{m-1}{m}$. If $m$ is large, the resulting correlation structure is approximately independence with variance $\sigma^{2}$.
 
 If the errors in the original data have constant variance, $\B V=\sigma^{2}I$, and are exchangeable with $\B R_{i}(\rho) = \rho\B 1 \B 1^{T} + (1-\rho) I$, then the covariance of the transformed vector is
 \begin{align*}
 Cov(\B A\B Y_{i}) &= \sigma^{2}\B A\B R_{i}(\rho)\B A^{T}\\
 &= \sigma^{2}(I-m^{-1}\B1\B1^{T})(\rho\B1\B1^{T}+(1-\rho)I)(I-m^{-1}\B1\B1^{T})^{T}\\
 &= \sigma^{2}(1-\rho)(I-m^{-1}\B1\B1^{T})\\
 &=\sigma^{2}(1-\rho)\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)I)
 \end{align*} 
 where $a=\frac{-1}{m-1}$. This transformation maintains the exchangeable structure but with negative correlation on the off diagonal and decreased variance of $\sigma^{2}(1-\rho)\left(\frac{m-1}{m}\right)$.  Again, if the number of observed data points is large, then the structure is approximately independent with variance $\sigma^{2}(1-\rho)$.

 On the other hand, if the original correlation is exponential such that the correlation decreases as time lags increases, $Cor(Y_{ij},Y_{il}) = \exp(-|t_{ij}-t_{il}|/\rho)$, the resulting covariance after transformation is not a recognizable structure. In fact, the covariance can no longer be written as a function of time lags. The covariance matrix is a linear combination of the original correlation matrix, column and row means, and the overall mean correlation,
   \begin{align*}
 Cov(\B A\B Y_{i}) &= \sigma^{2}\left[\B R_{i}(\rho)-m^{-1}\B1\B1^{T}\B R_{i}(\rho)-m^{-1}\B R_{i}(\rho)\B1\B1^{T} + m^{-2}\B1\B1^{T}\B R_{i}(\rho)\B1\B1^{T}\right]\\
  &= \sigma^{2}\left[\B R_{i}(\rho)-\text{ column mean vector }-\text{ row mean vector } + \text{ overall mean}\right].
 \end{align*} 
 This non-stationary covariance matrix includes negative correlations when the mean of the correlations within each column and within each row are positive and substantial. For example, if $\sigma^{2}=1$, $\rho = 2$ and $\B t_{i}=(1,2,3,4)$, then the symmetric covariance matrix of the transformed vector is
$$ Cov(\B A\B Y_{i}) = \left[ \begin{array}{cccc}
 0.499&  0.009& -0.229& -0.278\\
  0.009&  0.307& -0.087& -0.229\\
 -0.229& -0.087&  0.307&  0.009\\
 -0.278& -0.229&  0.009&  0.499
\end{array}\right]$$
The variance and covariance changes over time with the correlation becoming negative as the time lag increases.  If the number of observation times increase such that the observation period expands, the covariance of the transformed vector will be close to the original covariance as column, row, and overall means will decrease to zero as the number of pairs of measurements with large time lags increases. However, if the observation period remains fixed as the number of observations increases, the covariance after transformation continues to be non-stationary and has negative correlations.  

We have calculated the covariance of the transformed random vector when we know the original covariance structure. For example, we showed that if prior to transformation, the errors are independent or exchangeable, the correlation of the resulting transformed data is exchangeable equal to $\frac{-1}{m-1}$. This particular value has significant meaning as it is the lower bound for correlation in an exchangeable matrix. This means that the true parameter value of the correlation for the transformed vector is on the boundary of the parameter space. Therefore, even though we know the true structure, estimating parameters for the true model is difficult. If $m$ is moderately large, conditional independence may be an adequate approximation to regularize the estimation. 
 
\subsubsection{Random observation times} 
Up until now, we have assumed the observation times are fixed. However, in practice, individuals in a longitudinal study are not observed at exactly the same times but rather at random times. When the times are random, the deterministic function, $\mu(t)$, is evaluated at different times. Therefore, the transformed vector will not only have variability due to the errors but also an induced random intercept from the average function value. 

To illustrate, imagine an error-free outcome vector such that the observations are twice the time of observation, $y_{ij}=2t_{ij}$. Let $t_{1}=(1,2,3)$ and $t_{2}=(1,2.5,4)$ be two sets of observation times. Then we have two realizations of the vector $y_{1}=(2,4,6)$ and $y_{2}=(2,5,8)$. The transformed vectors after removing the means equal $y^{*}_{1j}=2t_{1j}-4$ and  $y^{*}_{2j}=2t_{2j}-5$. Evaluating the function at different sets of observation times and subtracting the mean induces a variability in the intercept, the variance of which depends not only on the variability in the random times but also the derivative of the function. For example, if the function has a steeper slope, $y_{it}=10t_{ij}$, then the transformed vectors equal $y^{*}_{1j} = 10t_{1j}-20$ and $y^{*}_{2j} = 10t_{1j}-25$. This variability needs to be incorporated into the covariance matrix of the model. 

If the original covariance is dependent on the observation times through an exponential function of time lags, the random intercept and error structure both indirectly depend on the times of observation. We explore the impact of transforming the data through empirical simulations. We assume that observation times are equal to random perturbations around specified goal times such that $\B t = \B T + \B \tau$ where $\tau\sim N(0,\sigma^{2}_{\tau})$ and $\B T = (1,2,...,9,10)$. We generate $n=500$ data realizations such that
$$\B Y_{i} = \BS\mu_{i} + \epsilon_{i}\quad\text{ where }\epsilon_{i}\sim N(0,\B R_{i}(\rho))$$
where $\mu_{ij}=\mu(t_{ij})$ and under different assumptions for the mean function $\mu(t)$, and variance of the observation times through $\tau$. Figure \ref{fig:cov1} and Figure \ref{fig:cov} show the estimated autocorrelation functions of the deviations from the mean when $\B R_{i}(\rho)$ is an exchangeable correlation matrix with $\rho=0.5$ and when  $\B R_{i}(\rho)$ is an exponential correlation matrix with range parameter $\rho=2$, respectively, under varying conditions for observations times and shape functions.
\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Cov2}
\end{center}
\caption{Estimated autocorrelation functions of the deviations from the mean from data generated with an exchangeable correlation error structure and perturbed observation times under different mean functions, $\mu(t)$, and variances of the perturbation.}
\label{fig:cov1}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Cov}
\end{center}
\caption{Estimated autocorrelation functions of the deviations from the mean from data generated with an exponential correlation error structure and perturbed observation times under different mean functions, $\mu(t)$, and variances of the perturbation.}
\label{fig:cov}
\end{figure}

As the variance of the perturbation in observation time and the magnitude of the derivative mean function increases, the estimated correlation of the deviations from the mean at large lags becomes more positive. In general, the curves resemble covariance functions for exchangeable random variables or a scaled and shifted version of the exponential function with an asymptote other than zero. These functions with variable times result in structures that are no longer singular. The covariance of the transformed data is still singular, but the covariance of the deviations from the mean is invertible because the mean vector is random due to the random times.

In practice, if the data are regularly or very close to regularly sample, true negative correlations are problematic for estimation and an independence or exponential correlation structure may be the best option. If the data are irregularly sampled such that the times drastically differ, an additive model that combines a random intercept with the exponential correlation \cite{diggle2002} may be appropriately flexible to approximate the covariance of the deviations from the mean of the transformed data.

\subsubsection{Unbalanced observation times}
In addition to the issues of fixed versus random sampling, having an unequal number of observations per subject can impact the estimation of covariance of transformed vector. As we saw above, the length of the vector, $m$, impacts the covariance of the transformed vector. Suppose we have a sample of individuals that have the same mean shape and covariance over time. However, we observe each subject a different number of times because they were unavailable for an interview or two. Transforming the vectors by subtracting means based on a variety of number of observations induces a different covariance structure for each individual. If there is quite a bit of variability in the number of observations, it may impact clustering to assume they share the same covariance structure during the estimation/clustering process. However, if the number of observation times is large for all subjects and the observation period is long, then the covariance matrices should be similar. 

Additionally, if the unbalanced nature of the data is due to lost to follow up during a longitudinal study, clustering based on the shape should be done with caution. If the general shape of the curve during the observation period is not measured adequately by the number of observations, it does not make sense to try and cluster those individuals with the rest who have fully observed curves. 

\section{Discussion}
In this chapter, I have describe three different approaches to the problem of clustering irregularly sampled longitudinal data by shape. The first focuses on clustering derivative functions, which should contain all the information about the shape ignoring the level. The most difficult aspect of this approach is estimating individual derivative functions. Projecting the data onto a B-spline basis should remove noise and provide a smooth underlying function, which should be an improvement over quotient differences, which simply linearly interpolates the data points with no regard to error. One difficulty with this method is choosing the correct basis so as to not over fit the data when there are only a few data points. While this may improve upon other methods, this approach has limitations. Individuals cannot borrow strength in estimating their derivative function even though we hypothesize it to be a common factor. Also, partitioning methods do not lend themselves to analysis of baseline factors since by definition, there is no uncertainty in group membership.  

Rather than trying to ignore the level, the second approach attempts to directly model the variability in the level by assuming that for each shape group, there are subgroups that differ in level. Assuming a multilayered mixture model provides a probability framework to take into account uncertainty while estimating the relationship between baseline factors and group membership. However, this model assumes that level varies discretely rather than continuously, which is a strong assumption. It is not clear how robust this method is to violations of this assumption.

Lastly, the third approach directly removes the level by subtracting individual-specific means prior to modeling. This allows individuals to be compared without making specific assumptions about the distribution of the level while providing the probability framework.  There are two difficulties with this method. First, subtracting an observed mean impacts the covariance in a way that makes it harder to model with a known correlation structure. Second, care needs to be taken when there is sparse and irregularly sampling. 

We compare these methods in practice with a simulation study in the next chapter.
