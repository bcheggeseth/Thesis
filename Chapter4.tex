\documentclass[11pt]{article}
\usepackage{fullpage,amsmath,amsfonts,graphicx,amsthm}
\usepackage{setspace}
\title{Three Proposed Methods}
\author{Brianna C. Heggeseth}

\newtheorem{theorem}{Theorem}
\newcommand{\B}[0]{\mathbf}
\newcommand{\bs}[0]{\boldsymbol}
\newcommand{\Cov}[0]{\text{Cov}}
\newcommand{\Cor}[0]{\text{Cor}}

\begin{document}
\doublespace
\maketitle
\noindent In this chapter, I present three clustering techniques---derivative spline coefficient partitioning, multilayer mixture model, and vertically shifted mixture model---that attempt to answer the three research questions presented in the last chapter: Are there distinct shape patterns in the data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? For each method, I introduce the concept in the context of related work and then introduce necessary background, notation, and the model specification. I describe the implementation process and any foreseen issues and limitations. After presenting the three methods, I discuss them in terms of obvious advantages and disadvantages. In the next chapter, a simulation study compares the proposed methods with those presented earlier in the thesis in addressing the three research questions above.
\section{Derivative Spline Coefficients Partitioning}
The first method uses the technique of calculating the derivative to remove the level from the curve. Unlike the quotient difference method to calculate the derivative, this method attempts to remove noise before calculating the underlying derivative and is not dependent on observed the data at the same time points for all subjects. Inspired by functional data analysis \cite{ramsay2002}, we use project the data onto a functional basis to estimate a smooth curve over time and then differentiate the basis to get an indirect estimate of the derivative function.
\subsection{Related Work}
In Chapter 3, we discuss the quotient difference distance metric suggested by D'Urso and M{\''o}ller-Levet et. al \cite{d2000,moller2003}. Along the same line of reasoning as D'Urso, Zerbe presents three distance measures for growth curves based on position, velocity, and acceleration \cite{zerbe1979,schneiderman1993}. Rather than using quotient differencing, he suggests estimating the each individual's function by fitting a polynomial of degree $D$ using least squares. For individual $i$, we let $\hat{f}(t)_{i} = [1\; t\;t^{2}\;...\;t^{D}]\;\hat{\bs\beta}_{i}$ where 
$$\hat{\bs\beta}_{i} = (\B W^{T}_{i}\B W_{i})^{-1}\B W^{T}_{i}\B y_{i}$$
and $\B W_{i}$ is the within-individual design matrix based on the polynomial vectors specific to individual $i$. Then, the estimated derivative is equal to
$\hat{f}^{'}(t)_{i} = [0\; 1\;2t\;...\;Dt^{D-1}]\;\hat{\bs\beta}_{i}$
 In other words, he projects the data onto a polynomial basis, differentiates the basis and calculates the estimated derivative function, which can be represented by another polynomial basis of degree $D-1$. Rather than using Euclidean distance, Zerbe used a natural dissimilarity measure between two functions which is the squared $L_{2}$ distance \cite{schneiderman1993} thus for the $i$th and $k$th individuals,
$$d_{ik} =\left[ \int_{\mathcal{T}} [\hat{f}^{'}_{i}(t)-\hat{f}^{'}_{k}(t)]^{2}dt\right]^{1/2}$$
for a chosen interval, $\mathcal{T}$.\\\\
Similarly, Tarpey and Kinateder mentioned the use of a Fourier basis to estimate the derivative at the end of their paper \cite{tarpey2003}. By projecting the data onto a finite set of Fourier basis functions, it is easy to represent the derivative function in terms another Fourier basis by differentiating the Fourier basis and rearranging the terms to end up with a new function defined by the original basis. The dissimilarity measure is then defined as the Euclidean distance between the coefficients of the new function. \\\\
Both of these methods smooth the data by projecting the individual data onto a chosen basis and estimating the coefficients and then calculating the derivative function by differentiating the basis functions. They differ in how the distance or dissimilarity measure is defined between two individuals. Zerbe used the squared $L_{2}$ between derivative functions and and Tarpey and Kinateder calculated the Euclidean distance between the coefficient vectors for the derivative function. This reflects the diversity in the functional cluster analysis literature; some use the $L_{2}$ distance based on the functions \cite{hitchcock2007} while others use the linear coefficients of the basis function\cite{serban2005, tarpey2003, abraham2003}. Tarpey demonstrated the difference between these options and how the $L_{2}$ distance can be calculated using the coefficients \cite{tarpey2007}. Additionally, he pointed out that clustering using K-means on the raw data vectors often gives results similar to that from using K-means based on coefficients from an orthogonal basis. Some advantages to using the coefficients rather than the $L_{2}$ distance is the ease of calculation by avoiding an integral and the dimension reduction from the number of observations to the number of basis functions. In general, smoothing the data to first estimate individual functions results in the estimated derivatives less sensitive to high measurement error.\\\\
Both the polynomial and Fourier bases are restrictive and therefore biased when the data are not periodic or more complex than a lower order polynomial and there are limited data points for each individual. Another popular basis is the class of B-splines \cite{deboor1978, schumaker1981}, which extend the advantages of polynomials to include greater flexibility \cite{abraham2003}. To the author's knowledge, no other study have focused on clustering longitudinal data using the coefficients of the B-spline derivative estimate. \\\\
\subsection{B-spline background}
We fit a m-order B-spline function to each subject $i$ in order to estimate $f_i$. For the sake of being self-contained, we include some background on B-splines. Let $t\in[a,b]$ where $a,b\in\mathbb{R}$ and $\xi_0=a<\xi_{1}<\cdots<\xi_{L} < b = \xi_{L+1}$ be a subdivision of  the interval $[a,b]$ by $L$ distinct points, known as internal knots. We now define the augmented knot sequence, $\tau=[\tau_{1},...,\tau_{L+2m}]$ for $m\in\mathbb{N}$, such that 
\begin{align*}
\tau_{1}&=\tau_{2}=\cdots =\tau_{m} =\xi_{0}\\
\tau_{j+m}& = \xi_{j}, \quad\quad j=1,...,L\\
\xi_{L+1}&=\tau_{L+m+1}=\tau_{L+m+2}=\cdots =\tau_{L+2m} 
\end{align*}
The spline function, $s(t)$, is a polynomial of order $m$ on every interval $[\tau_{j-1},\tau_{j}]$ and has $m-2$ continuous derivatives on $(a,b)$. The set of splines of order $m$ for a fixed sequence of knots, $\tau = [\tau_1,...,\tau_{L+2m}]$, is a linear space of functions with $L+m$ free parameters. A useful basis $B_{1,m}(t),...,B_{L+m,m}(t)$ for this linear space is given by Schoenbergs' B-splines \cite{curry1966, de1976} defined as
\begin{align*}
B_{j,1}(t) &= \begin{cases}
1 \text{ if }\tau_j\leq t < \tau_{j+1}\\
0\text{ otherwise}
\end{cases}\\
B_{j,l}(t) &= \frac{t-\tau_j}{\tau_{j+l-1}-\tau_j} B_{j,l-1}(t)+\frac{\tau_{j+l}-t}{\tau_{j+l}-\tau_{j+1}} B_{j+1,l-1}(t)
\end{align*}
where $l=2,...,m$ and $j=1,...,L+2m-l$.  If we adopt the convention that $B_{j,1}=0$ if $\tau_{j}=\tau_{j+1}$, then by induction $B_{j,l}=0$ if $\tau_{j}=\tau_{j+1}=\cdots=\tau_{j+l}$. Hence, $B_{1,l}=0$ for $l<m$ on the defined knot sequence. The B-spline function of order $m$ is defined by
$$s(t) = \sum^{L+m}_{j=1} P_j B_{j,m}(t).$$
In practice, we fix $m$ and for each subject $i$, estimate the coefficients, $P^{(i)}_1,...,P^{(i)}_{L+m}$ using Least Squares  and $\hat{f}_i(t)=\sum^{L+m}_{j=1} \hat{P}^{(i)}_j B_{j,m}(t)$. \\
To estimate $f_i'(t)$, we differentiate $\hat{f}_i(t)$ with respect to $t$ and we get
$$\hat{f}'_i(t)=\sum^{L+m}_{j=1} \hat{P}^{(i)}_j B'_{j,m}(t)$$
Prochazkova \cite{prochazkova2005} showed that this can be simplified to
$$\hat{f}'_i(t)=\sum^{L+m}_{j=2} \hat{P}^{(i)}_j [\frac{m-1}{\tau_{j+m-1}-\tau_j} B_{j,m-1}(t)-\frac{m-1}{\tau_{j+m}-\tau_{j+1}} B_{j+1,m-1}(t)].$$
However, we can go one step further and write this in terms of a B-spline basis of one order lower.
\begin{align*}
\hat{f}'_i(t)&=\sum^{L+m-1}_{j=1} (\hat{P}^{(i)}_{j+1} -\hat{P}^{(i)}_j)\frac{m-1}{\tau_{j+m}-\tau_{j+1} }B_{j+1,m-1}(t)
\end{align*}
If we adjust the knot sequence to only have $m-1$ replicates at the beginning and end, then $\hat{f}'_{i}(t)$ can be written as
$$\hat{f}'_i(t)=\sum^{L+m-1}_{j=1}\hat{Q}^{(i)}_jB_{j,m-1}(t)$$
where $\hat{Q}^{(i)}_j= (\hat{P}^{(i)}_{j+1} -\hat{P}^{(i)}_j)\frac{m-1}{\tau_{j+m}-\tau_{j+1} }$.
We use these new coefficients, $\hat{\B Q}^{(i)} = [\hat{Q}^{(i)}_1,...,\hat{Q}^{(i)}_{ L+m-1}]$, for $i=1,...,n$, to cluster trajectories with similar shape, in terms of the estimated derivative using K-means \cite{macqueen1967, hartigan1979}. The K-means algorithm searches for $K$ vectors $\B a=\{\B a_{1}, ...,\B a_{K}\}$ that minimize the objective function
$$\frac{1}{n} \sum^{n}_{i=1}\min_{1\leq k\leq K}||\hat{\B  Q}^{(i)}-\B a_{k}||^{2}$$
\subsection{Implementation}
To use this method in practice, decisions about the B-spline basis need to be made. First, the order of the polynomials must be chosen. This together with the number of internal knots impacts the flexibility of the B-spline function. Cubic B-splines of order four have been shown to have good mathematical properties and are used frequently in practice \cite{james2003}. However, depending on the number of data points per subject, it may be necessary to use a quadratic polynomial ($m = 3$) due to the restriction that $L+m$ must be less than or equal to the minimum number of data points per subject. \\\\
As mentioned, the number of internal knots, $L$, also plays a role in shaping the class of spline functions. In deciding on L, the main limiting factor will be the number of data points per subject, but if there are many repeated measures, information criteria model selection or cross-validation can be used to choose $L$ \cite{rice2001}. \\\\
The location of the internal knots is another issue of discussion. There are some suggested data-driven ways to select knot location \cite{shanggang2001}, but Ruppert \cite{ruppert2002} supports fixing the knots at sample quantiles given the number of knots. Once the order of the polynomials and number and location of knots are chosen, then smooth functions and their derivatives are estimated for every subject. \\\\
The coefficients from the estimated derivative functions are used to cluster individuals. The K-means algorithm is a standard clustering algorithm that given a fixed number of clusters, minimizes an objective function mentioned above after initially randomizing vectors to a groups. This algorithm will converge but is not guaranteed to find the grouping that globally minimizes the objective function. Therefore, in practice, the algorithm needs to run multiple times with different random initializations and the clustering that minimizes the objective function is chosen.\\\\
However, in order to run K-means, you need to fix the number of clusters, $K$, which is usually unknown and of interest to researchers. While no perfect mathematical criterion exists, a number of heuristics (see \cite{tibshirani2001} and discussion therein) are available for choosing $K$. We choose the $K$ that maximizes the overall average silhouette width \cite{rousseeuw1987}.  For each subject $i$, the silhouette width, $s_{i}$, is calculated by
$$s_{i}=\frac{b_{i}-a_{i}}{\max\{a_{i},b_{i}\}}$$
where $a_{i}$ is the average dissimilarity of $i$ to all other trajectories in its cluster, $A$, $d_{i}(C)$ is the average dissimilarity of $i$ to all trajectories of cluster $C$ and $b_{i}$ is equal to $\min_{C\not= A} d_{i}(C)$. The overall average silhouette width, $\bar{s}$, is the average of $s_{i}$ over all subjects in the whole data set. By maximizing the overall average silhouette width, you are maximizing $b_{i}$, the dissimilarity between clusters and minimizing $a_{i}$, the dissimilarity within clusters. Thus, we choose $K$ that best separates the data so that we have heterogenous group of homogenous clusters.\\\\
K-means is a partitioning algorithm, therefore,  every subject is `hard' clustered into only one of the groups. There is no stated uncertainty in the group memberships even if a subject is on the edge between two clusters. In order to estimate the relationship between baseline variables and group membership, we use ad hoc a posterior method. Given the subject grouping labels from the partition, $\{W_{i}\}$ such that $W_{i}\in\{1,2,...,K\}$ for all $i=1,...n$, we fit a multinomial logit regression model, which is an extension of the logistic regression model, using the group labels as the outcome and baseline variables, $\B z_{i}$, as explanatory variables such that
$$P(W_{i} = k) = \frac{\exp(\B z_{i}^{T}\bs\gamma_{k})}{\sum_{j=1}^{K}\exp(\B z_{i}^{T}\bs\gamma_{j})}$$
for all $k=1,...,K$ and $i=1,...,n$ with $\bs\gamma_{K}=0$. We then estimate the parameters $\bs\gamma_{1},...,\bs\gamma_{K-1}$ using maximum likelihood estimation. However, the estimated standard errors do not include any of the potential uncertainty in group membership as we assumed the hard clustering labels were known during the estimation process.

\section{Multilayer model}
\subsection{Related Work}
Model-based methods provides the probability framework that dissimilarity-based methods lack. One of the main benefits is the ability to take into account uncertainty---the uncertainty of the mean estimates as to make inferences of a larger population and the uncertainty of cluster membership when estimating relationships between baseline variables and clusters. As mentioned before, the main model-based method is the finite mixture model. However, these methods as they are do not distinguish between the level and shape and will cluster subjects based on the dominant source of variability. Another limitation Gaussian mixtures for grouping data results from the potential non-normality of cluster densities. For these reasons, the mixture model has been extended into multilayer mixture models, which are a generalization of finite mixture models in that each component is allowed to be a mixture itself \cite{li2005} (see Figure \ref{fig:dia} for diagram). Researchers have used a similar idea in clustering by combining cluster components after fitting the model to make more meaningful clusters \cite{hennig2010}. \\\\
\begin{figure}[h]
\begin{center}
\includegraphics[width=4in]{Chp4multilayer_diagram}
\end{center}
\label{fig:dia}
\caption{Stuff}
\end{figure}
In our case, we want the clusters to be meaningful in terms of distinguishing between shape. Therefore, we can use this generalization in order to model $K$ non-normal shape clusters comprised of mixture components with the same mean shape at different levels.\\\\

\subsection{Multilayer background}
In a multilayer mixture model, let $K$ be the number of clusters and $J_{k}$ be the number of components in cluster $k=1,...,K$ such that $J=\sum_{k=1}^{K}J_{k}$ is the total number of components in the entire model. Let $j$ index all of the components such that $j=1,...,J$. For ease of explanation, we define a cluster assigning function, $c(j):\{1,...,J\}\rightarrow \{1,2,...,K\}$, to specify the cluster to which a component belongs. Let $f(y|x,\mu,\Sigma)$ be the probability density function of a multivariate normal distribution with mean $\mu$ and covariance $\Sigma$. Then the mixture probability density function for cluster $k$ is
$$f_{k}(\B y|\B x) = \sum_{j: c(j) = k} \pi_{j|c(j)}f(\B y|\B x,\mu_{j},\Sigma_{j})$$
where $\pi_{j|c(j)}$ is the probability of being in component $j$ given a sample is in cluster $c(j)$ and $\sum_{j: c(j) = k}\pi_{j|c(j)}=1$ for all $k=1,2,...,K$. Let the probability of cluster $k$ given baseline variables, $\B z$, be $\pi_{k}(\B z,\bs\gamma)=\exp(\B z^{T}\bs\gamma_{k})/\sum^{K}_{l=1}\exp(\B z^{T}\bs\gamma_{l})$ such that $\bs\gamma_{K}=0$ and $\gamma=(\gamma_{1},...,\gamma_{K})$. Then the density for the multilayer mixture can be written as
$$g(\B y|\B x,\B z) = \sum_{k=1}^{K}\pi_{k}(\B z,\bs \gamma)f_{k}(\B y| \B x) = \sum_{k=1}^{K}\pi_{k}(\B z,\bs\gamma)\sum_{j: c(j) = k} \pi_{j|c(j)}f( \B y| \B x,\mu_{j},\Sigma_{j})$$
Since every component belongs to only one shape cluster, the above equation reduces to a regular mixture model if we let $\bar{\pi}_{j}(\B z,\bs \gamma)=\pi_{c(j)}(\B z,\bs\gamma)\pi_{j|c(j)}$,
$$g(\B y|\B x,\B z) = \sum_{j=1}^{J}\bar{\pi}_{j}(\B z,\gamma)f(\B y|\B x,\alpha_{j},\bs\beta_{c(j)},\sigma^{2}_{j})$$
In order to estimate the parameters, we maximize the classification likelihood using a modified Expectation Maximization (EM) algorithm \cite{dempster1977} called the Classification EM algorithm \cite{mclachlan2000}. The details of the iterative algorithm can be found in Ji Lia \cite{li2005}, but the main modification involves hard clustering clusters while soft clustering the components through the iterative estimation process.\\\\
In order to accomodate our goals, we include regression parameterization for the mean such that we restrict the shape parameters to be the same within clusters, $\mu_{j}=\alpha_{j}+\B x \beta_{c(j)}$, where $\B x$ does not include an intercept term. This will allow the clusters to be based on shape while the subcomponents can have different levels (see figure \ref{fig:diashape} for diagram). Additionally, we assume $\Sigma_{j}=\sigma^{2}_{j}I$ for all $j=1,...,J$  for simplicity to allow for irregularly sampled data.
\begin{figure}[h]
\begin{center}
\includegraphics[width=4in]{Chp4multilayer_diagram_shape}
\end{center}
\label{fig:diashape}
\caption{Stuff}
\end{figure}
\subsection{Implementation}
Denote the shape cluster identity of sample $i$ by $\eta(i)$ where $\eta(i)\in\{1,...,K\}$. Let $\theta=\{\bs \gamma_{k}, \pi_{j|c(j)},\alpha_{j}, \bs\beta_{k},\sigma^{2}_{j};\; j=1,...,J, k=1,...,K\}$ and $\eta = \{\eta(i); i=1,...,n\}$ be the parameters for this model. In order to estimate the parameters in this multilayer mixture model, we used classification maximum likelihood \ref{mclachlan2000} such that we maximize the function
\begin{align}
\label{ll} L(\theta,\eta) = \sum^{n}_{i=1} \log \pi_{\eta(i)}(\B z_{i},\bs\gamma) f_{\eta(i)}(\B y_{i}|\B x_{i}) =  \sum^{n}_{i=1} \log\left[ \pi_{\eta(i)}(\B z_{i},\bs\gamma)  \sum_{j: c(j) = \eta(i)} \pi_{j|c(j)}f(\B y_{i}|\B x_{i},\alpha_{j},\bs\beta_{\eta(i)},\sigma^{2}_{j})\right]
\end{align} 
To maximize this log-likelihood, we use a modified EM algorithm called classification EM (CEM) \ref{celeux1992;mclachlan2000}. The modification includes a classification step between the E-step and M-step where individuals are grouped into shape clusters. Assume at iteration $t$, $\theta^{(t)}$ and $\eta^{(t)}$ are the current estimates of the parameters. The algorithm updates these estimates as follows:
\begin{enumerate}
\item E-step: Compute posterior probabilities, $p_{i,k}$, such that
$$p_{i,k}\propto \pi_{k}(\B z_{i},\B\gamma)f_{k}(\B y_{i},\B x_{i})$$
\item Classification: Hard classify subjects to shape clusters according to $\eta^{(t+1)}(i) = \arg\max_{k} p_{i,k}$.
\item M-step: For each shape cluster $k$, use maximum likelihood estimation to update parameter vector $\theta$ by embedding an EM procedure initialized by the current parameter values.
\end{enumerate} 
This algorithm increases the log-likelihood at each iteration and the statement is below. The proof is in the appendix. 
\begin{theorem} The classiÞcation likelihood $L(\theta,\eta)$ defined in (\ref{ll}) is non-decreasing after each update of $\eta$ and $\theta$ by the CEM algorithm. That is, $L(\theta^{(t+1)},\eta^{(t+1)}) \geq L(\theta^{(t)},\eta^{(t)})$ for all $t$.
\end{theorem}
Within the algorithm, you need to maximize the likelihood for each shape cluster to estimate parameters. Since we have constrained the shape parameters, we used computational methods suggested by Grun (Flexmix 2 citation). We first set the outcome vector for subject $i$ as a vector of $\B y_{i}$ repeated $J_{k}$ times, use design matrix $(I_{J_{k}}\otimes \bs 1_{m_{i}}, \bs 1_{J_{k}} \otimes \B x_{i})$ and a block diagonal covariance matrix with $I\sigma^{2}_{j}$ in each block such that $c(j)=k$. Assuming a multivariate Gaussian distribution and using profile likelihoods, we maximized the likelihood function with respect to $\sigma_{j}$ using an optimization routine.\\\\
Now, as in the derivative coefficient method, we want to allow the mean structure to be flexible enough to accommodate shapes other than lower ordered polynomials. So we use a B-spline basis as described in the early section. The same issues of the number and location of internal knots are dealt with in the same manner.\\\\
With this more complex mixture structure, it is more difficult to select $K$ as well as $J_{k}$ for $k=1,..,K$. One actually must fix $K$ and then use information criteria to select $J_{k}$ for each $k=1,...,K$. As suggested by Jia Li, we use BIC even though the regularity conditions do not hold as they have been shown to be useful informal guides in practice (or BIC-ICL).\\\\
Standard error derivations were done using the regular mixture form and following the same procedure as Boldea and Magnuson (cite). This will be available elsewhere (online).

\section{Vertical Shifting Mixture Model}
The goal of cluster analysis is to partition a collection of individuals into homogeneous groups. In this thesis, we define similar individuals as those with the same outcome pattern or shape over time. We aim to develop an effective clustering method that focuses on grouping longitudinal data with a special focus on shape similarities. If two curves only differ in intercepts by vertical shifts, they are placed in the same group. In this method, we consider vertically shifted trajectories which result from subtracting the outcome level. Each cluster comprises a density with a mean shape curve and a covariance function. The assumed finite mixture of densities is fit to the vertically shifted data to estimate parameters and group membership probabilities. \\\\
A mixture model is a standard method for clustering multivariate data \cite{everitt2009} and have been used for longitudinal applications \cite{muthen2010, jones2001}. However, for longitudinal data, the models are commonly used for the observed data without much regard to the real goal of clustering by shape which is implicitly stated in the discussion of the results. We suggest subtracting out the outcome level before modeling. Subtracting the level is not a novel idea in statistics or even cluster analysis. In most multivariate clustering applications, it is recommended that each variable is standardized by subtracting the mean of the variable measures and dividing by the standard deviation so that each standardized variable is in comparable units and equally contribute to the clustering process. In the longitudinal setting, each ``variable'' is a repeated measurement at a different time point; therefore, the variables are all in the same units. However, we suggest removing a subject-specific level rather than completing variable-specific centering. This seems like an obvious pre-processing step; however, it is rarely in practice and there are some consequences that need to be considered when modeling the transformed data as discussed later in the chapter. \\\\
A version of this idea has been implemented in the functional data analysis literature. For process in a Hilbert space of square integrable functions with respect to the Lebesgue measure $dt$ on $\mathcal{T}=[0,T]$, Chiou and Li \cite{chiou2008} propose using a mixture model and the Karhunen-Lo{\`e}ve expansion for centered stochastic processes within their correlation-based clustering algorithm. To center the processes, they subtract the integral of process over $\mathcal{T}$ divided by $T$, the length of the interval. The resulting process integrates to zero. This integral of the process is the functional analogue to a mean or average in finite vector space and similarly, the resulting vector after subtraction has mean zero. Although the centering process stems from the same idea, there are distinct consequences of subtracting the estimated level of a noisy curve observed at a finite number of points that don't arise when centering a known function. The term centering is used in the stochastic processes literature, but we use the term vertically shifting rather than centering since it graphically describes the transformation of the noisy longitudinal data.\\\\
In this chapter, we outline the model notation and specification for vertically shifted mixture models including the mean and covariance structure. Then we describe the implementation process of choosing a basis, the number of clusters, and estimating the parameters of the model. As mentioned there are some considerations and issues that we discuss in terms of the impact of transforming the data on the modeling process and final clustering results. Finally, we provide advantages and disadvantages of this method in practice.

\subsection{Model Specification}
 Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote a vector of repeated observations for individual $i$ for $i=1,...,n$. As longitudinal data is collected over a period of time, the vector of corresponding times of observation for individual $i$ is denoted as $\B t_{i}=(t_{i1},...,t_{im_{i}})$, $\B x_{i}$ is a matrix of variables based on the vector of observation times,  and $\B z_{i}$ is a vector of time-fixed covariates that are typically collect at or before time $t_{i1}$. Let $\bar{y}_{i}= m_{i}^{-1}\sum^{m_{i}}_{j=1} y_{ij}$ be the mean of the observed outcomes for individual $i$. This is a measure of the vertical level of a curve and to remove the level of the observations and leave the pattern over time, we apply a linear transform to the vector of observations and let
\begin{align*}
\B y^{*}_{i} = \B A_{i}\B y_{i}\quad\text{ where }\quad \B A_{i} =\B I_{m_{i}} - m_{i}^{-1}\B 1_{m_{i}}\B 1_{m_{i}}^{T}
\end{align*}
where $\B 1_{m}$ is a m-length vector of 1's and $\B I_{m}$ is an $m\times m$ identity matrix. Multiplying the matrix $\B A_{i}$ to the vector $\B y_{i}$ subtracts the individual mean, $\bar{y}_{i}$, from each element $\B y_{i}$. The resulting vector, $\B y^{*}_{i}$, is in units relative to the mean, $\bar{y}_{i}$, and is assumed to arise from one of $K$ shape groups characterized by probability densities that are centered around mean patterns with dependence summarized by covariance matrices for $i=1,...,n$. We assume that, conditional on $\B x$ and $\B z$, $\B y^{*}$ is a realization from a finite multivariate mixture model with density
\begin{align*}
 f(\B y^{*}|\B x,\B z,\bs\theta,K) =  \sum^{K}_{k=1}\pi_{k}(\B z_{i}, \bs\gamma)f_{k}( \B y^{*}|\B x_{i},\bs\theta_{k})\label{mixmodel}
\end{align*}
where $\pi_{k}(\B z_{i},\gamma)>0$ for $k=1,...,K$ and $\sum^{K}_{k=1}\pi_{k}(\B z_{i},\gamma)=1$ and $\bs\theta = (\bs\gamma,\bs\theta_{1},...,\bs\theta_{K})$. To allow baseline covariates to affect the probability of having a certain shape pattern over time, we parameterize the mixing proportions using the multinomial logit with the form
$$\pi_{k}(\B z_{i},\bs\gamma)=\frac{\exp(\B z_{i}^{T}\bs\gamma_{k})}{\sum_{j=1}^{K}\exp(\B z_{i}^{T}\bs\gamma_{j})}$$ 
for $k=1,...,K$ where $\bs \gamma_{k}\in\mathbb{R}^{q}$, $\bs\gamma = (\bs\gamma_{1},...,\bs\gamma_{K})$, and we fix $\bs\gamma_{K}=\B 0$. We assume the component densities $f_{k}(\B y^{*}|\B t_{i},\bs\theta_{k})$ are multivariate Gaussian with mean $\bs\mu_{k}(\B t_{i})$ and covariance matrix $\Sigma_{k}(\B t_{i})$ both based on the observation times and the vector of parameters $\bs\theta_{k}$.
\subsubsection{Mean Structure}
In the longitudinal setting, the main interest is in the change over time; therefore, the mean shape is a function of time. To parameterize the mean pattern in the mixture model, we let $\B x_{i}$ be a matrix of values from a chosen functional basis taken at observation times $\B t_{i}$. It is common to use a quadratic or cubic basis by letting $\B x_{i} = (\B 1_{m_{i}}^{T},\B t_{i}^{T}, (\B t^{2}_{i})^{T})$. However, this limits the ability of the model to capture the complexity of a mean structure. We use the class of B-splines basis functions \cite{deboor1978, schumaker1981,curry1966, de1976}, which extend the advantages of polynomials to include greater flexibility. The B-spline function of order $m$ is defined by a linear combination of coefficients and basis functions
$$s(t) = \sum^{L+m}_{j=1} P_j B_{j,m}(t)$$
where the basis functions, $B_{j,m}(t)$, are defined in Chapter 3. Therefore, we assume the mean pattern for the $k$th shape cluster is approximated by the function $s_{k}(t)=\sum^{L+m}_{j=1} P^{(k)}_j B_{j,m}(t)$.  Since we only observe measurements at discrete values, we let the covariate matrix include $L+m$ variables corresponding to the basis functions evaluated at the measurement times.  For each individual $i$ observed at times $t_{i1},...,t_{im_{i}}$, we let $$\B x_{i} = \left(\begin{array}{cccc} 
B_{1,m}(t_{i1})&B_{2,m}(t_{i1})&\cdots&B_{L+m,m}(t_{i1})\\
B_{1,m}(t_{i2})&B_{2,m}(t_{i2})&\cdots&B_{L+m,m}(t_{i2})\\
\vdots&\vdots&\ddots&\vdots\\
B_{1,m}(t_{im_{i}})&B_{2,m}(t_{im_{i}})&\cdots&B_{L+m,m}(t_{im_{i}})\end{array}\right)$$
and $\bs\beta_{k} = (P^{(k)}_{1},...,P^{(k)}_{L+m})$ so that the vector-based mean for individual $i$ is $\bs\mu_{k}(\B t_{i}) = \B x_{i}\bs\beta_{k}$.
\subsubsection{Covariance Structure}
Various assumptions can be made about the covariance matrix, $\B \Sigma_{k}(\B t_{i})$. Since it is common for longitudinal data to have sparse, irregular time sampling, we must impose some structure on the covariance matrix to allow for parameter estimation as described by Jennrich and Schluchter in their seminal paper \cite{jennrich1986}. A common parameterization is conditional independence with constant variance where $\B \Sigma_{k}(\B t_{i}) = \sigma_{k}^{2}I_{m_{i}}$. This is an unrealistic assumption for longitudinal data since there is inherent dependence between repeated measures on the same unit. Compound symmetry, which is also known as exchangeable correlation, is a popular correlation structure in longitudinal analysis where all repeated measures are equally correlated with strength $\rho_{k}$ and $\B \Sigma_{k}(\B t_{i}) = \sigma_{k}^{2}(\rho_{k}\B1_{m_{i}}\B1_{m_{i}}^{T}+(1-\rho_{k})I_{m_{i}}).$ It is typically paired with with constant variance as well. This dependence structure is not frequently observed in practice but approximates the correlation better than the independence model. The goal of having a better, but perhaps not perfect  approximation is mostly used when the estimation of mean parameters is robust to misspecifying the covariance structure, which is not the case with mixture models as evidenced in Chapter 2. \\\\
A more general structure that provides a compromise between the two is the exponential correlation structure in which the dependence decreases as the time between observations increases---$\B \Sigma_{k}(\B t_{i})_{jl} = \sigma_{k}^{2}\exp(-| t_{ij}-t_{il}| / r_{k})$ where $r_{k}\geq 0$. This structure is similar to the correlation matrix generated from an autoregressive model of order one where $\B\Sigma_{k}(\B t_{i})_{jl} = \sigma^{2}\rho_{k}^{|t_{ij}-t_{il}|}$ where $\rho_{k}$ is the correlation for measurements observed one unit of time apart and thus $-1\leq \rho_{k}\leq 1$.  If we set $\rho_{k} = \exp{-1/r_{k}}$, then $0\leq\rho_{k}\leq1$ and the two parameterization result in the same structure if the correlation between two measures is constrained to be positive. This is reasonable assumption for longitudinal data in the original form but many not be for the transformed data as we will discuss later in the chapter. DAMPED EXPONENTIAL? The covariation structures mentioned above are describe stationary processes in that they have constant variance and the correlation only depends on the time lag between observations. NON-STATIONARY MODELS WITH NON CONSTANT VARIANCE or CORRELATION NOT A FUNCTION?\\\\
As discovered in Chapter 2, the assumed covariance matrix can highly impact the results in terms of parameter estimates and the final clustering if the groups are not well-separated. A measure such as the RJ criteria defined in Chapter 2 can be useful to choose a covariance matrix structure in practice. However, these principals apply to longitudinal data in its original form. Once the mean is estimated and subtracted from the data, the resulting covariance structure of the transformed data may behave drastically different. Further discussion about the covariance of vertically shifted data will occur later in the paper.
\subsection{Implementation}
Given a collection of independent vectors or repeated observations $\B y_{1},...,\B y_{n}$, the first step of implementing this method is to calculate the mean for each subject, $\bar{y}_{i}$, $i=1,...,n$ and subtract the subject-specific mean from the observed outcome vector for each subject. This transformation leaves the updated independent vectors $\B y^{*}_{1},...,\B y^{*}_{n}$ Secondly, the order of the spline and the number and location of internal knots for the mean structure need to be decided prior to estimation. \\\\
The spline basis is kept constant for all components so the simplest way to select parameters is through visual inspection of the full data set. The idea to keep in mind  when making knot placement via visual inspection is that more knots are needed where the mean trend changes more rapidly. If the most complex shape patterns appear quadratic or cubic, no internal knots are necessary. However, if the most complex curve is more varied, adding knots and increasing the order of the polynomials can flexibly accommodate the twists and turns. In choosing both the order and number of knots, it is important to balance the number of mean parameters with the sample size. Every increase in the order or in the number of knots increases the number of parameters by $K$, the number of components. In terms of location of the knots, it has been suggested to place knots at quantiles based on the sampling times of all the observations (citations from chapter 3). However, if you are taking all of the observation times into account, this strategy will not work well if the median time is not the point at which the curve deviates from a typical polynomial. If possible, it is best to place knots at inflection points of the overall trends to as to accommodate the differences from a polynomial function \cite{embank1999}.  Once these are decided, then the matrixes $\B x_{i}$ as defined above can be calculated using available algorithms that calculated the values from B-spline functions. \\\\
Thirdly, the model above assumes the number of clusters, $K$, is known. In practice, this is not the case and we must choose $K$. The most popular procedure is to chose a maximum value of $K$ such that $K<n$, fit the model under all values of $K=2,...,K_{max}$, and choose the value that optimizes the chosen criteria. In this thesis, we use the Bayesian Information Criterion (BIC) \cite{schwarz1978} when maximizing the likelihood function. It is deÞned as
$$BIC = -2\log L(\hat{\bs\theta},K)- d\log(n)$$
where $d$ is the length of $\bs\theta$, the number of parameters in the mixture model, and $L(\bs\theta,K)$ is the log likelihood function for the parameter vector.The BIC has been widely used for mixture model ever since they were used by Roeder and Wasserman \cite{roeder1997} and in particular, they have been used for clustering \cite{dasgupta1999,fraley1999} with good results in practice. For regular models, the BIC is derived as an approximation to twice the log integrated likelihood using the Laplace method
\cite{tierney1986}, but the necessary regularity conditions do not hold for mixture
models in general \cite{aitkin1985}. However, Roeder and Wasserman \cite{roeder1997} showed
that BIC leads to to a consistent estimator of the mixture density, and Keribin \cite{keribin2000} showed
that BIC is consistent for choosing the number of components in a mixture model.\\\\
In order to fit the model and estimate the parameters, we use maximum likelihood estimation via the EM algorithm. Under the assumption that $\B y^{*}_{1},...,\B y^{*}_{n}$ are independent realizations from the mixture distribution, $f(\B y^{*} | \B x, \B z, \bs\theta,K)$, defined in \ref{mixdens}, the log likelihood function for the parameter vector, $\bs \theta$ is given by
$$\log L(\bs\theta,K)=\sum^{n}_{i=1}\log f(\B y^{*}_{i}|\B x_{i},\B z_{i},\bs \theta,K).$$
The ML estimate of $\bs\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\bs\theta)/\partial \bs\theta=\B 0.$
Solutions of this equation corresponding to local maxima can be found iteratively through the Expectation-Maximization (EM) algorithm \cite{dempster1977}. This algorithm is applied in the framework where given $(\B t_{i}, \B x_{i},\B z_{i})$ each $\B y^{*}_{i}$ is assumed to have stemmed from one of the components and the indicator denoting its originating component is missing. The complete-data log likelihood is based on these indicator variables as well as the observed data $\{(\B y^{*}_{i}, \B t_{i}, \B x_{i}, \B z_{i})\}$. The Expectation step (E-step) involves replacing the indicators by current values of the conditional expectation, which is the posterior probability of component membership, written as
$$\alpha_{ik}=\pi_{k}(\B z_{i},\bs\gamma)f_{k}(\B y^{*}_{i}|\B x_{i})/\sum_{j=1}^{K}\pi_{j}(\B z_{i},\bs\gamma)f_{j}(\B y^{*}_{i}|\B x_{i},\bs \theta_{j})$$
for $i=1,...,n$ and $k=1,...,K$ using current estimates of the parameters. In the Maximization step (M-step), the parameter estimates for the mixing proportions, regression effects, and covariance matrices are updated by maximizing the complete-data log likelihood using the posterior probabilities from the E-step in place of the indicator variables using numerical optimization. The E- and M-steps are alternated repeatedly until convergence. The EM algorithm guarantees convergence to a local maximum; global convergence may be attained through initializing the algorithm by randomly assigning individuals to initial components, running the algorithm multiple times and using the estimates associated with the highest log likelihood. Besides the parameter estimates, the algorithm returns the final posterior probability estimates of component membership. These probabilities can be used to partition individuals into distinct clusters such as K-means or PAM by selecting the cluster with the maximum posterior probability. However, unlike K-means, the posterior probability provides some measure of uncertainty in the hard clustering. \\\\

\subsection{Issues}
Many issues with mixture models as stated in Chapter 2. In this section, we will discuss some unique consequences of vertically shifting the data on the model and inference.
\subsubsection{Covariance of Transformed Data}
Let $Y=(Y_{1},...,Y_{m})$ be a random vector of length $m$ with mean zero and covariance, $\Cov(Y) = V^{1/2}R(\alpha)V^{1/2}$ where $R(\alpha)$ is a correlation matrix based on the parameter $\alpha$ and potentially the associated observation times $(t_{1},...,t_{m})$ and $V$ is a matrix with variance parameters along the diagonal. If we linearly transform the data to subtract the mean of the elements according to the matrix $A$ above, the covariance of the resulting random vector is
$$\Cov(AY) = A\Cov(Y)A^{T}$$
by the properties of covariance. First, let's assume that $V=\sigma^{2}I$ such that the variance is not time-dependent. Let $R(\alpha)=I$ and then $\Cov(Y) = \sigma^{2}I$ and
\begin{align*} 
\Cov(AY) &= \sigma^{2}A \\
&= \sigma^{2}(I - m^{-1}\B1_{m}\B1_{m}^{T})\\
&=\sigma^{2}\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)I)
\end{align*}
 where $a=\frac{-1}{m-1}$ since A is an idempotent matrix. Therefore, if the data has independent errors, by subtracting them estimated mean, you induce constant negative correlation between the observations of magnitude $\frac{-1}{m-1}$ and a decrease in variance. If you start with compound symmetry with  $R(\alpha) = \alpha\B 1_{m} \B 1_{m} + (1-\alpha) I$, then 
 \begin{align*}
 \Cov(AY) &= \sigma^{2}(1-\alpha)(I-m^{-1}\B1_{m}\B1_{m}^{T})\\
 &=\sigma^{2}(1-\alpha)\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)I)
 \end{align*} 
 where $a=\frac{-1}{m-1}$ as before then it ends up maintaining its compound symmetry structure with negative correlations on the off diagonal and decreases variance. Therefore, if the dependence in the original vector is either independence or compound symmetry, then the transformed vector has a covariance matrix with compound symmetry, but smaller variance and negative correlation. On the other hand, if the original correlation is exponential with $\Cor(Y_{1},Y_{2}) = \exp(-|t_{2}-t_{1}|/\alpha)$, the resulting covariance function after transformation is no longer exponential. The covariance structure is a linear combination of the original correlation matrix, column and row means, and the overall mean correlation:
   \begin{align*}
 \Cov(AY) &= \sigma^{2}\left[R(\alpha)-m^{-1}\B1_{m}\B1_{m}^{T}R(\alpha)-m^{-1}R(\alpha)\B1_{m}\B1_{m}^{T} + m^{-2}\B1_{m}\B1_{m}^{T}R(\alpha)\B1_{m}\B1_{m}^{T}\right]
 \end{align*} 
 This results in a non-stationary covariance matrix since the correlation is no longer just a function of the difference in observation times (Figure \ref{fig:exp}). Besides inducing non-stationarity in terms of the correlation, the transformation also results in negative correlations. The column and row mean values in the sum have a negative sign and will have a large magnitude if $|t_{m} - t_{1}|/\alpha$ is small. That means that the smallest correlation values are more negative when the observation period is small or if the range, $\alpha$, is large relative to one another. On the other hand, if $|t_{m} - t_{1}|/\alpha$ is large, then the curve could be approximated by a compound symmetric matrix, which allows for negative correlations but doesn't account for the decay of correlation for small lags or an exponential correlation that takes the shape into account but does not allow for negative correlations. However, if the ratio is small, there are no common parametric functions that  would fit the structure well and blindly using compound symmetry or even exponential correlation structure may be so far from the truth that it impacts the clustering. As seen in Chapter 2, if the components are well separated, then it may not have a huge impact. SEPARATION FOR SHAPE TRAJECTORIES.
\begin{figure}
\begin{center}
\includegraphics[width=6.5in]{Exponential.pdf}
\end{center}
\label{fig:exp}
\caption{Correlation by time lag for random vector with exponential correlation with $\alpha = 2$ in original units and then once linearly transformed by multiplying by matrix $A$ for four time period lengths: 10, 20, 50, 100 units.}
\end{figure}


\subsubsection{Sparsity and Irregularity}
We seek to cluster individuals based on similar shapes. There are many factors that may limit any method's ability to accurately deduce and cluster the underlying shapes in the data. In many longitudinal studies, subjects are followed over a period of time and are scheduled
to be assessed at a common set of pre-specified visit times after enrollment. However, subjects often selectively miss their visits or return at non-scheduled points in time. As a result, the measurement times are irregular yielding a highly imbalanced data structure. If the time between visits is long resulting in sparsity in the time sampling, the observed data may not be dense enough to capture subtle pattern changes in an individual's trajectory. People change in the short term and in the long term and the research question must dictate the desired level, micro or macro, of pattern detail and the preferred time between observations. \\\\
Additionally, irregularity in the observation times between subjects and lag between observation times is not constant within a subject can impact the observed pattern and the estimated mean. If the observation times are such for a few individuals that the outcome is not observed at in the extremes,  subtracting  the mean estimate may result in outliers in the vertically shifted space, which makes it hard to fit a Gaussian mixture model since it is sensitive to outliers.



%\section{Example} for technical report
%Toy examples

\section{Discussion}
{\bf Derivative Spline}\\
Pros: based on derivative which captures the shape of the trajectory, involves nonparametric estimation derivative (better than crude quotient difference derivative estimates that are sensitive to measurement error ), dimensionality reduction by summarizing trajectories with a few features
Cons: does not borrow strength between individuals even though we suspect many have similar shapes. Accuracy of estimation of derivatives may differ between individuals if time sampling is unbalanced and irregular.  Hard clustering (no uncertainty measures), no concomitant variable probability structure (have to do ad hoc),\\
{\bf Multilayer Mixture}\\
Pros: have probability structure and posterior probabilities
Cons: distribution assumptions about the data, need sub groups to be well-separated vertically in order for the estimation process to select more than one subcomponent and data do not appear in this form frequently. Usually much more noise. No distinct groups within shape clusters, so it ends up doing a regular mixture model that is dominated by level where we average over any interesting shapes.\\
{\bf Vertical Shifting}\\
Pros: Compare shapes without making specific assumptions about the distribution of the intercepts (all have mean 0), probability structure for  baseline variables, posterior probabilities. 
Cons: distributional assumptions (normality), not modeling the data generating distribution,  covariance issues, 


\bibliographystyle{plain}	
\bibliography{Dissertation}
\end{document}
