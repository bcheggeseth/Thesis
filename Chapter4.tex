\documentclass[11pt]{article}
\usepackage{fullpage,amsmath,amsfonts,graphicx,amsthm}
\usepackage{setspace}
\title{Three Proposed Methods}
\author{Brianna C. Heggeseth}

\newtheorem{theorem}{Theorem}
\newcommand{\B}[0]{\mathbf}
\newcommand{\bs}[0]{\boldsymbol}
\newcommand{\Cov}[0]{\text{Cov}}
\newcommand{\Cor}[0]{\text{Cor}}

\begin{document}
\doublespace
\maketitle
\noindent In this chapter, I present three clustering techniques---derivative spline coefficient partitioning, multilayer mixture model, and vertically shifted mixture model---that attempt to answer the three research questions presented in the last chapter: Are there distinct shape patterns in the data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? For each method, I discuss related work and then introduce necessary background, notation, and the model specification. I describe the implementation process and any foreseen issues and limitations. After presenting the three methods, I discuss the advantages and disadvantages of each. In the next chapter, a simulation study compares the proposed methods with those presented earlier in the thesis in addressing the three research questions above.
\section{Derivative Spline Coefficients Partitioning}
The first method uses the technique of calculating the derivative to remove the level from the curve. Unlike the quotient difference method to calculate the derivative described in Chapter 3, this method attempts to smooth out and remove noise before calculating the underlying derivative and is not dependent on observing the data at the same time points for all subjects. Inspired by functional data analysis \cite{ramsay2002}, we project the data onto a functional basis to estimate a smooth curve over time and then differentiate the basis to get an indirect estimate of the derivative function. These estimated derivatives then are used to calculate the dissimilarity between individuals. By first removing the levee, this  method should cluster individuals with similar shapes.
\subsection{Related Work}
In Chapter 3, we discuss the quotient difference distance metric suggested by D'Urso and M{\"o}ller-Levet et. al. \cite{d2000,moller2003}. D'Urso refers to the quotient difference distance as the velocity distance and also proposed a quotient difference of the first derivatives to calculate a distance based on acceleration. Similarly, Zerbe presents three distance measures for growth curves based on position, velocity, and acceleration as well \cite{zerbe1979,schneiderman1993}. Rather than using quotient differencing, he suggests estimating each individual's function by fitting a polynomial of degree $D$ using least squares. For individual $i$, we let $\B y_{i}$ be the vector of observed outcomes at times $\B t_{i}$. Then, the estimated curve is $\hat{f}(t)_{i} = [1\; t\;t^{2}\;...\;t^{D}]\;\hat{\bs\beta}_{i}$ where 
$$\hat{\bs\beta}_{i} = (\B W^{T}_{i}\B W_{i})^{-1}\B W^{T}_{i}\B y_{i}$$
and $\B W_{i}$ is the within-individual design matrix based on the polynomial vectors specific to individual $i$. Then, the estimated derivative is equal to
$\hat{f}^{'}(t)_{i} = [0\; 1\;2t\;...\;Dt^{D-1}]\;\hat{\bs\beta}_{i}$. In other words, he projects the data onto a polynomial basis, differentiates the basis and calculates the estimated derivative function, which can be represented by another polynomial basis of degree $D-1$. Rather than using Euclidean distance, Zerbe suggests a natural dissimilarity measure between two functions which is the $L_{2}$ distance \cite{schneiderman1993}. Thus, for the $i$th and $k$th individuals, the distance between the estimated derivative functions is
$$d_{ik} =\left[ \int_{\mathcal{T}} [\hat{f}^{'}_{i}(t)-\hat{f}^{'}_{k}(t)]^{2}dt\right]^{1/2}$$
for a chosen interval, $\mathcal{T}$.\\\\
Likewise, Tarpey and Kinateder proposed using a Fourier basis to estimate the derivative at the end of their paper \cite{tarpey2003}. By projecting the data onto a finite set of Fourier basis functions, it is easy to represent the estimated derivative function in terms another Fourier basis by differentiating the Fourier basis and rearranging the terms to end up with a new function defined by the original basis. The implied dissimilarity measure is defined as the Euclidean distance between the basis coefficients of the derivative function. \\\\
Both of these methods smooth the data by projecting the individual data onto a chosen basis and estimating the coefficients and then calculate the derivative function by differentiating the basis functions. In general, smoothing the data to first estimate individual functions results in the estimated derivatives less sensitive to high measurement error. The two methods differ in how the distance or dissimilarity measure is defined between two individuals. Zerbe used the $L_{2}$ between derivative functions; Tarpey and Kinateder calculated the Euclidean distance between the basis coefficient vectors for the derivative function. This reflects the diversity in the functional cluster analysis literature; some use the $L_{2}$ distance based on the functions \cite{hitchcock2007} while others use the linear coefficients of the basis function \cite{serban2005, tarpey2003, abraham2003}. Tarpey demonstrated the difference between these dissimilarity measures and how the $L_{2}$ distance can be calculated using the coefficients \cite{tarpey2007}. Additionally, he pointed out that clustering using K-means on the raw data vectors often gives results similar to that from using K-means based on coefficients from an orthogonal basis. Some advantages to basing the distance solely on the coefficients rather than the $L_{2}$ distance is the ease of calculation by avoiding an integral and the dimension reduction from the number of observations to the number of basis functions. \\\\
Both the polynomial and Fourier bases are restrictive and therefore biased when the data are more complex than a lower order polynomial  or not periodic and there are limited data points for each individual. Another popular basis is the class of B-splines \cite{deboor1978, schumaker1981}, which extend the advantages of polynomials to include greater flexibility \cite{abraham2003}. To the author's knowledge, no other study have focused on clustering longitudinal data using the coefficients of the B-spline derivative estimate.\\\\
In this following sections, we introduce B-splines and demonstrate how they are used to estimate derivative functions. Then, the implementation and practical decisions that need to be made in this clustering method are presented and discussed.
\subsection{B-spline background}
We fit a m-order B-spline function to each subject $i$ in order to estimate $\hat{f}_i$. For the sake of being self-contained, we include some background on B-splines. Let $t\in[a,b]$ where $a,b\in\mathbb{R}$ and $\xi_0=a<\xi_{1}<\cdots<\xi_{L} < b = \xi_{L+1}$ be a subdivision of  the interval $[a,b]$ by $L$ distinct points, known as internal knots. We now define the augmented knot sequence, $\tau=[\tau_{1},...,\tau_{L+2m}]$ for $m\in\mathbb{N}$, such that 
\begin{align*}
\tau_{1}&=\tau_{2}=\cdots =\tau_{m} =\xi_{0}\\
\tau_{j+m}& = \xi_{j}, \quad\quad j=1,...,L\\
\xi_{L+1}&=\tau_{L+m+1}=\tau_{L+m+2}=\cdots =\tau_{L+2m} 
\end{align*}
The spline function, $s(t)$, is a polynomial of order $m$ on every interval $[\tau_{j-1},\tau_{j}]$ and has $m-2$ continuous derivatives on the interval $(a,b)$. The set of spline functions of order $m$ for a fixed sequence of knots, $\tau = [\tau_1,...,\tau_{L+2m}]$, is a linear space of functions with $L+m$ free parameters. A useful basis $B_{1,m}(t),...,B_{L+m,m}(t)$ for this linear space is given by Schoenbergs' B-splines \cite{curry1966, de1976} defined as
\begin{align*}
B_{j,1}(t) &= \begin{cases}
1 \text{ if }\tau_j\leq t < \tau_{j+1}\\
0\text{ otherwise}
\end{cases}\\
B_{j,l}(t) &= \frac{t-\tau_j}{\tau_{j+l-1}-\tau_j} B_{j,l-1}(t)+\frac{\tau_{j+l}-t}{\tau_{j+l}-\tau_{j+1}} B_{j+1,l-1}(t)
\end{align*}
where $l=2,...,m$ and $j=1,...,L+2m-l$.  If we adopt the convention that $B_{j,1}(t)=0$ for all $t\in\mathbb{R}$ if $\tau_{j}=\tau_{j+1}$, then by induction $B_{j,l}(t)=0$ if $\tau_{j}=\tau_{j+1}=\cdots=\tau_{j+l}$. Hence, $B_{1,l}(t)=0$ for $t\in\mathbb{R}$ and $l<m$ on the defined knot sequence. The B-spline function of order $m$ is defined by
$$s(t) = \sum^{L+m}_{j=1} P_j B_{j,m}(t).$$
Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote a vector of repeated observations for individual $i$ for $i=1,...,n$ observed at times $\B t_{i}=(t_{i1},...,t_{im_{i}}).$ We assume that $y_{ij} = f_{i}(t_{ij}) + \epsilon_{ij}$ such that $E(\epsilon_{ij}) = 0$ for all $j=1,...,m_{i}$ and $i=1,...,n$.  To estimate $f_{i}$, we fix the order of the B-spline, $m$, and internal knots. Then we estimate the coefficients, $\B P^{(i)} = (P^{(i)}_1,...,P^{(i)}_{L+m})$ using least squares, 
$$\hat{\B P}^{(i)} = (\B W_{i}^{T}\B W_{i})^{-1}\B W_{i}\B y_{i}$$
where $\B W_{i}$ is the within-individual design matrix based on the B-spline basis functions. Then, the estimated function is $\hat{f}_i(t)=\sum^{L+m}_{j=1} \hat{P}^{(i)}_j B_{j,m}(t)$. \\\\
To estimate $f_i'(t)$, we differentiate $\hat{f}_i(t)$ with respect to $t$ and we get
$$\hat{f}'_i(t)=\sum^{L+m}_{j=1} \hat{P}^{(i)}_j B'_{j,m}(t)$$
Prochazkova \cite{prochazkova2005} showed that this can be simplified to
$$\hat{f}'_i(t)=\sum^{L+m}_{j=2} \hat{P}^{(i)}_j \left[\frac{m-1}{\tau_{j+m-1}-\tau_j} B_{j,m-1}(t)-\frac{m-1}{\tau_{j+m}-\tau_{j+1}} B_{j+1,m-1}(t)\right].$$
However, we can go one step further and write this in terms of a B-spline basis of one order lower.
\begin{align*}
\hat{f}'_i(t)&=\sum^{L+m-1}_{j=1} (\hat{P}^{(i)}_{j+1} -\hat{P}^{(i)}_j)\frac{m-1}{\tau_{j+m}-\tau_{j+1} }B_{j+1,m-1}(t)
\end{align*}
If we adjust the knot sequence to only have $m-1$ replicates at the beginning and end, then $\hat{f}'_{i}(t)$ can be written as
$$\hat{f}'_i(t)=\sum^{L+m-1}_{j=1}\hat{Q}^{(i)}_jB_{j,m-1}(t)$$
where $\hat{Q}^{(i)}_j= (\hat{P}^{(i)}_{j+1} -\hat{P}^{(i)}_j)\frac{m-1}{\tau_{j+m}-\tau_{j+1} }$.
We use these new coefficients, $\hat{\B Q}^{(i)} = [\hat{Q}^{(i)}_1,...,\hat{Q}^{(i)}_{ L+m-1}]$, for $i=1,...,n$, to cluster trajectories with similar shape, in terms of the estimated derivative using the K-means algorithm \cite{macqueen1967, hartigan1979}. 
\subsection{Implementation}
To use this method in practice, decisions about the B-spline basis need to be made a priori. First, the order of the polynomials must be chosen. This together with the number of internal knots impacts the flexibility of the B-spline function. Cubic B-splines of order four have been shown to have good mathematical properties and are used frequently in practice \cite{james2003}. However, depending on the number of data points per subject, it may be necessary to use a quadratic polynomial ($m = 3$) due to the restriction that $L+m$ must be less than or equal to the minimum number of observation points per subject. \\\\
As mentioned, the number of internal knots, $L$, plays a role in shaping the class of spline functions. In choosing L, the main limiting factor will be the number of data points per subject, but if there are many repeated measures, model selection information criteria or cross-validation can be used to choose $L$ \cite{rice2001}. The location of the internal knots is another issue of discussion. There are some suggested data-driven ways to select knot location \cite{shanggang2001}, but Ruppert \cite{ruppert2002} supports fixing the knots at sample quantiles given the number of knots. \\\\
Once the order of the polynomials and number and location of knots are chosen, then smooth functions and their derivatives are estimated for every subject. The coefficients from the estimated derivative functions are used to cluster individuals. The K-means algorithm is a standard clustering algorithm that given a fixed number of clusters, minimizes an objective function mentioned above after initially randomizing vectors to a groups. This algorithm will converge but is not guaranteed to find the grouping that globally minimizes the objective function. Therefore, in practice, the algorithm needs to run multiple times with different random initializations and the clustering that minimizes the objective function is chosen. We use 25 random starts.\\\\
However, in order to run K-means, you need to fix the number of clusters, $K$, which is usually unknown and of interest to researchers. While no perfect mathematical criterion exists, a number of heuristics (see \cite{tibshirani2001} and discussion therein) are available for choosing $K$. For this thesis, we choose the $K$ that maximizes the overall average silhouette width \cite{rousseeuw1987}.  For each subject $i$, the silhouette width, $s_{i}$, is calculated by
$$s_{i}=\frac{b_{i}-a_{i}}{\max\{a_{i},b_{i}\}}$$
where $a_{i}$ is the average dissimilarity of $i$ to all other trajectories in its cluster, $A$, $d_{i}(C)$ is the average dissimilarity of $i$ to all trajectories of cluster $C$ and $b_{i}$ is equal to $\min_{C\not= A} d_{i}(C)$. The overall average silhouette width, $\bar{s}$, is the average of $s_{i}$ over all subjects in the whole data set. By maximizing the overall average silhouette width, you are maximizing $b_{i}$, the dissimilarity between clusters and minimizing $a_{i}$, the dissimilarity within clusters. In this case, the dissimilarity between two individuals is defined as the Euclidean distance between derivative coefficients. We choose $K$ that best separates the data so we have groups of individuals with similar shape patterns over time.\\\\
K-means is a partitioning algorithm, therefore, by definition every subject is `hard' clustered into only one of the groups. There is no stated uncertainty in the group memberships even if a subject is on the edge between two clusters. In order to estimate the relationship between baseline variables and shape group membership, we assume the cluster labels are known. Given the subject grouping labels from the partition, $\{c_{i}\}$ such that $c_{i}\in\{1,2,...,K\}$ for all $i=1,...n$, we fit a multinomial logit regression model, which is an extension of the logistic regression model, using the group labels as the outcome and intercept plus baseline variables, $\B z_{i}$, as explanatory variables such that
$$P(c_{i} = k) = \frac{\exp(\B z_{i}^{T}\bs\gamma_{k})}{\sum_{j=1}^{K}\exp(\B z_{i}^{T}\bs\gamma_{j})}$$
for all $k=1,...,K$ and $i=1,...,n$ with $\bs\gamma_{K}=0$. We then estimate the parameters $\bs\gamma_{1},...,\bs\gamma_{K-1}$ using maximum likelihood estimation. The estimated standard errors from a standard Hessian calculation do not include any potential uncertainty in group membership as we assumed the hard clustering labels were known during the estimation process.

\section{Multilayer model}
The second method attempts to make up for the lack of uncertainty in groups memberships by using a model-based approach. The circumstances in which traditional methods fail are when the level and shape are weakly dependent resulting in individuals with the same shape but with different levels. Therefore, it may make sense to to imagine shape clusters having a mixture distribution of levels.
\subsection{Related Work}
Model-based methods provides the probability framework that dissimilarity-based methods lack. One of the main benefits is the ability to take into account uncertainty---the uncertainty of the mean estimates as to make inferences of a larger population and the uncertainty of cluster membership when estimating relationships between baseline variables and clusters. As mentioned in Chapter 1, the main model-based method is the finite mixture model. However, a standard mixture of Gaussians fit to the raw data does not distinguish between the level and shape and will cluster subjects based on the dominant source of variability. Another limitation of Gaussian mixtures for grouping data results from the potential non-normality of cluster densities. For these reasons, the mixture model has been extended into multilayer mixture models, which are a generalization of finite mixture models in that each cluster is allowed to be a mixture itself \cite{li2005} (see Figure \ref{fig:dia} for diagram). Researchers have used a similar idea in clustering by first fitting a mixture and then combining cluster components to make more meaningful clusters \cite{hennig2010}. \\\\
\begin{figure}[h]
\begin{center}
\includegraphics[width=4in]{Chp4multilayer_diagram}
\end{center}
\label{fig:dia}
\caption{Diagram of multilayer mixture model showing that each cluster is comprise of potentially more than one component.}
\end{figure}
In our case, we want the clusters to be meaningful in terms of distinguishing between shape. Therefore, we can use this generalization in order to model $K$ non-normal shape clusters comprised of mixture components with the same mean shape at different levels.\\\\

\subsection{Multilayer background}
In a multilayer mixture model, let $K$ be the number of clusters and $J_{k}$ be the number of components in cluster $k=1,...,K$ such that $J=\sum_{k=1}^{K}J_{k}$ is the total number of components in the entire model. Let $j$ uniquely index all of the components such that $j=1,...,J$. For ease of explanation, we define a cluster assigning function, $c(j):\{1,...,J\}\rightarrow \{1,2,...,K\}$, to specify the cluster to which a component belongs. Let $f(\B y|\bs\mu,\bs\Sigma)$ be the probability density function of a multivariate normal distribution with mean $\bs\mu$ and covariance matrix $\bs\Sigma$. Then the mixture probability density function for cluster $k$ is
$$f_{k}(\B y) = \sum_{j: c(j) = k} \pi_{j|c(j)}f(\B y|\mu_{j},\Sigma_{j})$$
where $\pi_{j|c(j)}$ is the probability of being in component $j$ given a sample is in cluster $c(j)$ and $\sum_{j: c(j) = k}\pi_{j|c(j)}=1$ for all $k=1,2,...,K$. Let the probability of cluster $k$ given baseline variables, $\B z$, be $\pi_{k}(\B z,\bs\gamma)=\exp(\B z^{T}\bs\gamma_{k})/\sum^{K}_{l=1}\exp(\B z^{T}\bs\gamma_{l})$ such that $\bs\gamma_{K}=0$ and $\bs\gamma=(\bs\gamma_{1},...,\bs\gamma_{K})$. Then the probability density function for the multilayer mixture can be written as
$$g(\B y|\B z) = \sum_{k=1}^{K}\pi_{k}(\B z,\bs \gamma)f_{k}(\B y) = \sum_{k=1}^{K}\pi_{k}(\B z,\bs\gamma)\sum_{j: c(j) = k} \pi_{j|c(j)}f( \B y| \bs\mu_{j},\bs\Sigma_{j})$$
Since every component belongs to only one shape cluster, the above equation reduces to a regular mixture model if we let $\bar{\pi}_{j}(\B z,\bs \gamma)=\pi_{c(j)}(\B z,\bs\gamma)\pi_{j|c(j)}$,
$$g(\B y|\B z) = \sum_{j=1}^{J}\bar{\pi}_{j}(\B z,\gamma)f(\B y|\bs \mu_{j},\bs\Sigma_{j})$$
In order to accommodate our goal of clustering on shape, we include a regression parameterization for the mean structure to model the smooth underlying mean function. We use a B-spline basis for the design matrix as presented earlier in this chapter. However, we restrict the regression parameters to be the same within shape clusters but allow the intercept to differ in each component. Hence, $\mu_{j}=\alpha_{j}+\B x \bs\beta_{c(j)}$, where $\B x$ is a matrix of B-spline basis functions excluding the first basis so as to allow for estimation of intercept terms for each component. This allows the clusters to be based on shape while the components can have different levels (see figure \ref{fig:diashape} for diagram). Additionally, we assume $\Sigma_{j}=\sigma^{2}_{j}I$ for all $j=1,...,J$  for simplicity to allow for irregularly sampled data.\\\\
In order to estimate the shape cluster and component-specific parameters, we maximize the classification likelihood using a modified Expectation Maximization (EM) algorithm \cite{dempster1977} called the Classification EM algorithm \cite{mclachlan2000}. The details of the iterative algorithm can be found in Ji Lia \cite{li2005}, but the main modification involves hard clustering clusters while soft clustering the components through the iterative estimation process.
\begin{figure}[h]
\begin{center}
\includegraphics[width=4in]{Chp4multilayer_diagram_shape}
\end{center}
\label{fig:diashape}
\caption{Stuff}
\end{figure}
\subsection{Implementation}
Denote the shape cluster identity of sample $i$ by $\eta(i)$ where $\eta(i)\in\{1,...,K\}$. Then $\eta = \{\eta(i); i=1,...,n\}$ and $\theta=\{\bs \gamma_{k}, \pi_{j|c(j)},\alpha_{j}, \bs\beta_{k},\sigma^{2}_{j};\; j=1,...,J, k=1,...,K\}$ are the parameters for the model. In order to estimate the parameters in this multilayer mixture model, we use the classification maximum likelihood \ref{mclachlan2000} such that we maximize the function
\begin{align}
 L(\theta,\eta) &= \sum^{n}_{i=1} \log \pi_{\eta(i)}(\B z_{i},\bs\gamma) f_{\eta(i)}(\B y_{i}|\B x_{i})\\
\label{ll}& =  \sum^{n}_{i=1} \log\left[ \pi_{\eta(i)}(\B z_{i},\bs\gamma)  \sum_{j: c(j) = \eta(i)} \pi_{j|c(j)}f(\B y_{i}|\B x_{i},\alpha_{j},\bs\beta_{\eta(i)},\sigma^{2}_{j})\right]
\end{align} 
To maximize this log-likelihood, we use a modified EM algorithm called classification EM (CEM) \ref{celeux1992;mclachlan2000}. The modification includes a classification step between the E-step and M-step where individuals are grouped into shape clusters.\\\\
INITIALIZATION (TALK MORE) In order to start the iterative process, the algorithm first needs to have initial clusters and components groups. Initialization can involve randomly partitioning individuals into components or strategically partitioning individuals into shape cluster using a quick procedure such as the first method, derivative coefficient K-means, and then randomly partitioning the individuals into components. \\\\
At iteration $t$, $\theta^{(t)}$ and $\eta^{(t)}$ are the current estimates of the parameters. The algorithm updates these estimates as follows:
\begin{enumerate}
\item E-step: Compute posterior probabilities of being in shape cluster $k$, $p_{i,k}$, such that
$$p_{i,k}\propto \pi_{k}(\B z_{i},\bs\gamma)f_{k}(\B y_{i}|\B x_{i})$$
\item Classification: Hard classify subjects to shape clusters according to $\eta^{(t+1)}(i) = \arg\max_{k} p_{i,k}$.
\item M-step: For each shape cluster $k$, use maximum likelihood estimation to update parameter vector $\theta$ by embedding an EM procedure initialized by the current parameter values.
\end{enumerate} 
This algorithm increases the log-likelihood at each iteration and the statement is below. The proof is in the appendix. 
\begin{theorem} The classiÞcation likelihood $L(\theta,\eta)$ defined in (\ref{ll}) is non-decreasing after each update of $\eta$ and $\theta$ by the CEM algorithm. That is, $L(\theta^{(t+1)},\eta^{(t+1)}) \geq L(\theta^{(t)},\eta^{(t)})$ for all $t$.
\end{theorem}
The proof is included in the appendix.\\\\
Within the algorithm, parameters are estimated by maximizing the likelihood for each shape cluster. Since we have constrained the shape parameters for all components within the cluster, we use computational methods suggested by Gr{\"u}n \cite{grun2008}. We calculate the estimates for all components simultaneously. The outcome vector for subject $i$ is replaced by a vector of $\B y_{i}$ repeated $J_{k}$ times. The design matrix  is set equal to $(I_{J_{k}}\otimes \bs 1_{m_{i}}, \bs 1_{J_{k}} \otimes \B x_{i})$. Lastly, the covariance matrix structure is set to block diagonal with $\sigma^{2}_{j}I_{m_{i}}$ in each block for $j$ that satisfy $c(j)=k$. Assuming a multivariate Gaussian distribution and using profile likelihoods, we maximize the likelihood function with respect to the parameters for cluster $k$ and its components using an optimization routine.\\\\
Now, as in the derivative coefficient method, we want to allow the mean structure to be flexible enough to accommodate shapes other than lower ordered polynomials. So we use a B-spline basis for the mean structure. The same issues of the number and location of internal knots are dealt with in the same manner.\\\\
With this more complex mixture structure, it is more difficult to select the number of shape clusters, $K$, as well as the number of components for each cluster $k$, $J_{k}$, $k=1,..,K$. It is actually recommended to fix $K$ and then use information criteria to select $J_{k}$ for each $k=1,...,K$. As suggested by Jia Li, we use BIC even though the regularity conditions do not hold as they have been shown to be useful informal guides in practice \cite{li2005}.\\\\
Robust standard error derivations were done using the regular mixture form and following the same procedure as Boldea and Magnuson \cite{boldea2009}. These are available in the appendix.
\section{Vertical Shifting Mixture Model}
The goal of cluster analysis is to partition a collection of individuals into homogeneous groups. In this thesis, we define similar individuals as those with the same outcome pattern or shape over time. We aim to develop an effective clustering method that focuses on grouping longitudinal data with a special focus on shape similarities. If two curves only differ in intercepts by vertical shifts, they are placed in the same group. Up until now, we have used derivatives to implicitly remove the level and directly modeled the variability in the level.  In this method, we consider vertically shifted trajectories which result from subtracting the mean outcome level. Each cluster comprises a density with a mean shape curve and a covariance function. The assumed finite mixture of densities is fit to the vertically shifted data to estimate parameters and group membership probabilities. \\\\
\subsection{Recent Work}
A mixture model is a standard method for clustering multivariate data \cite{everitt2009} and have been used for longitudinal applications \cite{muthen2010, jones2001}. However, for longitudinal data, the models are commonly used for the observed data without much regard to the real goal of clustering by shape. We suggest subtracting out the mean outcome level before modeling. Subtracting the level is not a novel idea in statistics or even cluster analysis. In most multivariate clustering applications, it is recommended that each variable is standardized by subtracting the mean of the variable measures and dividing by the standard deviation so that each standardized variable is in comparable units and equally contribute to the clustering process. In the longitudinal setting, each ``variable'' is a repeated measurement at a different time point; therefore, the variables are all in the same units. However, we suggest removing a subject-specific level rather than completing variable-specific centering. This seems like an obvious pre-processing step; however, it is rarely used in practice and there are some consequences that need to be considered when modeling the transformed data as discussed later in the chapter. \\\\
A partial version of this idea has been implemented in the functional data analysis literature. For processes in a Hilbert space of square integrable functions with respect to the Lebesgue measure, $dt$, on $\mathcal{T}=[0,T]$, Chiou and Li \cite{chiou2008} propose using a mixture model and the Karhunen-Lo{\`e}ve expansion for centered stochastic processes within their correlation-based clustering algorithm. To center the processes, they subtract the integral of the process over interval $\mathcal{T}$ divided by $T$, the length of the interval. The resulting process integrates to zero. The integral of the process is the functional analogue to a mean in finite vector space and similarly, the resulting vector after subtraction has mean zero.\\\\
 Although the centering process stems from the same idea, there are distinct consequences of subtracting the estimated level of a noisy curve observed at a finite number of points that don't arise when centering a smooth function. The term centering is used in the stochastic processes literature, but we use the term vertically shifting rather than centering to refer to the procedure of subtracting the mean since it graphically describes the transformation of the noisy longitudinal data.\\\\
In this chapter, we outline the model notation and specification for vertically shifted mixture models including the mean and covariance structure. Then we describe the implementation process of choosing a basis, the number of clusters, and estimating the parameters of the model. As mentioned there are some considerations and issues that we discuss in terms of the impact of transforming the data on the modeling process and final clustering results. 
\subsection{Model Specification}
 Let $\B y_{i}=(y_{i1},...,y_{im_{i}})$ denote a vector of repeated observations for individual $i$ for $i=1,...,n$. As longitudinal data is collected over a period of time, the vector of corresponding times of observation for individual $i$ is denoted as $\B t_{i}=(t_{i1},...,t_{im_{i}})$ and $\B z_{i}$ is a vector of time-fixed covariates that are typically collect at or before time $t_{i1}$. Let $\bar{y}_{i}= m_{i}^{-1}\sum^{m_{i}}_{j=1} y_{ij}$ be the mean of the observed outcomes for individual $i$. This is one measure of the vertical level of a curve. To remove the level of the observations and leave the pattern over time, we apply a linear transform to the vector of observations and let
\begin{align*}
\B y^{*}_{i} = \B A_{i}\B y_{i}\quad\text{ where }\quad \B A_{i} =\B I_{m_{i}} - m_{i}^{-1}\B 1_{m_{i}}\B 1_{m_{i}}^{T}
\end{align*}
where $\B 1_{m}$ is a m-length vector of 1's and $\B I_{m}$ is an $m\times m$ identity matrix. Multiplying the matrix $\B A_{i}$ to the vector $\B y_{i}$ subtracts the individual mean, $\bar{y}_{i}$, from each element $\B y_{i}$. The resulting vertically-shifted vector, $\B y^{*}_{i}$, is in units relative to the mean, $\bar{y}_{i}$, and is assumed to arise from one of $K$ shape groups characterized by probability densities that are centered around mean patterns with dependence summarized by covariance matrices. We assume that, conditional on $\B t$ and $\B z$, $\B y^{*}$ is a realization from a finite multivariate mixture model with density
\begin{align*}
 f(\B y^{*}|\B t,\B z,\bs\theta,K) =  \sum^{K}_{k=1}\pi_{k}(\B z_{i}, \bs\gamma)f_{k}( \B y^{*}|\B t_{i},\bs\theta_{k})\label{mixmodel}
\end{align*}
where $\pi_{k}(\B z_{i},\gamma)>0$ for $k=1,...,K$ and $\sum^{K}_{k=1}\pi_{k}(\B z_{i},\gamma)=1$ and $\bs\theta = (\bs\gamma,\bs\theta_{1},...,\bs\theta_{K})$. To allow baseline covariates to affect the probability of having a certain shape pattern over time, we parameterize the mixing proportions using the multinomial logit with the form
$$\pi_{k}(\B z_{i},\bs\gamma)=\frac{\exp(\B z_{i}^{T}\bs\gamma_{k})}{\sum_{j=1}^{K}\exp(\B z_{i}^{T}\bs\gamma_{j})}$$ 
for $k=1,...,K$ where $\bs \gamma_{k}\in\mathbb{R}^{q}$, $\bs\gamma = (\bs\gamma_{1},...,\bs\gamma_{K})$, and we fix $\bs\gamma_{K}=\B 0$. We assume the component densities $f_{k}(\B y^{*}|\B t_{i},\bs\theta_{k})$ are multivariate Gaussian with mean $\bs\mu_{k}(\B t_{i})$ and covariance matrix $\Sigma_{k}(\B t_{i})$ both based on the observation times and the vector of parameters $\bs\theta_{k}$.
\subsubsection{Mean Structure}
In the longitudinal setting, the main interest is in the change over time; therefore, the mean shape is a function of time. To parameterize the mean pattern in the mixture model, we let the design matrix, $\B x_{i}$, be a matrix of values from a chosen functional basis taken at observation times $\B t_{i}$. It is common to use a quadratic or cubic basis by letting $\B x_{i} = (\B 1_{m_{i}}^{T},\B t_{i}^{T}, (\B t^{2}_{i})^{T})$. However, this limits the ability of the model to capture the complexity of a mean structure. We use the class of B-splines basis functions \cite{deboor1978, schumaker1981,curry1966, de1976}, which extend the advantages of polynomials to include greater flexibility. The B-spline function of order $m$ is defined by a linear combination of coefficients and basis functions
$$s(t) = \sum^{L+m}_{j=1} P_j B_{j,m}(t)$$
where the basis functions, $B_{j,m}(t)$, are defined in Chapter 3. We assume the mean pattern for the $k$th shape cluster is approximated by the function $s_{k}(t)=\sum^{L+m}_{j=1} P^{(k)}_j B_{j,m}(t)$.  Since we only observe measurements at discrete times, we let the design matrix include $L+m$ variables corresponding to the basis functions evaluated at the measurement times.  For each individual $i$ observed at times $t_{i1},...,t_{im_{i}}$, we let $$\B x_{i} = \left(\begin{array}{cccc} 
B_{1,m}(t_{i1})&B_{2,m}(t_{i1})&\cdots&B_{L+m,m}(t_{i1})\\
B_{1,m}(t_{i2})&B_{2,m}(t_{i2})&\cdots&B_{L+m,m}(t_{i2})\\
\vdots&\vdots&\ddots&\vdots\\
B_{1,m}(t_{im_{i}})&B_{2,m}(t_{im_{i}})&\cdots&B_{L+m,m}(t_{im_{i}})\end{array}\right)$$
and $\bs\beta_{k} = (P^{(k)}_{1},...,P^{(k)}_{L+m})$ so that the vector-based mean for individual $i$ is $\bs\mu_{k}(\B t_{i}) = \B x_{i}\bs\beta_{k}$.
\subsubsection{Covariance Structure}
Various assumptions can be made about the covariance matrix, $\B \Sigma_{k}(\B t_{i})$. Since it is common for longitudinal data to have sparse, irregular time sampling, we must impose some structure on the covariance matrix to allow for parameter estimation as described by Jennrich and Schluchter in their seminal paper \cite{jennrich1986}. A common parameterization is conditional independence with constant variance where $\B \Sigma_{k}(\B t_{i}) = \sigma_{k}^{2}I_{m_{i}}$. This is an unrealistic assumption for longitudinal data since there is inherent dependence between repeated measures on the same unit. \\\\
Compound symmetry, which is also known as exchangeable correlation, is a popular correlation structure in longitudinal analysis where all repeated measures are equally correlated with strength $\rho_{k}$ and $\B \Sigma_{k}(\B t_{i}) = \sigma_{k}^{2}(\rho_{k}\B1_{m_{i}}\B1_{m_{i}}^{T}+(1-\rho_{k})I_{m_{i}}).$ It is typically paired with constant variance as well. This dependence structure is not frequently observed in practice but approximates the correlation better than the independence model. The goal of having a better but perhaps not perfect  approximation is mostly used when the estimation of mean parameters is robust to misspecifying the covariance structure, which is not the case with mixture models as evidenced in Chapter 2. \\\\
A more general structure that provides a compromise between the two is the exponential correlation structure in which the dependence decreases as the time between observations increases---$\B \Sigma_{k}(\B t_{i})_{jl} = \sigma_{k}^{2}\exp(-| t_{ij}-t_{il}| / r_{k})$ where $r_{k}\geq 0$. This structure is similar to the correlation matrix generated from an autoregressive model of order one where $\B\Sigma_{k}(\B t_{i})_{jl} = \sigma^{2}\rho_{k}^{|t_{ij}-t_{il}|}$ where $\rho_{k}$ is the correlation for measurements observed one unit of time apart and thus $-1\leq \rho_{k}\leq 1$.  If we set $\rho_{k} = \exp(-1/r_{k})$, then $0\leq\rho_{k}\leq1$ and the two parameterization result in the same structure if the correlation between two measures is constrained to be positive. This is reasonable assumption for longitudinal data in the original form but many not be for the transformed data as we will discuss later in the chapter. DAMPED EXPONENTIAL? The covariation structures mentioned above are describe stationary processes in that they have constant variance and the correlation only depends on the time lag between observations. NON-STATIONARY MODELS WITH NON CONSTANT VARIANCE or CORRELATION NOT A FUNCTION?\\\\
As discovered in Chapter 2, the assumed covariance matrix can highly impact the results in terms of parameter estimates and the final clustering if the groups are not well-separated. A measure such as the RJ criteria defined in Chapter 2 can be useful to choose a covariance matrix structure in practice. However, these principals apply to longitudinal data in its original form. Once the mean is estimated and subtracted from the data, the resulting covariance structure of the transformed data may behave drastically different. Further discussion about the covariance of vertically shifted data will occur later in the paper.
\subsection{Implementation}
Given a collection of independent vectors or repeated observations $\B y_{1},...,\B y_{n}$, the first step of implementing this method is to calculate the mean for each subject, $\bar{y}_{i}$, $i=1,...,n$ and subtract the subject-specific mean from the observed outcome vector for each subject. This transformation leaves the updated independent vectors $\B y^{*}_{1},...,\B y^{*}_{n}$. Second, the order of the spline and the number and location of internal knots for the mean structure need to be decided prior to estimation. \\\\
The spline basis is kept constant for all components so the simplest way to select their values is through visual inspection of the full data set. When making knot placement via visual inspection, more knots are needed where the mean trend changes more rapidly. If the most complex shape patterns appear quadratic or cubic, no internal knots are necessary. However, if the most complex curve is more varied, adding knots and increasing the order of the polynomials can flexibly accommodate the twists and turns of the mean patterns. In choosing both the order and number of knots, it is important to balance the number of mean parameters with the sample size. Every increase in the order or in the number of knots increases the number of parameters by $K$, the number of components. In terms of location of the knots, it has been suggested to place knots at quantiles based on the sampling times of all the observations (citations from chapter 3). However, if you are taking the observation times of all subjects into account, this strategy may not work well if the median time is not the point at which the curve deviates from a typical polynomial. If possible, it is best to place knots at inflection points of the overall trends to as to accommodate the differences from a polynomial function \cite{embank1999}.  Once these are decided, then the matrixes $\B x_{i}$ as defined above can be calculated using available algorithms that calculate B-spline functions. \\\\
Thirdly, the model above assumes the number of clusters, $K$, is known. In practice, this is not the case and we must choose $K$. The most popular procedure is to chose a maximum value of $K$ such that $K<n$, fit the model under all values of $K=2,...,K_{max}$, and choose the value that optimizes the chosen criteria. In this thesis, we use the Bayesian Information Criterion (BIC) \cite{schwarz1978} when maximizing the likelihood function. It is defined as
$$BIC = -2\log L(\hat{\bs\theta},K)- d\log(n)$$
where $d$ is the length of $\bs\theta$, the number of parameters in the mixture model, and $L(\bs\theta,K)$ is the log likelihood function for the parameter vector.The BIC has been widely used for mixture model ever since they were used by Roeder and Wasserman \cite{roeder1997} and in particular, they have been used for clustering \cite{dasgupta1999,fraley1999} with good results in practice. For regular models, the BIC is derived as an approximation to twice the log integrated likelihood using the Laplace method
\cite{tierney1986}, but the necessary regularity conditions do not hold for mixture
models in general \cite{aitkin1985}. However, Roeder and Wasserman \cite{roeder1997} showed
that BIC leads to a consistent estimator of the mixture density, and Keribin \cite{keribin2000} showed
that BIC is consistent for choosing the number of components in a mixture model.\\\\
In order to fit the model and estimate the parameters, we use maximum likelihood estimation via the EM algorithm. Under the assumption that $\B y^{*}_{1},...,\B y^{*}_{n}$ are independent realizations from the mixture distribution, $f(\B y^{*} | \B t, \B z, \bs\theta,K)$, defined in \ref{mixdens}, the log likelihood function for the parameter vector, $\bs \theta$, is given by
$$\log L(\bs\theta,K)=\sum^{n}_{i=1}\log f(\B y^{*}_{i}|\B t_{i},\B z_{i},\bs \theta,K).$$
The ML estimate of $\bs\theta$ is obtained by finding an appropriate root of the score equation, $\partial \log L(\bs\theta)/\partial \bs\theta=\B 0.$
Solutions of this equation corresponding to local maxima can be found iteratively through the Expectation-Maximization (EM) algorithm \cite{dempster1977}. This algorithm is applied in the framework where given $(\B t_{i},\B z_{i})$ each $\B y^{*}_{i}$ is assumed to have stemmed from one of the components and the indicator denoting its originating component is missing. The complete-data log likelihood is based on these indicator variables as well as the observed data $\{(\B y^{*}_{i}, \B t_{i}, \B z_{i})\}$. The Expectation step (E-step) involves replacing the indicators by current values of the conditional expectation, which is the posterior probability of component membership, written as
$$\alpha_{ik}=\pi_{k}(\B z_{i},\bs\gamma)f_{k}(\B y^{*}_{i}|\B x_{i})/\sum_{j=1}^{K}\pi_{j}(\B z_{i},\bs\gamma)f_{j}(\B y^{*}_{i}|\B x_{i},\bs \theta_{j})$$
for $i=1,...,n$ and $k=1,...,K$ using current estimates of the parameters. In the Maximization step (M-step), the parameter estimates for the mixing proportions, regression effects, and covariance matrices are updated by maximizing the complete-data log likelihood using the posterior probabilities from the E-step in place of the indicator variables using numerical optimization. The E- and M-steps are alternated repeatedly until convergence. The EM algorithm guarantees convergence to a local maximum; global convergence may be attained through initializing the algorithm by randomly assigning individuals to initial components, running the algorithm multiple times and using the estimates associated with the highest log likelihood. Besides the parameter estimates, the algorithm returns the posterior probability estimates of component membership. These probabilities can be used to partition individuals into distinct clusters such as K-means or PAM by selecting the cluster with the maximum posterior probability. However, unlike K-means, the posterior probability provides some measure of uncertainty in the hard clustering. \\\\

\subsection{Issues}
There are many issues with mixture models as stated in Chapter 2. In this section, we will discuss some unique consequences of vertically shifting the data on the model and inference.
\subsubsection{Covariance of Transformed Data}
Let $\B Y=(Y_{1},...,Y_{m})$ be a random vector of length $m$ with mean zero and covariance, $\Cov(Y) = \B V^{1/2}\B R(\alpha)\B V^{1/2}$ where $\B R(\alpha)$ is a correlation matrix based on the parameter $\alpha$ and potentially the associated observation times of $\B Y$ $(T_{1},...,T_{m})$ and $\B V$ is a matrix with variance parameters along the diagonal. If we linearly transform the data to subtract the mean of the elements according to the matrix $A$ previously defined, the covariance of the resulting random vector is
$$\Cov(\B A\B Y) = \B A\Cov(\B Y)\B A^{T}$$
by the properties of covariance. First, let's assume that $\B V=\sigma^{2}I$ such that the variance is not time-dependent. Let's assume independence, $\B R(\alpha)=I$, and then $\Cov(\B Y) = \sigma^{2}I$ and
\begin{align*} 
\Cov(\B A\B Y) &= \sigma^{2}\B A\B A^{T} \\
&=\sigma^{2}\B A\\
&= \sigma^{2}(I - m^{-1}\B1_{m}\B1_{m}^{T})\\
&=\sigma^{2}\left(\frac{m-1}{m}\right)(a\B 1\B 1^{T}+ (1-a)I)
\end{align*}
 where $a=\frac{-1}{m-1}$ since A is an idempotent matrix. Therefore, if the data has independent errors, by subtracting them estimated mean, you induce constant negative correlation between the observations of magnitude $\frac{-1}{m-1}$ with a decreased variance.\\\\
 If you start with compound symmetry with  $R(\alpha) = \alpha\B 1_{m} \B 1_{m} + (1-\alpha) I$, then 
 \begin{align*}
 \Cov(\B A\B Y) &= \sigma^{2}\B A\B R(\alpha)\B A^{T}\\
 &= \sigma^{2}(I-m^{-1}\B1_{m}\B1_{m}^{T})(\alpha\B1_{m}\B1_{m}^{T}+(1-\alpha)I)(I-m^{-1}\B1_{m}\B1_{m}^{T})^{T}\\
 &= \sigma^{2}(1-\alpha)(I-m^{-1}\B1_{m}\B1_{m}^{T})\\
 &=\sigma^{2}(1-\alpha)\left(\frac{m-1}{m}\right)(a\B 1_{m}\B 1_{m}^{T}+ (1-a)I)
 \end{align*} 
 where $a=\frac{-1}{m-1}$ as before. It maintains its compound symmetry structure with negative correlations on the off diagonal and decreased variance. Therefore, if the dependence in the original vector is either independence or compound symmetry, then the transformed vector has a covariance matrix with compound symmetry, but smaller variance and negative correlation. \\\\
 On the other hand, if the original correlation is exponential with $\Cor(Y_{1},Y_{2}) = \exp(-|t_{2}-t_{1}|/\alpha)$, the resulting covariance function after transformation is no longer exponential. The covariance structure is a linear combination of the original correlation matrix, column and row means, and the overall mean correlation:
   \begin{align*}
 \Cov(\B A\B Y) &= \sigma^{2}\left[\B R(\alpha)-m^{-1}\B1_{m}\B1_{m}^{T}\B R(\alpha)-m^{-1}\B R(\alpha)\B1_{m}\B1_{m}^{T} + m^{-2}\B1_{m}\B1_{m}^{T}\B R(\alpha)\B1_{m}\B1_{m}^{T}\right]
 \end{align*} 
 This results in a non-stationary covariance matrix since the correlation is no longer just a function of the difference in observation times (Figure \ref{fig:exp}). Besides inducing non-stationarity in terms of the correlation, the transformation also results in negative correlations. The column and row mean values in the sum have a negative sign and will have a large magnitude if $|t_{m} - t_{1}|/\alpha$ is small. That means that the smallest correlation values are more negative when the observation period is small or if the range, $\alpha$, is large relative to one another. \\\\
 On the other hand, if $|t_{m} - t_{1}|/\alpha$ is large, then the curve could be approximated by a compound symmetric matrix, which allows for negative correlations but doesn't account for the decay of correlation for small lags or an exponential correlation that takes the shape into account but does not allow for negative correlations. However, if the ratio is small, there are no common parametric functions that  would fit the structure well and blindly using compound symmetry or even exponential correlation structure may be so far from the truth that it impacts the clustering. As seen in Chapter 2, if the components are well separated, then it may not have a huge impact. SEPARATION FOR SHAPE TRAJECTORIES.
\begin{figure}
\begin{center}
\includegraphics[width=6.5in]{Chp4Exponential.pdf}
\end{center}
\label{fig:exp}
\caption{Correlation by time lag for random vector with exponential correlation with $\alpha = 2$ in original units and then once linearly transformed by multiplying by matrix $A$ for four time period lengths: 10, 20, 50, 100 units.}
\end{figure}


\subsubsection{Sparsity and Irregularity}
We seek to cluster individuals based on similar shapes. There are many factors that may limit any method's ability to accurately deduce and cluster the underlying shapes in the data. In many longitudinal studies, subjects are followed over a period of time and are scheduled
to be assessed at a common set of pre-specified visit times after enrollment. However, subjects often selectively miss their visits or return at non-scheduled points in time. As a result, the measurement times are irregular yielding a highly imbalanced data structure. If the time between visits is long resulting in sparsity in the time sampling, the observed data may not be dense enough to capture subtle pattern changes in an individual's trajectory. People change in the short term and in the long term and the research question must dictate the desired level, micro or macro, of pattern detail and the preferred time between observations. \\\\
Additionally, irregularity in the observation times between subjects and lag between observation times is not constant within a subject can impact the observed pattern and the estimated mean. If the observation times are such for a few individuals that the outcome is not observed at in the extremes,  subtracting  the mean estimate may result in outliers in the vertically shifted space, which makes it hard to fit a Gaussian mixture model since it is sensitive to outliers.



%\section{Example} for technical report
%Toy examples

\section{Discussion}
{\bf Derivative Spline}\\
Pros: based on derivative which captures the shape of the trajectory, involves nonparametric estimation derivative (better than crude quotient difference derivative estimates that are sensitive to measurement error ), dimensionality reduction by summarizing trajectories with a few features
Cons: does not borrow strength between individuals even though we suspect many have similar shapes. Accuracy of estimation of derivatives may differ between individuals if time sampling is unbalanced and irregular.  Hard clustering (no uncertainty measures), no concomitant variable probability structure (have to do ad hoc),\\
{\bf Multilayer Mixture}\\
Pros: have probability structure and posterior probabilities
Cons: distribution assumptions about the data, need sub groups to be well-separated vertically in order for the estimation process to select more than one subcomponent and data do not appear in this form frequently. Usually much more noise. No distinct groups within shape clusters, so it ends up doing a regular mixture model that is dominated by level where we average over any interesting shapes.\\
{\bf Vertical Shifting}\\
Pros: Compare shapes without making specific assumptions about the distribution of the intercepts (all have mean 0), probability structure for  baseline variables, posterior probabilities. 
Cons: distributional assumptions (normality), not modeling the data generating distribution,  covariance issues, 


\bibliographystyle{plain}	
\bibliography{Dissertation}
\end{document}
