\chapter{Shape-based clustering}

Longitudinal data is collected with the intent of studying the change over time. One side effect of collecting this type of data is the inherent dependence between the repeated measure. In the last chapter, we showed that this nuisance cannot be ignored when clustering data using a mixture model. In this chapter, I return to discussing clustering methods with the research goals of longitudinal studies in mind. 

\section{Motivation}
There is no one way to cluster data. There are many characteristics of quantitative data on which to base a grouping. This is not only true for data sets. We constantly group products, people, and projects based on different qualities and characteristics. However, we tend forget this choice when faced with a data set filled with continuous measurements. The most popular method is to use the squared Euclidean distance to determine similarity. This may work adequately when the data are organized into vectors of independent explanatory factors all of which are equally important features of the object or subject. However, when the vector has time-ordered structure, a dissimilarity measure that is based only the element-wise differences between vectors ignores the important structural properties of the data.  

There as been some work in adapting and developing clustering methods for longitudinal data in specific areas of applications such as psychology and genomics \cite{schneiderman1993,genolini2010, jones2001, muthen2010, mcnicholas2010}. Most of these methods involve fitting a finite mixture model with adjustments to the mean or covariance to make it more adaptable to longitudinal data. Others cluster subjects with a partition algorithm such as K-means \cite{macqueen1967,hartigan1979,} or partitioning around mediods (PAM) \cite{kaufman1990} using a dissimilarity measure developed specifically to take the time-ordered structure of longitudinal data into account. Applying cluster methods to longitudinal data has gained popularity in other areas of application. For example, there is a growing literature about clustering distinct childhood body mass index growth patterns over time and which early life factors that contribute to following the patterns \cite{pryor2011,carter2012}. 

The main goal of a longitudinal study is to study individual change over time. However, none of the popular methods explicitly group subjects based on the shape of the pattern over time. Although they claim to group similar patterns, the similarity is not defined in terms of any one specific feature of the data. In the time series literature, Wang et. al. suggested estimating global features of time series processes such as the trend, seasonality, autocorrelation, and kurtosis to use when clustering \cite{wang2006}. Some of these features apply to longitudinal data. The trend over time in particular is of interest. We go a step further and break trend into two components: level and shape. 

In many circumstances, these two characteristics provide distinct information that can elicit different actions. For example, when investing in stocks, the magnitude of the share price provides information about value and practices of a company. While that information is important, the feature that drives a decision to buy or sell shares of a stock is the historical change in price over time. Similarly, a high average body mass index triggers concerns about health, but knowing how that value has changed over time indicates whether the problem is improving or worsening. When studying gender inequality in salaries, it is important to distinguish discrimination in starting salaries  and systematic discrimination over time. I distinguish between these two features of a trajectory when thinking about clustering as they provide different information.  

Despite the interest in the shape of the change over time, too many researchers just apply a standard clustering method without much thought on the implications on their results. Not only will the groups be driven by the level rather than the shape of the trajectories, the means within each cluster may not be representative of any individual's trajectory since it may be an average of trajectories with different shapes at the same level. Most importantly, the shape of the mean curves does not represent the shapes of the individuals. It is common for authors make these generalizations when the subtitles of a method are not clearly understood \cite{windle2004,mulvaney2006,broadbent2008,pryor2011,mccoy2010}. 

Imagine a measuring alcohol consumption over time for a sample of adolescents. The cohort may contain individuals who do not drink at all as well as some that have moderate or heavy drinking habits. Besides different levels of alcohol consumption, there may be patterns of changing behavior such as escalation, reduction, or stabilization. All of these patterns can occur within every level of alcohol consumption. Figure \ref{fig:3-1} illustrates a possible scenario of four individuals, two of which drink heavily and two of which are low to moderate drinkers. Within both level categories, one of the individuals has escalating behavior and the other is reducing alcohol consumption. We assume linear change for simplicity. The shape of the curve, the slope in this case, is independent of the consumption level. If we use K-means on the observed vectors, we find two clusters that are determined by the level of consumption; the two heavy drinkers are grouped together and the two moderate consumers in another group. Consequently, the mean trajectory for each group is a horizontal line, which disguises the fact that the alcohol consumption is not stable for any of the individuals. Although the level of alcohol consumption is important for public health, the knowledge of whether or not behavior is escalating or decreasing in a population informs whether or not and when interventions need to be implemented. This is a trivial, highly simplified example, but it illustrates the type of results that occurs in practice \cite{mccoy2010}.

\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Exp}
\end{center}
\caption{Graph of linear trajectories representing the hypothetical alcohol consumption of four individuals.}
\label{fig:3-1}
\end{figure}

There are factors that influence the level and perhaps other factors that affect the shape of the patterns and it is important to separate them to as to not muddy the results. If the goal is to detect and compare groups based on their temporal change over time, we need methods that answer the following research questions: Are there distinct shape patterns in the data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? In the next section, I discuss two popular standard clustering methods in the context of cluster based on shape and highlight the situations in which these methods fail to address the research questions above. 

\section{Limitations of standard clustering methods}
Two standard methods for clustering multivariate data include partition methods based on a dissimilarity measure such as the Euclidean distance and model-based methods such as finite mixture models. These methods were introduced in the first chapter of this thesis and I now illustrate the limitations of these methods in answering the research questions above. 

I assume that there are $n$ subjects such that for subject $i$, we have $m_{i}$ repeated measures of an outcome of interest. I denote the vector of measured outcomes as $\B y_{i}=(y_{i1},...,y_{im_{i}})$ for $i=1,...,n$ and $\B w_{i}\in \mathbb{R}^{q}$ as the design vector based on baseline variables that may impact group membership. Lastly, the corresponding time of the data collection for subject $i$ is $\B t_{i}=(t_{i1},...,t_{im_{i}})$. 

\subsection{Partitioning algorithms}
If the outcome is measured at the same time points for every subject such that $m_{i}=m$ and $\B t_{i}=\B t$ for all $i=1,...,n$, longitudinal data fit into a typical multivariate data framework where a partitioning method like K-means \cite{macqueen1967, hartigan1979} on the observed vector is typically appropriate to use. For the K-means algorithm, given the number of clusters, $K$, subjects are randomly assigned into $K$ groups. A mean vector, known as a centroid, is calculated for each group of vectors. Subjects are then reassigned to the group with the closest centroid in terms of the squared Euclidean distance. This process of calculating the centroid and then reassigning subjects is repeated until convergence. At convergence, subjects are assigned to one and only one group. This strict partitioning of the data is termed `hard' clustering. In other words, the K-means algorithm searches for $K$ centroid vectors $\B a=\{\B a_{1}, ...,\B a_{K}\}$ that minimize the following objective function
$$\frac{1}{n} \sum^{n}_{i=1}\min_{1\leq k\leq K}\|\B  y_{i}-\B a_{k}\|_{2}^{2}$$
where $\|\B y - \B x\|^{2}_{2}$ is the squared Euclidean distance between two vectors $\B y$ and $\B x$.

I now illustrate how this algorithm fails to address all three of the questions previously posed. Using the simple alcohol consumption example, we let $n=4$, and suppose we observe the average number of drinks per week for ten years, $\B t=(10,11,12,13,14,15,16,17,18,19,20)$. Let the observed outcome vectors be linear in trend as shown in Figure \ref{fig:3-1}. Note that there are two shape groups: increasing and decreasing. In this circumstance, the level and the shape are not strongly associated since there are different levels within each shape group. The K-means algorithm groups based on the squared Euclidean distance between individuals. The distances put into a symmetric distance matrix for these four subjects is listed in Table \ref{tab:3-1}. 
\begin{table}[h]
\begin{center}
\begin{tabular}{c|cccc}
&Low Decreasing& High Decreasing&Low Increasing&High Increasing\\
\hline
Low Decreasing&0&275.0&17.6&292.6\\
High Decreasing&275.0  &0 &  292.6 &17.6 \\                     
Low Increasing& 17.6 &292.6  &0   &275.0   \\          
High Increasing& 292.6 &17.6 &275.0   &0 
\end{tabular}
\end{center}
\caption{Squared Euclidean distance matrix for the hypothetical alcohol consumption vectors of four individuals: high level but slowly decreasing, high level but slowly increasing, low level but slowly decreasing, and low level but slowly increasing. }
\label{tab:3-1}
\end{table}

In terms of distance, the level dominates such that the subjects with the same level (high or low) are clearly the most similar with this measure. The distance between those with same shape have a distance of about fifteen times that of level. This clustering method detects distinct levels and if the distinct shapes do not coincide with the distinct levels, the K-means algorithm fails at grouping individuals based on shape and detecting the number of patterns. The mean curves over time for the two groups end up being horizontal even though the alcohol consumption is not stable over time for any of the individuals (Figure \ref{fig:3-1}).

Imagine if the shape of the trajectory were correlated with the vertical level where all of the heavy drinkers slowly decrease the number of drinks per week and the light drinkers increase over their teenage years, then K-means algorithm gives clusters based on level and thus on shape in this case (Figure \ref{fig:3-2}). This only works in cases where the trajectories with similar shapes also have similar levels, which may be the case in some applications, but for many data sets, this is not true. We have focused on the K-means algorithm right now, but these conclusions hold for the PAM algorithm when the Euclidean or squared Euclidean distance is used.
\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Exp2}
\end{center}
\caption{Graph of linear trajectories representing the hypothetical alcohol consumption of four individuals.}
\label{fig:3-2} 
\end{figure}

Lastly, if baseline factors are related to the cluster membership, the only option is to complete an analysis after the clustering algorithm is complete. One technique is to fit a multinomial logistic regression model for the group labels with baseline factors as the explanatory variables. This analysis automatically assumes that the group labels are known and fixed; it does not take into account the uncertainty in the group memberships and sample variability. Imagine a subject with an outcome measure that puts him/her on the boundary of two groups. A partitioning algorithm puts them in one and only one group without stating any uncertainty in that classification. Therefore, any inference should be done with caution as the standard errors are calculated conditional on cluster labels and do not take into account other sources of uncertainty.

\subsection{Finite mixture models}
In contrast to partition methods, finite mixture models provide a probability framework in which to take into account the uncertainty of group memberships and simultaneously estimate the relationship between baseline factors and groups. Additionally, the model is flexible to accommodate irregular sampling which frequently occurs in longitudinal studies. Mixture models are useful in modeling distinct subgroups in the population. An overview of finite mixture models is available in many texts \cite{everitt1981,mclachlan1988,mclachlan2000} and earlier in this thesis. A finite mixture model is a weight sum of component distributions. We assume a parametric form for these distributions and that parameters differ between components.

In general, I assume there are $K$ latent groups that occur in the population with frequencies $\pi_{1},...,\pi_{K}$. If subject $i$ is a member of the $k$th group, the observed data vector for that subject is given by
$$\B y_{i} = \BS\mu_{k}+\BS\epsilon_{i}\quad \BS\epsilon \sim N(0,\BS\Sigma_{k}).$$
Maximum likelihood estimation via the EM algorithm is used to estimate the model parameters and the posterior probability a subject belongs to each component. These probabilities provide a way to `soft' cluster subjects to groups as a subject belongs to every cluster to some degree. Subjects can then be `hard' clustered through assignment based on the maximum posterior probability. 

If the outcome vector is repeated measures over time, there are some necessary adjustments that need to be made to the mixture model. The outcome vectors $\{\B y_{i}\}$ may not have equal length and the measurements may not be observed at the same times between individuals. Therefore, we must impose structure on the mean vector and the covariance matrix. A regression structure can be used by assuming a linear model for the mean, $\BS\mu_k=\B x \BS\beta_k$. The matrices of explanatory variables, $\{\B x_{i}\}$, must include time components that can model the shape of the curve.  In terms of the covariance structure, most software packages restrict the structure to conditional independence, which can be problematic as shown in Chapter 2.

If the correlation structure is assumed conditional independence, then the likelihood function is based on the squared Euclidean distance between observations and the mean scaled by the estimated variance. Akin to the K-means algorithm, maximizing the likelihood with the original data vectors generally fails to group individuals by shape if shape and level are weakly dependent. If the level of the outcome vector is determined by a random intercept such that $\B y_{i} = \B x_{i}\BS\beta_{k} + \alpha_{i}+\BS\epsilon_{i}$ where $\alpha_{i}\sim N(0,\tau_k^{2})$ and $\BS\epsilon\sim N(0,\sigma^{2}_{k})$ for subjects in the $k$th group, then it is possible to model the variability of the level through the correlation stricture. In general, if the distribution of the level/intercept is known and can be correctly modeled within each shape group, then including that into the model should produce shape groups. In practice, the distributions are not known and clustering results are be sensitive to incorrect assumptions.

It only takes a simple simulation to show that the standard finite mixture fails to group individuals by shape, but Nagin's book leads people to believe that a finite mixture model assuming conditional independence ``lends itself to analyzing questions that are framed in terms of the shape of the developmental course of the outcome of interest''  and  ``focuses on identification of different trajectory shapes and on examining how the prevalence of the shape and the shape itself relate to predictors'' \cite{nagin2005}. 



\section{Methods extended for shape}
There has been work done in developing and adapting clustering methods for longitudinal data, but there has been much less discussion in the literature about the best methods to utilize when shape of pattern over time is the feature of interest. In this thesis, the term shape refers to the shape of the underlying functional pattern over time.

The shape of a function can be describe in many ways. The first derivative of the function with respect to time indicates the instantaneous rate of change at a point. The sign of the derivative indicates whether the function is increasing or decreasing at a point.  Local and global minima and maxima of the function are determined by where the first derivative is zero. Lastly, the second derivative provides information about the growth of the first derivative and thus whether the function is concave up or down at any point. 

These are all aspects of the shape and they may be important in different scientific settings. For example, when clustering genes, the location of the peaks in expression levels may be of interest so focusing local maxima may be useful when clustering \cite{luan2003}. In other circumstances, it may only be important whether the function is increasing or decreasing so that only the sign of the first derivative is needed \cite{phang2003}. 

I do not separate these components and define shape as the pattern of the function after disregarding the vertical level. There are a few ways to compare the shape of two function using this definition. One way is to calculate the first derivative as it removes the level and uniquely describes the shape of the remaining curve. However, it is difficult to estimate the derivative of a discretized, noisy version of the function. Another popular approach involves calculating the Pearson correlation coefficient between two vectors of discretized versions of the functions. If the shape is the same, the correlation is equal to 1. These two ideas have been implemented in a clustering framework for longitudinal data.

%%
\subsection{Derivative-based dissimilarity}
One approach to clustering on the basis of shape is to compare derivative functions. We do not directly observe the derivative function; therefore, researchers such as M\"{o}ller-Levet et. al. \cite{moller2003} and D'Urso \cite{d2000} suggested estimating the derivative function through differencing, calculating the slope of a linear interpolation between points via a difference quotient estimator, and then calculating the Euclidean distance between the two vectors from those estimates. This leads to calculating the dissimilarity between individuals based only on their estimated derivative curve ignoring the original level of the vector. However, many issues arise with this procedure. First, this method also requires regularly sampled data such that every subject is observed at the same times to compare derivative estimates from two vectors. Secondly, if we assume that the observed data is a realization of the model
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma^{2})$ then the difference quotient estimator of the derivative is $\hat{f}_{i}^{'}(\tau) = (y_ij+1-y_ij)/(t_{j+1}-t_j)$ for $\tau\in[ t_{j}, t_{j+1}]$. This estimator is unbiased for the difference quotient, but not for $f_{i}^{'}(\tau)$ with $E(\hat{f}_{i}^{'}(\tau)) = (f_i(t_{j+1})-f_i(t_j))/(t_{j+1}-t_j)$. However, if the variance of the random noise is substantial, this estimator results in highly inefficient estimates such that
$$\text{Var}(\hat{f}_{i}^{'}(\tau)) =  2\sigma^{2}/ (t_{j+1}-t_j)^{2},$$
assuming the sample times are fixed. The variance of the estimates are a function of the ratio between the standard deviation of the errors and the length of the time sampling increments. This suggests that maximizing the time between observations produces the most efficient estimates. Given a fixed observation period, maximizing the time lags results in one baseline and one follow up measurement at the end of the observation period. This may be adequate if the underlying function is linear; however, limiting the number of observations prevents the detection of complex functional shapes during the observation period. More observation points are required in the middle to accurately calculate the derivative of the function but at the expense of the variability of this estimator. Besides the issues of estimating individual derivative functions, this procedure compares the estimates directly without borrowing strength between individuals that are thought to have similar derivatives.

The main concern is how well these methods can cluster based on shape; therefore, it is important to determine the behavior of Euclidean distance using these estimated slopes and how well it can distinguish between two noisy curves in terms of their shape over time. Assume we have 3 subjects ($n=3$) observed at 10 times ($m=10$). Let $f_1(t) = f_2(t) =0$, $f_3(t)=a*t$, and $\sigma_1=\sigma_2=\sigma_3=\sigma$. Also, let $\B t_1 = \B t_2=\B t_3 = (\Delta_t,2\cdot\Delta_t,...,10\cdot\Delta_t)$. Then the outcome for individual $i$ at the $j$th observation time equals
$$y_{ij}=f_{i}(t_{ij})+\epsilon_{ij}\quad \epsilon_{ij}\sim N(0,\sigma^{2})$$
for $i=1,2,3$ and $j=1,...,10$. The vectors of calculated slopes between the realizations for each $\B y_1$, $\B y_2$, and $\B y_3$ are denoted as $\B d\B y_1$, $\B d\B y_2$, $\B d\B y_3$. Then we are interested in the probability of the event that the distance between $\B d\B y_1$ and $\B d\B y_2$, which are both generated from horizontal lines, is less than the distance between $\B d\B y_2$ and $\B d\B y_3$. Given $\sigma^2$, $\Delta_t$, $a$, we want to calculate 
$$P(\|\B d\B y_1-\B d\B y_2\|^2_2 < \|\B d\B y_2-\B d\B y_3\|^{2}_{2}).$$
Generating the data vectors, calculating the distance between calculated slopes, and comparing the distances 5000 times provides an approximation to the probability stated above. We completed this simulation for many conditions specified by a combination of $a = 0.25, 1, 5$, $\Delta_t = 0.5,1,1.5,2,2.5,3$, and $\sigma = 0.5,1,1.5,2,2.5,3$. The estimated probabilities are plotted against the ratio, $\sigma/\Delta_t$, for different slopes of $f_{3}(t)$ (Figure \ref{fig:3-3}). 

\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Deriv}
\end{center}
\caption{Estimate probabilities that the distance between $\B d\B y_{1}$ and $\B d\B y_{2}$, which share the same underlying horizontal function, is smaller than the distance between $\B d\B y_{1}$ and $\B d\B y_{3}$ for different ratios of standard deviation to length of time lags and slopes of the linear function underlying $\B y_{3}$.}
\label{fig:3-3} 
\end{figure}

We note that as the ratio increases, the probability of the two horizontal realizations being closer than the two vectors from different shapes decreases to 0.50. Therefore, if the variance is large in comparison to the sampling increments, it is a toss up as to whether this method correctly determines that two vectors were generated from the same shape. On the other hand, if the variance is small relative to the time between samples, the procedure will find the two horizontal lines more similar than two lines with different slopes. As the slope increases and thus the difference between shapes increases, the method correctly finds the horizontal lines more similar. Therefore, if the individuals are not observed frequently relative to the measurement error and the shapes drastically differ, this clustering procedure may behave appropriately by grouping individuals with similar patterns over time. 

\subsection{Correlation-based dissimilarity}
The Pearson correlation coefficient has been generally used to calculate dissimilarities between vectors based on the shape of the data  \cite{chouakria2007,  eisen1998}. Not only is it used in the multivariate data setting, Chiou and Li \cite{chiou2008} suggested using the functional version of correlation as a similarity measure to cluster functions with the same shape. This clustering approach requires densely collected longitudinal data and therefore, it is not applicable in our circumstance. However, this proposals have lacked an in-depth discussion about how well correlation discriminates between functional shapes.

In the multivariate setting, the Pearson correlation coefficient between two vectors can be the basis of a dissimilarity measure such as $d_{Cor}(\B x,\B y) = (1-Cor(\B x,\B y))/2$ where $$Cor(\B x,\B y) = \frac{\sum^{m}_{j=1}(x_{j}-\bar{x})(y_{j}-\bar{y})}{\sqrt{\sum^{m}_{j=1}(x_{j}-\bar{x})^{2}}\sqrt{\sum^{m}_{j=1}(y_{j}-\bar{y})^{2}}}.$$
Assuming there is no random error, the correlation should be close to 1, thus the dissimilarity equal to 0, when two vectors have the same shape such that one is a vertical shift of the other. It is also equal to 1 when the vectors are affine transformations of each other, $\B y = a\B x + b$. This means that two lines with different positive slopes are considered to have the same shape according to the correlation dissimilarity measure. Additionally, if the vectors are constant in that its graph is a horizontal line, the correlation coefficient will always be close to 0 and thus the distance equals to 1/2. This means that the Pearson correlation coefficient cannot detect two horizontal curves as having the same shape.  When functions are observed with noise the correlation is generally less than that for the err-free vector. Thus, this methods fails in many cases where the longitudinal vectors are noisy, stable over time, or scalar multiples of each other. These characteristics do not get discussed in the literature, but they have huge ramifications for clustering typical longitudinal data.

\section{Discussion}
The standard clustering methods generally fail to answer the three research questions posed at the beginning of this chapter: Are there distinct shape patterns in the data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? There have been some attempt to adjust the input into partition algorithms to focus on shape, but these methods based on quotient derivatives and the Pearson correlation coefficient generally fail when data observed equals a smooth function plus moderate variability. Additionally both methods require data to be collected at fixed time points for all of the subjects. Ideally, we want a method that exploits the properties of longitudinal data, which includes the possibility of unbalanced, irregular data.

We propose three extensions of existing method that fit this criteria while attempting to answer the research questions about shape. Before we discuss the proposed methodology, it is worth discussing the decomposition of a curve into level and shape and how that translates into a model. There are two common data-generating models that suggest groups are based on shape and individuals could differ in the vertical level within the group. In a functional data approach \cite{ramsay2002}, we assume that the $j$th observation for the $i$th individual at time $t_{ij}$ is
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma_{i}^{2})$ and $f_{i}$ is a random function sampled from a Hilbert space of square integrable functions on a real interval. In the typical longitudinal data analysis  approach \cite{diggle2002}, we assume that the $j$th observation for the $i$th individual at time $t_{ij}$ is
$$y_{ij}=\lambda_{i}+f_k(t_{ij})+\epsilon_{ij}$$
given the $i$th individual is in group $k$, where $\lambda_{i}$ adjusts the vertical level of an individuals' curve, $f_k$ is a continuous mean function for cluster $k$, and $\epsilon_{ij}$ is random error that may be correlated within subject $i$ but independent between subjects. If $\lambda_{i}$ is assumed random, then the model is considered a random intercept model. 

 In the next chapter, we present three new methods adapted from related literature that address issues of shape or pattern over time and apply them to growth trajectories data of children. Additionally, we compare the methods in a simulation study and discuss the advantages and disadvantages of using the methods in practice. 
	
