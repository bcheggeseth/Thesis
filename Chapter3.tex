\chapter{Shape-based clustering}
\label{chap:motivate}

Longitudinal data is collected with the intent of studying the change over time. One side effect of collecting this type of data is the inherent dependence between the repeated measures. The last chapter provided evidence that this nuisance cannot be ignored when clustering data using a finite mixture model. In this chapter, I discuss clustering methods with the research goals of longitudinal studies in mind. 

\section{Motivation}
There is no one way to cluster data. There are many characteristics of quantitative data on which to base a grouping. This is not only true for data sets. We constantly group products, people, and projects based on different qualities and characteristics. However, we tend to forget this subjective choice when faced with a data set filled with continuous measurements. Many automatically use squared Euclidean distance to determine similarity. This may work adequately when the data are organized into vectors of independent explanatory factors all of which are equally important features of the object or subject. However, when the vector has time-ordered structure, a dissimilarity measure that is based only the element-wise differences between vectors ignores this important structural property of the data.  

There has been some work in adapting and developing clustering methods for longitudinal data in the fields of behavior science and genomics \cite{schneiderman1993,genolini2010, jones2001, muthen2010, mcnicholas2010}. Most of these methods involve fitting a finite mixture model with adjustments to the mean or covariance to make it more adaptable to longitudinal data. Others cluster subjects with a partition algorithm using a dissimilarity measure developed specifically to take the time-ordered structure of longitudinal data into account. Applying cluster methods to longitudinal data has gained popularity in other areas of application. For example, there is a growing literature about clustering distinct childhood body mass index growth patterns over time with a view to determining life factors that contribute to patterns \cite{pryor2011,carter2012}. 

The main goal of a longitudinal study is to study individual change over time. However, none of the standard clustering methods adapted for longitudinal data explicitly group subjects based on the shape of the pattern over time. Although they claim to group similar patterns, the similarity is not defined in terms of any one specific feature of the data but rather just in terms of the difference in the vector elements. In the time series literature, \textcite{wang2006} suggested estimating and using global features of time series processes such as the trend, seasonality, autocorrelation, and kurtosis to define similarity when clustering. Some of these features apply to longitudinal data. The trend over time in particular is of considerable interest. We go a step further and break trend into two components: level and shape. 

In many circumstances, these two characteristics provide distinct information that can elicit different actions. For example, when investing in stocks, the magnitude of the share price provides information about the value and financial practices of a company. While that information is important, the feature that may drive a decision to buy or sell shares of a stock is the historical change in price over time. Similarly, a high average body mass index triggers concerns about health, but knowing how that value is changing over time indicates whether the problem is improving or worsening and if an intervention may be necessary. When studying gender inequality in salaries, it is important to differentiate between discrimination in starting salaries  and systematic discrimination over time. I distinguish between these two features of a trajectory when thinking about clustering as they provide different information.  

Despite the interest in the shape of the change over time, too many researchers apply a standard clustering method without  thought on the interpretation of their results. Not only are the groups driven by the level rather than the shape of the trajectories, but the means within a given cluster may not be representative of any specific individual's trajectory since it may be an average of trajectories with different shapes at the same level. It is common for authors applying these standard clustering methods to make incorrect conclusions about shape \cite{windle2004,mulvaney2006,broadbent2008,pryor2011,mccoy2010}. 

Imagine measuring alcohol consumption over time for a sample of young adults. This cohort may contain individuals who never drink as well as those that have moderate or heavy drinking habits. Besides different levels of alcohol consumption, there may be a variety of behavior patterns over time such as escalation, reduction, or stabilization. All of these patterns can occur at each level of alcohol consumption. Figure \ref{fig:3-1} illustrates a possible scenario of four individuals, two of who drink heavily and two of who are low to moderate drinkers. Within both levels, one of the individuals has escalating behavior and the other is reducing alcohol consumption over time. We assume linear change for simplicity. The shape of the curve, the slope in this case, is independent of the consumption level. If K-means is applied to the observed vectors, two clusters are discovered that are determined by the level of consumption with the two heavy drinkers grouped together and the two moderate consumers in another group. Consequently, the mean trajectory for each group is a horizontal line, which disguises the fact that the alcohol consumption is not stable for any of the individuals. Knowing what factors impact the alcohol consumption level is important for public health, but the knowledge of what type of people have escalating or reducing behavior in a population informs what and when interventions need to be implemented. This is a trivial, highly simplified example, but it illustrates the type of results that occur in practice \cite{mccoy2010}.

\begin{figure}[ht]
\centering
\includegraphics[width=4in]{Chp3Exp}
\caption{Graph of linear trajectories representing the hypothetical alcohol consumption of four individuals.}
\label{fig:3-1}
\end{figure}

There are factors that influence the level and perhaps other factors that affect the shape of the patterns and it is important to separate these two relationships so as to not muddy the interpretations. If the goal is to detect and compare groups based on their temporal change over time, we need methods that answer the following research questions: Are there distinct shape patterns in the data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? In the next section, I discuss two popular standard clustering methods in the context of cluster based on shape and highlight the situations in which these methods fail to address such research questions. 

\section{Limitations of standard clustering methods}
Two standard methods for clustering multivariate data include partition methods based on a dissimilarity measure (e.g. Euclidean distance) and model-based methods such as finite mixture models. These methods were introduced in the first chapter of this thesis and I now illustrate the limitations of these methods in answering research questions about shape. 

I assume that there are $n$ subjects such that for subject $i$, we observe $m_{i}$ repeated measures of an outcome of interest. I denote the vector of measured outcomes as $\B y_{i}=(y_{i1},...,y_{im_{i}})$ for $i=1,...,n$ and $\B w_{i}\in \mathbb{R}^{q}$ as the design vector based on baseline variables that may impact group membership. Lastly, the corresponding time of the data collection for subject $i$ is $\B t_{i}=(t_{i1},...,t_{im_{i}})$. 

\subsection{Partitioning algorithms}
If the outcome is measured at the same times for every subject such that $m_{i}=m$ and $\B t_{i}=\B t$ for all $i=1,...,n$, longitudinal data fit into a typical multivariate data framework where a partitioning method like K-means \cite{macqueen1967, hartigan1979} on the observed vector is typically appropriate to use. See Chapter \ref{chap:intro} for details of the iterative algorithm.

I now illustrate how this algorithm fails to address all three of the questions previously posed. Using the simple alcohol consumption example, let $n=4$, and suppose the average number of drinks per week is collected over a ten year period, $\B t=(15,16,...,24,25)$. Let the observed outcome vectors be linear in trend as shown in Figure \ref{fig:3-1}. Note that there are two shape groups: increasing and decreasing. In this circumstance, the level and the shape are not strongly associated since there are different levels within each shape group. The K-means algorithm groups based on the squared Euclidean distance between individuals. The distances between these four subjects are listed in Table \ref{tab:3-1}. 
\begin{table}[ht]
\centering
\begin{tabular}{c|cccc}
&Low Decreasing& High Decreasing&Low Increasing&High Increasing\\
\hline
Low Decreasing&0&275.0&17.6&292.6\\
High Decreasing&275.0  &0 &  292.6 &17.6 \\                     
Low Increasing& 17.6 &292.6  &0   &275.0   \\          
High Increasing& 292.6 &17.6 &275.0   &0 
\end{tabular}
\caption{Squared Euclidean distance matrix for the hypothetical alcohol consumption vectors of four individuals: high level but slowly decreasing, high level but slowly increasing, low level but slowly decreasing, and low level but slowly increasing. }
\label{tab:3-1}
\end{table}

The level dominates such that the subjects with the same level (high or low) are clearly the most similar using this dissimilarity measure. The distance between those with same shape have a distance of about fifteen times that of distances across levels. Clustering using Euclidean distance thus detects distinct levels, and if the distinct shapes do not coincide with the distinct levels, the K-means algorithm fails at grouping individuals based on shape and detecting the number of trend patterns. The mean curves over time for the two groups end up being horizontal even though the alcohol consumption is not stable over time for any individual.

Imagine if the shape of the trajectory were correlated with the vertical level where all of the heavy drinkers slowly decrease the number of drinks per week and the light drinkers increase over the years; then, K-means algorithm results in clusters based on level and thus on shape in this case (Figure \ref{fig:3-2}). The algorithm detects the shape groups only when the trajectories with similar shapes also have similar levels, which may be the case in some applications, but is not true for many data sets. I have focused on the K-means algorithm right now, but these conclusions hold for the PAM algorithm when Euclidean or squared Euclidean distance is used.
\begin{figure}[ht]
\centering
\includegraphics[height=4in]{Chp3Exp2}
\caption{Graph of linear trajectories representing the hypothetical alcohol consumption of four individuals where shape and level are strongly associated.}
\label{fig:3-2} 
\end{figure}

Lastly, if baseline factors are related to the cluster membership, the only option is to complete an analysis after the clustering algorithm is complete. One technique is to fit a multinomial logistic regression model for the group labels with baseline factors as the explanatory variables. This analysis automatically assumes that the group labels are known and fixed; it does not take into account the uncertainty in the group memberships. Therefore, any inference should be done with caution as the standard errors are calculated conditional on cluster labels and do not take into account other sources of uncertainty.

\subsection{Finite mixture models}
In contrast to partition methods, finite mixture models provide a probability framework in which to take into account the uncertainty of group memberships and simultaneously estimate the relationship between baseline factors and groups. Additionally, the model is sufficiently flexible to accommodate irregular sampling which frequently occurs in longitudinal studies. A finite mixture model is a weighted sum of component distributions. I assume a parametric form for these distributions and that parameters differ between components.

In general, I assume there are $K$ latent groups that occur in the population with frequencies $\pi_{1},...,\pi_{K}$. If subject $i$ is a member of the $k$th group, the observed data vector for that subject is given by
$$\B y_{i} = \BS\mu_{k}+\BS\epsilon_{i}\quad \BS\epsilon_{i} \sim N(0,\BS\Sigma_{k}).$$
Maximum likelihood estimation via the EM algorithm is used to estimate the model parameters and the posterior probabilities of whether a subject belongs to each component. These probabilities provide a way to soft cluster subjects to groups as a subject `belongs' to every cluster with some probability. Subjects can then be hard clustered through assignment based on the maximum posterior probability. 

If the outcome vector is repeated measures over time, there are some necessary adjustments that need to be made to the mixture model. The outcome vectors may not have equal length and the measurements may not be observed at the same times between individuals. Therefore, structure must be imposed on the mean vector and the covariance matrix. A regression structure can be used by assuming a linear model for the mean, $\BS\mu_k=\B x \BS\beta_k$. The design matrices based on explanatory variables $\B x_{i}$ must include time components that can model the shape of the curve.  In terms of the covariance structure, most software packages restrict the structure to conditional independence, which can be problematic as shown in Chapter \ref{chap:misspecify}.

If conditional independence is assumed for the correlation structure, then the likelihood function is based on squared Euclidean distance between observations and group mean scaled by the estimated variance. Akin to the K-means algorithm, maximizing the likelihood with the original data vectors generally fails to group individuals by shape if shape and level are weakly dependent. If the level of the outcome vector is largely determined by a random intercept such that $\B y_{i} = \lambda_{i}\B 1+ \B x_{i}\BS\beta_{k}  +\BS\epsilon_{i}$ where $\lambda_{i}\sim N(0,\tau^{2}_{k})$ and $\BS\epsilon_{i}\sim N(0,\sigma^{2}_{k}I)$ for subjects in the $k$th group, then it is possible to model the variability of the level through the correlation structure. In general, if the distribution of the level within a shape group is known and can be correctly modeled within each shape group, then including that into the model can produce the shape groups. In practice, such distributions are not known and clustering results are sensitive to incorrect assumptions.

It only takes a simple simulation to show that the standard finite mixture fails to group individuals by shape, but Nagin's book leads people to believe that a finite mixture model assuming conditional independence ``lends itself to analyzing questions that are framed in terms of the shape of the developmental course of the outcome of interest''  and  ``focuses on identification of different trajectory shapes and on examining how the prevalence of the shape and the shape itself relate to predictors'' \cite{nagin2005}. This shoes a gap in the understanding present in the literature.

\section{Methods extended for shape}
There has been work done in developing and adapting clustering methods for longitudinal data, but there has been much less discussion in the literature about the best methods to utilize when shape of pattern over time is the feature of interest. In this dissertation, the term shape refers to the shape of the underlying functional pattern over time.

There are many features that could describe the shape of a function. The first derivative of the function with respect to time provides the instantaneous rate of change at a point in time. The sign of the derivative indicates whether the function is increasing or decreasing at a point.  Local and global minima and maxima of the function are determined by where the first derivative is zero. Lastly, the second derivative provides information about the growth of the first derivative and thus whether the function is concave up or down at any point. 

These are all aspects of the shape and they may be important in different scientific settings. For example, when clustering genes, the location of the peaks in expression levels may be of interest so focusing on local maxima may be useful when clustering \cite{luan2003}. In other circumstances, it is only necessary to know whether the function is increasing or decreasing so that only the sign of the first derivative is needed \cite{phang2003}. 

For this thesis, I define shape as the pattern of the function after disregarding the vertical level. There are a few ways to compare the shape of two function using this definition. One way is to compare the first derivatives as differentiation removes the level and uniquely describes the shape of the remaining curve. However, it is difficult to estimate the derivative of a discretized, noisy version of the function. Another popular approach involves calculating the Pearson correlation coefficient between two vectors of discretized versions of the functions. These two ideas have been implemented in a clustering framework for longitudinal data.


\subsection{Derivative-based dissimilarity}
One approach to clustering on the basis of shape is to compare derivative functions. The first derivative of a function describes the rate of change while ignoring the intercept or level of the original function. With longitudinal data, we do not directly observe the derivative function for each individual. Assume that the $j$th observed outcome for individual $i$ at time $t_{ij}$ is a realization of the model
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
for $i=1,...,n$ and $j=1,...,m_{i}$ where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma_{i}^{2})$, $f_{i}$ is a subject-specific differentiable, continuous function, and $m_{i}$ is relatively small (usually around 5 to 10). If the observation times are consistent across individuals with $m_{j}=m$ and $t_{ij} = t_{j}$ for $j=1,..,m$ and $i=1,...,n$, \Textcite{moller2003} and \textcite{d2000} independently suggested estimating the derivative of the underlying smooth function via the difference quotient by calculating the slope of a linear interpolation between adjacent repeated measures,
$$\hat{f}_{i}^{'}(t_{j}) = (y_{i\;j+1})-y_{ij})/(t_{j+1}-t_j).$$
By the mean value theorem, this is an unbiased estimate of the true derivative, $f_{i}^{'}(\tau),$ at a point $\tau\in[t_{j},t_{j+1}]$ such that
$$E(\hat{f}_{i}^{'}(t_{j})) = f_i(t_{j+1})-f_i(t_j))/(t_{j+1}-t_j) =f_{i}^{'}(\tau). $$
However, the estimate is highly variable if $\sigma_{i}^{2}$ is large since
$$\text{Var}(\hat{f}_{i}^{'}(\tau)) =  2\sigma^{2}_{i}/ (t_{j+1}-t_j)^{2}.$$

Large variability in the estimates impacts the cluster analysis if enough estimates are far from the true derivative. The dissimilarity between two individuals is measured as the squared Euclidean distance between the vectors of difference quotients. High variability can lead to high dissimilarity for individuals with similarly underlying shape. One way to minimize the variance is to maximize the time between observations. Observing only two observations, one at baseline and another at the end of the follow-up period, minimizes the variance but at the expense of observing the rate of change during the follow-up period. If the times of observation are densely sampled, a functional approach smoothes out the noise using splines to estimate the function and then the derivative function \cite{tarpey2003}. In either circumstance, the derivatives are independently estimated for each individual and there is no direct way to borrow strength between individuals to better estimate the derivative even if some individuals are thought to have a common shape.

Our main concern is how well these methods cluster based on shape; therefore, it is important to determine the behavior of the Euclidean distance using these estimated derivatives and how well it can distinguish between two noisy curves in terms of their shape over time. Assume there are three subjects ($n=3$) observed ten times ($m=10$) uniformly at intervals of $\Delta$, $\B t_1 = \B t_2=\B t_3 = (\Delta,2\cdot\Delta,...,10\cdot\Delta)$. Two subjects have the same horizontal shape over time and the third has a positive slope, $f_1(t) = f_2(t) =0$, $f_3(t)=a*t$, and the variability of the noise is the same amongst the subjects, $\sigma_1=\sigma_2=\sigma_3=\sigma$. The outcomes for individual $i$ at the $j$th observation time equals
$$y_{ij}=f_{i}(t_{j})+\epsilon_{ij}\quad \epsilon_{ij}\sim N(0,\sigma^{2})$$
for $i=1,2,3$ and $j=1,...,10$. The vectors of difference quotients for the three subjects are denoted as $\B d\B y_1$, $\B d\B y_2$, $\B d\B y_3$. I am interested in the probability of the event that the distance between $\B d\B y_1$ and $\B d\B y_2$, which are both generated from the same horizontal shape, is less than the distance between $\B d\B y_2$ and $\B d\B y_3$, which are generated from different shapes. Given values for $\sigma$, $\Delta$, $a$, I empirically estimate
$$P(\|\B d\B y_1-\B d\B y_2\|^2_2 < \|\B d\B y_2-\B d\B y_3\|^{2}_{2})$$
by generating data for the three subjects 5000 times. I complete this simulation under many conditions specified by a combination of $a = 0.25, 1, 5$, $\Delta = 0.5,1,1.5,2,2.5,3$, and $\sigma = 0.5,1,1.5,2,2.5,3$. The estimated probabilities are plotted against the ratio, $\sigma/\Delta$, for different slopes of $f_{3}(t)$ (Figure \ref{fig:3-3}). 

\begin{figure}[ht]
\centering
\includegraphics[width=5in]{Chp3Deriv}
\caption{Empirical probability that the distance between $\B d\B y_{1}$ and $\B d\B y_{2}$, which share the same underlying horizontal function, is smaller than the distance between $\B d\B y_{1}$ and $\B d\B y_{3}$ for different ratios of standard deviation to length of time lags ($\sigma/\Delta$) and slopes of the linear function underlying $\B y_{3}$ based on 5000 replications.}
\label{fig:3-3} 
\end{figure}

I note that as the ratio of variance to time interval increases, the probability of the two horizontal observed vectors being closer than the two vectors from different shapes decreases to 0.50. Therefore, if the variance is large in comparison to the sampling increments, it is a toss up as to whether the distance between the difference quotients correctly determines which vectors were generated from the same shape. On the other hand, if the variance is small relative to the time between samples, the procedure finds the two horizontal lines more similar than two lines with different slopes most of the time. As the slope increases and thus the difference between shapes increases, the method correctly determines similarity. There is a trade off of bias and variance so if the individuals are not observed frequently relative to the measurement error and degree to which the shapes drastically differ, this clustering procedure may behave appropriately by grouping individuals with similar patterns over time. 

\subsection{Correlation-based dissimilarity}
Another approach is to calculate the Pearson correlation coefficient which is invariant to trajectory level \cite{chouakria2007, eisen1998, chiou2008}. Despite the wide use, there has been little discussion about how well the correlation does to discriminate between shapes in this context. In the multivariate setting, one dissimilarity measure based on the Pearson correlation coefficient between two comparable vectors is 
$$d_{Cor}(\B x,\B y) = 1-Cor(\B x,\B y)$$ 
where $$Cor(\B x,\B y) = \frac{\sum^{m}_{j=1}(x_{j}-\bar{x})(y_{j}-\bar{y})}{\sqrt{\sum^{m}_{j=1}(x_{j}-\bar{x})^{2}}\sqrt{\sum^{m}_{j=1}(y_{j}-\bar{y})^{2}}}.$$
This measure takes values between 0 and 2 with extremes attained only when there is perfect linear positive or negative correlation. If there is zero noise, a vector with an underlying non-constant functional shape of $f(t)$ is perfectly positively correlated with a vector with functional shape of $a\cdot f(t) + b$ where $a>0$ and $b\in\mathbb{R}$. As a result, two vectors that have the same shape over time but at different levels have a dissimilarity of zero since the correlation coefficient equals 1. However, two vectors with underlying functions that are scalar multiples of each other are also placed in the same cluster despite having drastically different shapes. While this method succeeds in grouping individuals with the same shape at different levels, it fails in the sense that is also groups individuals with different shapes. If the function is observed with error such that $\epsilon_{ij}\sim(0,\sigma_{i}^{2}),$ the magnitude of the correlation is generally deflated such that the dissimilarity moves closer to the middle value of 1.

The correlation between two constant functions is undefined. With noise, the expected dissimilarity value is 1 with variation dependent on the number of observations. In other words, the Pearson correlation coefficient does not consistently detect two horizontal curves to have the same shape especially with a small number of observations, which is typical in longitudinal data. This dissimilarity measure fails to detect similar underlying shapes when $\sigma_{i}^{2}$ is large, the number of observations is low, and the data set includes constant or stable patterns over time. 

The related cosine-angle dissimilarity measure has also been used to determine the similarity of two vector profiles \cite{eisen1998}. The measure is the uncentered version of the Pearson correlation dissimilarity,
$$d_{Cos}(\B x,\B y) = 1- \frac{\sum^{m}_{j=1}x_{j}y_{j}}{\sqrt{\sum^{m}_{j=1}x_{j}^{2}}\sqrt{\sum^{m}_{j=1}y_{j}^{2}}}.$$
Since the mean is not subtracted from the vector elements, it is defined for two vectors based on constant functions. However, the measure is invariant to scaling transformations of the vectors and not invariant to vertical shifts, which are undesirable properties for a dissimilarity measure based on our definition of shape similarity. 

\section{Discussion}
The standard clustering methods generally fail to answer the three research questions posed at the beginning of this chapter: Are there distinct shape patterns in the data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? There have been some attempt to adjust the input vector to group by shape, but these methods based on difference quotients and the Pearson correlation coefficient generally fail when data observed equals a smooth function plus moderate variability. Additionally, both methods require data to be collected at fixed time points for all of the subjects. Ideally, we want a method that exploits all of the properties of longitudinal data, which includes the possibility of sporadic, irregularly sampled data.

In the next chapter, I propose three extensions of existing method that attempt to answer the research questions above for real longitudinal data sets. Before I present the proposed methodology, it is worth discussing two common data-generating models that allow a group structure based on shape and individuals differ in the vertical level within the group. In a functional data approach \cite{ramsay2002}, we assume that the $j$th observation for the $i$th individual at time $t_{ij}$ is
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
for $i=1,...,n$ and $j=1,...,m_{i}$ where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma_{i}^{2})$ and $f_{i}$ is a random function sampled from a Hilbert space of square integrable functions on a real interval. 

In a typical longitudinal data analysis approach \cite{diggle2002}, we assume that the $j$th observation for the $i$th individual at time $t_{ij}$ in group $k$ is
$$y_{ij}=\lambda_{i}+\mu_k(t_{ij})+\epsilon_{ij}$$
for $i=1,...,n$ and $j=1,...,m_{i}$ where $\lambda_{i}$ adjusts the vertical level of an individual's curve, $\mu_k(t)$ is a continuous mean function of time for cluster $k$, and $\epsilon_{ij}$ is random error that may be correlated within subject $i$ but independent between subjects. If $\lambda_{i}$ is assumed random, then the model is considered a random intercept model within clusters. 

In the next chapter, I present three new methods adapted from related literature that address clustering based on shape over time and then compare them with the methods discussed in this chapter.	
