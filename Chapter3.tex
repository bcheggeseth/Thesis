\documentclass[12pt]{article}
\usepackage{fullpage,amsmath,amsfonts,graphicx,amsthm}
\usepackage{setspace}
\title{Motivation Chapter}
\author{Brianna C. Heggeseth}

\newtheorem{theorem}{Theorem}
  \newcommand{\B}[0]{\mathbf}
    \newcommand{\bs}[0]{\boldsymbol}

\begin{document}
\doublespace
\maketitle
\section{Introduction}
 In this chapter, I motivate clustering longitudinal data based on the {\em shape} of the trajectory over time by highlighting interpretations of standard clustering methods present in the literature. Next, I review the standard clustering methods and discuss their limitations to achieve this goal. Finally, I close by discussing some methods of clustering designed to particularly focus on shape. 

\section{Motivation}
A key advantage of a longitudinal study is its ability to measure individual change over time and to distinguish between ``aging'' affects and cohort variability. Typically longitudinal data analysis often involves data exploration by first plotting the outcome as a function of time, modeling the relationship between the outcome and explanatory variables which includes a variable for time via marginal or mixed effects models, estimating parameters through generalized estimating equations (GEE) or maximum likelihood (ML) and interpreting coefficients, and finally plotting the resulting mean outcome as a function of time for fixed values for other explanatory variables. The graph of the mean is often falsely interpreted as `representative' of the entire data sample when in fact no individual may follow that exact path. In contrast, the conditional mean for cross-sectional data is usually a good summary of the relationships in the data and it is well understood that the result is an average over all individuals with their unique combinations of explanatory variables. However, in the case of longitudinal data, since we observe individuals at many time points, it is more tempting to interpret the mean curve as representing a `typical' person. \\\\
If the sample is heterogenous with multiple relationships between explanatory variables such as time and the outcome, it is best to model partition the data into more homogenous clusters before estimating associations. Given the traditional analysis procedure in the standard longitudinal data literature, it is natural to expect applied researchers to interpret cluster results of longitudinal data by comparing cluster `representative' curves on a graph. These cluster representatives are usually simple graphs of the mean or median outcome vectors within each cluster or the mean outcome as a function of time calculated using estimated parameters. But, the same mistakes are made by summarizing the individual curves within the clusters using the one `representative.' More importantly, researchers use the shape of the `representative' curves to summarize properties of the individual curves in the clusters \cite{windle2004,mulvaney2006,broadbent2008,pryor2011,mccoy2010}. Now there are two problems with this situation; first, we have established that incorrect inferences are made based on a `representative' curve  even though it is an average over all the relationships. Second, it does not make sense to summarize clusters by the shape of the curves if the goal of the cluster analysis was not to group similarly shaped curves. The majority of the methods currently used to cluster longitudinal data do not group individuals based on the shape of their trajectory over time. Often conventional methods group individuals based on the Euclidean distance between two vectors that results in the magnitude of the outcome measures (vertical level of the curve) driving cluster formation whether or not that it is the main interest. This may result in clusters with potentially drastically different shapes and the `representative' curve for each cluster now being an average of all of the shapes and levels and not necessarily an accurate summary of the cluster members. \\\\
That is not to say that the conventional methods will never detect clusters based on the temporal change over time. If the shape of the trajectory is highly dependent on the vertical level and thus each cluster has a distinct level and shape combination, then the standard cluster methods likely detect these discrete groups. In this case, it is more reasonable to summarize the individuals based on the estimated level and shape of the representative trajectories for each clusters. But if any of the clusters share a similar shape, keeping them in their own cluster fails to exploit the opportunity to borrow strength from other subjects to better estimate the common shape.\\\\
On the other hand, if the shape of trajectories is only weakly dependent of the level if at all, traditional methods will cluster individuals on the most discriminating feature which is typically the level. For example, imagine a set of alcohol consumption trajectories for a set of adolescents. The cohort may contain individuals who do not drink at all as well as some that have moderate or heavy drinking habits. Besides different levels of alcohol consumption, there may be patterns of changing behavior such as escalation, reduction, or stabilization. All of these patterns can occur within every level of alcohol consumption. Figure \ref{fig:Chp3Exp} illustrates a possible scenario of four individuals, two of which drink heavily and two of which are low to moderate drinkers. Within both level categories, one of the individuals has escalating behavior and the other has a reduction. We assume linear change for simplicity. The shape of the curve, the slope in this case, is independent of the consumption level. If we use a standard clustering method such as K-means on the observed vectors to partition the four individuals into two groups, we find the clusters are determined by the level of consumption; the two heavy drinkers are grouped together and the two moderate in another group (Figure \ref{fig:Chp3Exp}). Consequently, the mean trajectory for each group is a horizontal line, which suggests alcohol consumption is stable for all individuals. This is a trivial, highly simplified example, but it illustrates the type of results that can occur in practice \cite{mccoy2010}. Although level of consumption is important to many public health officials, the knowledge of whether or not behavior is escalating or decreasing in a population informs whether or not and when interventions need to be implemented. In many circumstances, the level is one aspect of interest, but studying the shape is a different aspect that answers other questions. For example, stock market prices, gene expression intensities, and hormone concentrations all have meaning in and of themselves, but how they change over time describes a separate process. There are factors that influence the level and perhaps other factors that affect the change patterns and it is important to separate them to as to not muddy the results to the research question. If the goal is to detect and compare groups based on their temporal change over time, we need methods that answer the following research questions: Are there distinct shape patterns in the data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory?\\\\
In the next section, I discuss this two standard clustering methods in the context of cluster based on shape and highlight the situations in which these methods fail to address the research questions above. 
\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Exp}
\end{center}
\caption{Left: Linear trajectories (solid) representing the hypothetical alcohol consumption of four individuals. Right: Mean curves based on clustering the four individuals into two groups using standard a partitioning method such as K-means.}
\label{fig:Chp3Exp}
\end{figure}
\section{Standard Methods}
The two standard methods for clustering multivariate data include partition methods based on a dissimilarity measure such as Euclidean distance and model-based methods such as finite mixture models. These methods were introduced in the first chapter of this thesis and I now illustrate the limitations of these methods in answering the research questions above. I assume that there are $n$ subjects such that for subject $i$, we have $m_{i}$ repeated measures of an outcome of interest. I denote the vector of measured outcomes as $\B y_{i}=(y_{i1},...,y_{im_{i}})$ for $i=1,...,n$ and $\B z_{i}\in \mathbb{R}^{q}$ as the vector of baseline variables that may impact group membership. Lastly, the corresponding time of the data collection for subject $i$ is $\B t_{i}=(t_{i1},...,t_{im_{i}})$. 
\subsection{Partitioning algorithms}
If the outcome is measured at the same time points for every subject such that $m_{i}=m$ and $\B t_{i}=\B t$ for all $i=1,...,n$, longitudinal data fit into a typical multivariate data framework where a partitioning method such as K-means \cite{macqueen1967, hartigan1979} or partitioning around medoids (PAM) \cite{kaufman1990} on the observed vector is typically appropriate to use. For the K-means algorithm, given the number of clusters, $K$, subjects are randomly assigned into $K$ groups. The mean vector, known as the centroid, is calculated for each group of vectors. Subjects are then reassigned to the group with the closest centroid based on the Euclidean distance. This process of calculating the centroid and then reassigning subjects is repeated until convergence. At the end of the algorithm, subjects are a member of one and only group, which is termed `hard' clustering. In other words, the K-means algorithm searches for $K$ centroid vectors $\B a=\{\B a_{1}, ...,\B a_{K}\}$ that minimize the following objective function
$$\frac{1}{n} \sum^{n}_{i=1}\min_{1\leq k\leq K}\|\B  y_{i}-\B a_{k}\|_{2}^{2}$$
where $\|\B y - \B x\|^{2}_{2}$ is the squared Euclidean distance between two vectors $\B y$ and $\B x$. I now illustrate how this algorithm fails to address all three of the questions posed above. Using the simple alcohol consumption example, we let $n=4$, and suppose we observe the average number of drinks per week for ten years, $\B t=(11,12,13,14,15,16,17,18,19,20)$. Let the observed outcome vectors be the linear as shown in Figure \ref{Chp3Exp}.
Note that there are two shape groups: increasing and decreasing. In this circumstance, the level and the shape are not strongly related since there are different levels within each shape group. Now the K-means algorithm groups based on the Euclidean distance between individuals and the symmetric distance matrix for these four subjects is listed in Table \ref{tab:dist}. 
\begin{table}[h]
\begin{center}
\begin{tabular}{c|cccc}
&Low Decreasing& High Decreasing&Low Increasing&High Increasing\\
\hline
Low Decreasing&0&12.65&3.69&13.77\\
High Decreasing&12.65  &0 &  12.55 &3.69 \\                     
Low Increasing& 3.69 &12.55  &0   &12.65   \\          
High Increasing& 13.77 &3.69 &12.65   &0 
\end{tabular}
\end{center}
\caption{Euclidean distance between the observed alcohol consumption vectors for four individuals: high level but slowly decreasing, high level but slowly increasing, low level but slowly decreasing, and low level but slowly increasing. }
\label{tab:dist}
\end{table}
In terms of distance, the level dominates and the subject with the same level (high or low) are clearly the closest in terms of this common distance metric. The distance between those with same shape have a distance of about four times that of the closest. This clustering method detects distinct levels and if the distinct shapes don't concur with the distinct levels, this method fails at grouping individuals based on shape and detecting the number of patterns as seen in the cluster means (Figure \ref{fig:Chp3Exp}). However, if the shape of the trajectory is correlated with the vertical level in the circumstance where all of the heavy drinkers slowly decrease the number of drinks per week and the low drinks increase over their teenage years, then K-means algorithm gives you clusters based on level and thus on shape (Figure \ref{fig:Chp3Exp2}). This  only works in cases where the trajectories with similar shapes also have similar levels, which may be the case in some applications, but for many data sets, this is not true. We have focused on the K-means algorithm right now, but these conclusions hold for the PAM algorithm when the Euclidean or squared Euclidean distance is used.\\\\
\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Exp2}
\end{center}
\caption{Left: Linear trajectories (solid) representing the hypothetical alcohol consumption of four individuals. Right: Mean curves based on clustering the four individuals into two groups using standard a partitioning method such as K-means.}
\label{fig:Chp3Exp2} 
\end{figure}
Lastly, if baseline factors are related to the cluster membership, based on level or shape, post-cluster analysis can be done with the cluster labels as the outcome variable. Once the algorithm is complete, subjects are partitioned into groups and labeled accordingly. In practice many suggest doing a one-way ANOVA on the K groups assuming normality for the factors (Creating Market Insight: How Firms Create Value from Market Understanding). Another technique is to assume a multinomial logistic regression model for the labels and estimate parameters using observed baseline variables. This analysis automatically assumes that the group labels are known and fixed; it does not take into account the the clustering results are dependent on the input data, which may or may not represent the variability in level and shapes in the population and the uncertainty in the cluster labels. A subject could be on the edge of two groups and a partitioning algorithm has to put them in only on group. Therefore, any inference based on calculated standard errors should be done with caution as the standard errors are calculated conditional on cluster labels and do not take into account other sources of uncertainty.
\subsection{Finite Mixture Models}
If the time of data collection is inconsistent among subjects, any dissimilarity measure based on the data vectors cannot be used directly since the vectors may differ in length and the elements of the vectors may not be comparable. If the difference in length is due to missing values, some attempt to use weights and adjustments in the calculation of the distance between two vectors or impute the missing observations \cite{genolini2010}. For many longitudinal data sets, there is some variability in when they were able to interview subjects in addition to the problem of subjects missing interviews. In this case, the primary method of clustering is based on mixture models which are useful in modeling distinct subgroups in the population which are referred to as components and estimating the component membership probabilities. An overview of finite mixture models is available in many texts \cite{everitt1981,mclachlan1988,mclachlan2000} and earlier in this thesis. Data in each component are assumed to follow a parametric distribution, such as Gaussian, such that the parameters may vary between components. To illustrate, if there are $K$ latent relationships between the outcome $\B y$ and the explanatory variables, $\B x$, that occur with frequencies $\pi_{1},...,\pi_{K}$ and given that subject $i$ is a member of the $k$th group, the observed data vector for that subject is given by
$$\B y_{i} = \B x_{i}\bs\beta_{k}+\bs\epsilon_{i}$$
such that $\epsilon \sim N(0,\Sigma_{k})$. Techniques such as maximum likelihood are used to estimate the parameters in the Gaussian mixture model. Given data and the estimates, each subject has a posterior probability of belonging to each component; therefore, they are `soft' clustered to groups since they belong to all clusters but with different weights. Subjects can then be `hard' clustered (such as with K-means) by assigning groups based on the maximum posterior probability. \\\\
In terms of longitudinal data, there are some necessary restrictions that need to be put on the mixture model. First, the outcome vectors $\{\B y_{i}\}$ may not have equal length. Therefore, the covariance matrix needs structure so that it is based on time in some form. Most software assumes conditional independence even though it may cause bias in the estimates as mentioned in Chapter 2. Also, to model the shape over time, the matrices of explanatory variables $\{\B x_{i}\}$ includes variables that are a function of time such as a polynomial or spline basis. \\\\
PROC TRAJ ``group-based appoarch lends itself to analyzing questions that are framed int arms of the shape of the developmental course of the outcome of interest'' ``gropu-based...identification of different trajectory shapes and on examining how the prevalence of the shape and the shape itself relate to predictors''\\\\
If the correlation structure is assumed conditional independence, then the likelihood function, which is maximized for estimation, is based on the Euclidean distance between observations and the mean, akin to the K-means algorithm. Therefore, in practice, the means will be placed to minimize this distance, maximizing the likelihood function. Therefore, the Gaussian mixture model groups individuals based on their entire vector of observations rather than just the shape. \\\\
Now, it may be possible to cluster by shape using the standard Gaussian mixture model if the differences in the vertical level can be modeled by a correlation structure EXPLAIN BETTER. For example, the random intercept model assumes each individual has their own intercept which is a realization of a random variable such that $\B y_{i} = \B x_{i}\bs\beta_{k} + \alpha_{i}+\bs\epsilon_{i}$ and $\alpha_{i}\sim N(0,\tau^{2})$. If the individuals with similar shapes have normally distributed vertical levels, then the exchangeable correlation structure can take that variable into account and the clusters should represent the shape groups MORE DISCUSSION. However, if there is more variability in the levels that cannot be explained by any known correlation structure, the standard Gaussian mixture model still fails at attaining the goal of cluster based on shape. \\\\
Lastly, if it is believed that baseline factors are related to the cluster membership, the mixing proportions, $\pi_{1},...,\pi_{K}$, can be parameterized using the multinomial logit to model the relationship between variables and the estimated posterior probabilities. This analysis acknowledges that group labels are uncertain and allows the baseline variables to impact the estimation of the the clusters and mean parameters. Since there is a probability framework, inferences can be made using robust standard error estimates.
TALK ABOUT FUNCTIONAL DATA ANALYSIS HERE?
\section{Extensions for Shape}
Although there has been work done in developing clustering methods for longitudinal data, there has been much less work and discussion on the best methods to utilize when shape or change over time is the feature of interest. Now the term shape has many meanings. It is used geometrically to define objects such as circles and triangles. In this thesis, we are using the term to describe the functional pattern since longitudinal data fits the typical definition of a function where each input has only one output; for each individual, we measure only one outcome at each time point. Therefore, using the language from elementary calculus to describe functions we can determine whether the function is increasing or decreasing, the location of the extreme points, and the concavity. All of these components describe different aspects of the shape of a function. The first derivative of the function indicates whether the function is increasing or decreasing at a point. The location of extreme points where the first derivative is zero indicate the local and global minima and maxima of the function. Lastly, the second derivative indicates the growth of the first derivative and thus whether the function is concave up or down at any point. These are all worthy descriptions of shape and may be applicable in different scientific settings. For example, it may be of interest to cluster genes based on the location of the peaks in expression levels. For others, it may only be important whether the function is increasing, decreasing, or stable, so that only the sign of the first derivative is needed \cite{phang2003}. We are interested in defining shape as the pattern curve after disregarding the vertical level. There are a few ways of removing the level. One way is to calculate the first derivative as it removes the level and uniquely describes on part of the shape of the remaining curve. However, it is difficult to calculate the first derivative of a curve when we observe a noisy version of that curve. Another popular procedure involves calculating the Pearson correlation coefficient distance measure for two vectors in the dissimilarity measure. Within the correlation coefficient, the vectors are standardized and thus the level is removed within the process. SAY MORE The noise has less of an impact on the correlation coefficient if the signal is strong in comparison to the noise.
\subsection{Derivative-based dissimilarity}
 Since we do not directly observe the derivative function, researchers such as M\:{o}ller-Levet et. al. \cite{moller2003} and D'Urso \cite{d2000} suggested estimating the derivative function through differencing, calculating the slope of a linear interpolation between points via a difference quotient estimator, and then calculating the Euclidean distance between those estimates from two trajectories. This leads to calculating the dissimilarity between individuals based only on their estimated derivative curve which is separate from the original level of the curve. However, many issues arise with this procedure. First, this method also requires balanced, evenly sampled data to compare derivative estimates from two vectors. Secondly, if we assume that the observed data is a realization of the model
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma_{i}^{2})$ then the difference quotient estimator of the derivative is $\hat{f}_{i}^{'}(\tau) = (y_i(t_{j+1})-y_i(t_j))/(t_{j+1}-t_j)$ for $\tau= t_{j}$ or$ t_{j+1})$. This estimator is unbiased for the difference quotient, but not for $f_{i}^{'}(\tau)$ with $E(\hat{f}_{i}^{'}(\tau)) = (f_i(t_{j+1})-f_i(t_j))/(t_{j+1}-t_j)$. However, if the variance of the random noise is substantial, this estimator results in highly inefficient,
$$\text{Var}(\hat{f}_{i}^{'}(\tau)) =  2\sigma^{2}_{i}/ (t_{j+1}-t_j)^{2}$$
assuming the sample times are fixed. Therefore, the variance of the estimates are a function of the ratio between the standard deviation of the errors and the length of the time sampling increments. This would suggest that maximizing the time between observations gives the most efficient estimates. Therefore, if you have a fixed observation period, that would mean only taking a baseline and one follow up measurement. This may be fine if the function is linear; however, this prevents the detection of other behavior during the time period. It makes sense that more observation points are required in the middle to accurately calculate the derivative of the function, but it will increase the variability of this estimator. Under the suggestion of M\:{o}ller-Levet et. al. and D'Urso, the difference derivate estimates are compared between individuals by calculating the Euclidean distance between the vectors of estimates. MORE HERE IF LINEARITY IS THE MAIN EXPECTATION. Therefore, there is no way to borrow strength between individuals to estimate the derivative and the noisy can have a large impact on the calculated distances. Of course, this requires the data to balanced with equal number of observations per individual and equal sampling times. \\\\ 
The main concern is how these methods compare in the actual clustering; therefore, it is important to determine the behavior of Euclidean distance based on these estimated slopes and how well it can distinguish between two noisy curves in terms of their shape over time. DECIDE WHETHER TO LEAVE THIS EXAMPLE Let $n=3$, $f_1(t) = f_2(t) =0$, $f_3(t)=s*t$, and $\sigma_1=\sigma_2=\sigma_3=\sigma$. Also, let $\B t_1 = \B t_2=\B t_3 = (\delta_t,2\cdot\delta_t,...,m\cdot\delta_t)$. The $m-1$ length vectors of calculated slopes between the realizations for each $y_1$,$y_2$, and $y_3$ are denoted as $dy_1$, $dy_2$, $dy_3$. Then we are interested in the probability of the event that the distance between $dy_1$ and $dy_2$, which are both generated from horizontal lines, is less than the distance between $dy_2$ and $dy_3$. Given $\sigma^2$, $\delta_t$, $s$, and $m$, we want to calculate
$$P(\|dy_1-dy_2\|^2_2 < \|dy_2-dy_3\|)$$
assuming Gaussian errors. Generating the three realizations, calculating the distance between calculated slopes, and comparing the distances 5000 times provides an approximation to the probability stated above. We completed this simulation for many conditions specified by a combination of $s = 0.25, 0.5, 1,2,3,4,5$, $m = 3,5,10,30$, $\delta_t = 0.1,0.6,1.1,1.6$, and $\sigma = 0.1,0.6,1.1,1.6,2.1,2.6$. TRY MORE REASONABLE VALUES FOR THESE. Since the variance is a function of the ratio, $\sigma/\delta_t$, the probability is also affected by the ratio. The estimated probabilities are plotted against $\log(\sigma/\delta_t)$ for different slopes and lengths of vectors (Figure). FIX THE LENGTH OR FIX THE RANGE. The sinusoidal shape of estimated probabilities for each slope can be modeled with the logistic curve $g(x) = \frac{0.5}{1+ae^{bx}}+0.5$ using nonlinear least squares. We note that as the ratio increases, the probability of the two horizontal realizations being closer than the two observations from different shapes decreases to 0.50. Therefore, if the variance is large in comparison to the sampling increments, it is a toss up as to whether this method can determine that two observations were generated from the same shape. On the other hand, if the variance is small relative to the time between samples, the procedure will find the two horizontal lines more similar than two lines with different slopes. Therefore, if the individuals are not observed frequently and the measurement error is relatively small, this clustering procedure may behave appropriately by grouping individuals with similar patterns over time. 
\begin{center}
Insert slope figures here
\end{center}


\subsection{Correlation-based dissimilarity}
The correlation coefficient has been generally used to calculate dissimilarities based on the shape of the data without much discussion about how well it does to discriminate between shapes \cite{chouakria2007,  eisen1998, chiou2008}. In the context of functional clustering, Chiou and Li \cite{chiou2008} suggested using the functional correlation as a similarity measure to cluster similar functions. However, their approach involves estimating parameters of a mixture of stochastic process in an iterative procedure that seeks to maximizing the functional correlation among all clusters. This clustering approach requires densely collected longitudinal data and therefore, it is not applicable in our circumstance. In the multivariate setting, the Pearson correlation coefficient between two vectors can be the basis of a dissimilarity measure such as $d_{cor}(x,y) = (1-cor(x,y))/2$ where $$cor(x,y) = \frac{\sum^{m}_{j=1}(x_{j}-\bar{x})(y_{j}-\bar{y})}{\sqrt{\sum^{m}_{j=1}(x_{j}-\bar{x})^{2}}\sqrt{\sum^{m}_{j=1}(y_{j}-\bar{y})^{2}}}.$$
The correlation should be close to one for two curves with the same shape, as long as the signal is stronger than the noise. This results in a distance of zero and thus these two curves would be placed in the same cluster. The correlation coefficient involves the product of standardized elements, therefore, the largest distances occur when the two curves are mirrored images of each other over their mean lines. However, if the vector is constant in that its graph is a horizontal line, the correlation coefficient will always be close to zero and thus the distance equals to 1/2. This means that the Pearson correlation coefficient cannot detect two horizontal curves as having the same shape. Thus, this methods fails in many cases where the longitudinal values may be stable over time. This characteristic does not get discussed in the literature, but it has huge ramifications for clustering typical longitudinal data.

\section{Conclusion}
All of the dissimilarity-based methods described require data collected at fixed time points for all of the subjects. Ideally, we want a method that will exploit and work with the properties of longitudinal data, which includes the possibility of unbalanced, irregular data. We propose three extensions of existing method that fit this criteria. Before we discuss the proposed methodology, it is worth discussing the decomposition of a curve into level and shape and how that translates into a model. Additionally,  there are two common models to assume for the data-generating mechanism that suggests that groups are based on shape and individuals could differ in the vertical level within the group. First, like above in a functional data approach, we assume that the $j$th observation for the $i$th individual at time $t_{ij}$ is
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma_{i}^{2})$ and $f_{i}$ is a random function sampled from a Hilbert space of square integrable functions on a real interval. Secondly, in the model-based approach, we assume that the $j$th observation for the $i$th individual at time $t_{ij}$ is
$$y_{ij}=\alpha_i+f_k(t_{ij})+\epsilon_{ij}$$
given the $i$th individual is in group $k$, where $\alpha_i$ is adjusts the vertical level of an individuals' curve, $f_k$ is a continuous mean function for cluster $k$, and $\epsilon_{ij}$ is random error that may be correlated within subject $i$ but independent between subjects. The type of model is used frequently in longitudinal data analysis \cite{diggle2002} and if $\alpha_{i}$ is considered a random variable following some distribution, then it is considered a random intercept model. \\\\
 In the next few chapters, we will review the standard methods, and then present three new methods adapted from related literature that address issues of shape or pattern over time and apply them to growth trajectories data of children. Additionally, we will compare the methods in a simulation study and discuss the advantages and disadvantages of using the methods in practice. 
\bibliographystyle{plain}	
\bibliography{Dissertation}	

\end{document}