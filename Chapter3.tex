\documentclass[12pt]{article}
\usepackage{fullpage,amsmath,amsfonts,graphicx,amsthm}
\usepackage{setspace}
\title{Motivation Chapter}
\author{Brianna C. Heggeseth}

\newtheorem{theorem}{Theorem}
  \newcommand{\B}[0]{\mathbf}
    \newcommand{\bs}[0]{\boldsymbol}

\begin{document}
\doublespace
\maketitle
 In this chapter, I motivate clustering longitudinal data based on the {\em shape} of the trajectory over time by highlighting the goal of longitudinal data is to study change over time. Next, I review the standard clustering methods presented in Chapter 1 and discuss their limitations in achieving these goals. Finally, I close by discussing some extension that have been designed to particularly focus on shape.

\section{Motivation}
There is no one way to cluster data. There are many characteristics of data on which to base a grouping. This is not only true for data sets. We constantly group products, people, and projects based on different qualities and characteristics. However, many forget this choice when faced with a data set of continuous variables. Many clustering procedures involve the squared Euclidean distance on multivariate vectors. This may work adequately when the data vectors are comprised of independent explanatory factors all of which are equally important features of the object. However, when the vector has time-ordered structure, a distance measure that is based on taking the difference between vectors, element-wise, ignores important structural properties of the data. Wang et. al. suggested estimating global features of time series processes such as the trend, seasonality, autocorrelation, and kurtosis to use when clustering \cite{wang2006}. Some of these features apply to longitudinal data where data is sparsely, irregular sampled with a small number of repeated measures. \\

A key advantage of a longitudinal study is its ability to measure individual change over time. Trend in particular is of interest. We go a step further and break trend into two components, level and shape of the trend. In many circumstances, these two characteristics provide distinct information that can elicit different actions. For example, when investing in stocks, the magnitude of the share price provides information about value of a company and their practice of splitting shares. That is important, but the characteristic that drives a decision to buy or sell shares is the change in price over time. Similarly, a high average body mass index triggers concerns about health, but knowing how that value has changed over time indicates whether the problem is getting worse or better. In studying gender inequality in salaries, it is important to distinguish discrimination in starting salaries or systematic discrimination over time. We separate these two features as they answer different questions. \\

PARAGRAPH FROM TECH REPORT?\\

The majority of the methods currently used to cluster longitudinal data do not group individuals based on the shape of their trajectory over time. Often conventional methods directly or indirectly group individuals based on the squared Euclidean distance. This results in the magnitude and thus the level of the outcome measures driving cluster formation. For instance, assume we observed outcome measures for four individuals over time where the four vectors of observations are below.
\begin{align*}
\B v_{1}&=[1,2,3,4,5]\\
\B v_{2}&=[5,4,3,2,1]\\
\B v_{3}&=[6,7,8,9,10]\\
\B v_{4}&=[10,9,8,7,6]
\end{align*}
The first and second individual have similar values with one increasing and one decreasing over time. The outcome values for the third and fourth individuals are vertical shifts of the first two. The pairwise squared Euclidean distance between vectors are calculated below:
\begin{align*}
d(\B v_{1},\B v_{2}) = d(\B v_{3},\B v_{4}) &= 4^{2}+2^{2}+2^{2}+4^{2} = 40\\
d(\B v_{1},\B v_{3}) = d(\B v_{2},\B v_{4}) &= 5^{2}+5^{2}+5^{2}+5^{2}+5^{2}=125\\
d(\B v_{1},\B v_{4}) = d(\B v_{2},\B v_{3}) &= 9^{2}+7^{2}+5^{2}+3^{2}+1^{2}=175
\end{align*}
The vectors with similar levels are closest to each other using this measure. Note that the distance between the vectors with the same shape (increasing or decreasing by one unit) is over three times that between vectors of the same level. Any partitioning algorithm such as K-means would group the first two together separately from the last two individuals, which defines the clustering based on level. Traditional methods cluster individuals on the most discriminating feature which is typically the level.\\

That is not to say that the conventional methods based on the Euclidean distance will never detect clusters based on the temporal change over time. If the shape of the trajectory is highly dependent on the vertical level, then the standard cluster methods likely detect these shape groups. We adjust the simple example above such that the outcome vectors for the four individuals are
\begin{align*}
\B v_{1}&=[1,2,3,4,5]\\
\B v_{2}&=[2,3,4,5,6]\\
\B v_{3}&=[10,9,8,7,6]\\
\B v_{4}&=[11,10,9,8,7].
\end{align*}
Now, level and shape are perfectly correlated in that the first two individuals with lower levels have increases values over time and the second two with higher levels decrease over time. The squared Euclidean distance between the pairs of vectors are calculated below:
\begin{align*}
d(\B v_{1},\B v_{2}) = d(\B v_{3},\B v_{4}) &= 1^{2}+1^{2}+1^{2}+1^{2} + 1^{2} = 5\
d(\B v_{1},\B v_{3}) = d(\B v_{2},\B v_{4}) &= 9^{2}+7^{2}+5^{2}+3^{2}+1^{2}=175\\
d(\B v_{1},\B v_{4}) &= 10^{2}+8^{2}+6^{2}+4^{2}+2^{2}=220\\
d(\B v_{2},\B v_{3}) &= 8^{2}+6^{2}+4^{2}+2^{2}=120
\end{align*}
The groups based on these distances are separately by level but also by shape. \\

As with any data set, one of the most common summary statistics is the mean. In a cross-sectional study, it is easy to remember that the mean does not represent any one person, but with longitudinal data, it is tempting to interpret the mean vectors as representative of those in the group even though no one person may follow the average path. Now for most clustering methods, it is typical to report the group means to summarize the individuals in each cluster. It is natural for applied researchers to summarize cluster results of longitudinal data by comparing `representative curves' between clusters. These curves are usually simple graphs of the mean or median outcome vectors within each cluster or the mean outcome as a function of time calculated using estimated parameters. But, the same mistakes are made by inferring the individual within the clusters follow the `representative' path. More importantly, researchers incorrectly use the shape of the `representative' curves to summarize the shape of the individual curves in the clusters \cite{windle2004,mulvaney2006,broadbent2008,pryor2011,mccoy2010}. Now there are two problems with this situation; first, we have established that incorrect inferences are made based on a `representative' curve  even though it is an average over all the relationships. Second, it does not make sense to summarize clusters by the shape of the curves if the clustering procedure did not group similarly shaped curves. \\

We have discussed the importance of clustering separately for level and for shape as studying the variability of the two characteristics answer different questions. Despite the interest in the shape of the change over time, too many researchers just apply a standard method based on the squared Euclidean distance without much thought on the implications. The squared differences generally lead to level-based groups. Imagine a measuring alcohol consumption over time for a sample of adolescents. The cohort may contain individuals who do not drink at all as well as some that have moderate or heavy drinking habits. Besides different levels of alcohol consumption, there may be patterns of changing behavior such as escalation, reduction, or stabilization. All of these patterns could occur within every level of alcohol consumption. Figure \ref{fig:3-1} illustrates a possible scenario of four individuals, two of which drink heavily and two of which are low to moderate drinkers. Within both level categories, one of the individuals has escalating behavior and the other is reducing consumption. We assume linear change for simplicity. The shape of the curve, the slope in this case, is independent of the consumption level. If we use a standard clustering method based on the squared Euclidean distance, such as K-means, on the observed vectors, we find two clusters that are determined by the level of consumption; the two heavy drinkers are grouped together and the two moderate consumers in another group. Consequently, the mean trajectory for each group is a horizontal line, which disguises the fact that the alcohol consumption is not stable for all individuals. This is a trivial, highly simplified example, but it illustrates the type of results that can occur in practice \cite{mccoy2010}.\\

\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Exp}
\end{center}
\caption{Graph of linear trajectories representing the hypothetical alcohol consumption of four individuals.}
\label{fig:3-1}
\end{figure}
Although the level of alcohol consumption is important to many public health officials, the knowledge of whether or not behavior is escalating or decreasing in a population informs whether or not and when interventions need to be implemented. In many circumstances, the level is one aspect of interest, but studying the shape is a different aspect that answers other questions. There are factors that influence the level and perhaps other factors that affect the change patterns and it is important to separate them to as to not muddy the results to the research question. If the goal is to detect and compare groups based on their temporal change over time, we need methods that answer the following research questions: Are there distinct shape patterns in the data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? In the next section, I discuss this two standard clustering methods in the context of cluster based on shape and highlight the situations in which these methods fail to address the research questions above. 


\section{Standard clustering methods}
The two standard methods for clustering multivariate data include partition methods based on a dissimilarity measure such as Euclidean distance and model-based methods such as finite mixture models. These methods were introduced in the first chapter of this thesis and I now illustrate the limitations of these methods in answering the research questions above. I assume that there are $n$ subjects such that for subject $i$, we have $m_{i}$ repeated measures of an outcome of interest. I denote the vector of measured outcomes as $\B y_{i}=(y_{i1},...,y_{im_{i}})$ for $i=1,...,n$ and $\B z_{i}\in \mathbb{R}^{q}$ as the design vector based on baseline variables that may impact group membership. Lastly, the corresponding time of the data collection for subject $i$ is $\B t_{i}=(t_{i1},...,t_{im_{i}})$. 
\subsection{Partitioning algorithms}
If the outcome is measured at the same time points for every subject such that $m_{i}=m$ and $\B t_{i}=\B t$ for all $i=1,...,n$, longitudinal data fit into a typical multivariate data framework where a partitioning method such as K-means \cite{macqueen1967, hartigan1979} or partitioning around medoids (PAM) \cite{kaufman1990} on the observed vector is typically appropriate to use. For the K-means algorithm, given the number of clusters, $K$, subjects are randomly assigned into $K$ groups. A mean vector, known as a centroid, is calculated for each group of vectors. Subjects are then reassigned to the group with the closest centroid in terms of the squared Euclidean distance. This process of calculating the centroid and then reassigning subjects is repeated until convergence. At convergence, subjects are assigned to one and only one group. This strict partitioning of the data is termed `hard' clustering. In other words, the K-means algorithm searches for $K$ centroid vectors $\B a=\{\B a_{1}, ...,\B a_{K}\}$ that minimize the following objective function
$$\frac{1}{n} \sum^{n}_{i=1}\min_{1\leq k\leq K}\|\B  y_{i}-\B a_{k}\|_{2}^{2}$$
where $\|\B y - \B x\|^{2}_{2}$ is the squared Euclidean distance between two vectors $\B y$ and $\B x$.\\

I now illustrate how this algorithm fails to address all three of the questions posed above. Using the simple alcohol consumption example, we let $n=4$, and suppose we observe the average number of drinks per week for ten years, $\B t=(10,11,12,13,14,15,16,17,18,19,20)$. Let the observed outcome vectors be linear in trend as shown in Figure \ref{fig:3-1}. Note that there are two shape groups: increasing and decreasing. In this circumstance, the level and the shape are not strongly associated since there are different levels within each shape group. Now the K-means algorithm groups based on the squared Euclidean distance between individuals. The distances put into a symmetric distance matrix for these four subjects is listed in Table \ref{tab:3-1}. 
\begin{table}[h]
\begin{center}
\begin{tabular}{c|cccc}
&Low Decreasing& High Decreasing&Low Increasing&High Increasing\\
\hline
Low Decreasing&0&275.0&17.6&292.6\\
High Decreasing&275.0  &0 &  292.6 &17.6 \\                     
Low Increasing& 17.6 &292.6  &0   &275.0   \\          
High Increasing& 292.6 &17.6 &275.0   &0 
\end{tabular}
\end{center}
\caption{Squared Euclidean distance between the hypothetical alcohol consumption vectors for four individuals: high level but slowly decreasing, high level but slowly increasing, low level but slowly decreasing, and low level but slowly increasing. }
\label{tab:3-1}
\end{table}
In terms of distance, the level dominates such that the subjects with the same level (high or low) are clearly closest with this measure. The distance between those with same shape have a distance of about fifteen times that of level. This clustering method detects distinct levels and if the distinct shapes do not coincide with the distinct levels, the K-means algorithm fails at grouping individuals based on shape and detecting the number of patterns. The `representative' curves over time for the two groups end up being horizontal even though the alcohol summation of no individual is stable over time (Figure \ref{fig:3-1}).\\

Imagine though if the shape of the trajectory were correlated with the vertical level where all of the heavy drinkers slowly decrease the number of drinks per week and the light drinkers increase over their teenage years, then K-means algorithm gives clusters based on level and thus on shape in this case (Figure \ref{fig:3-2}). This only works in cases where the trajectories with similar shapes also have similar levels, which may be the case in some applications, but for many data sets, this is not true. We have focused on the K-means algorithm right now, but these conclusions hold for the PAM algorithm when the Euclidean or squared Euclidean distance is used.\\
\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Exp2}
\end{center}
\caption{Graph of linear trajectories representing the hypothetical alcohol consumption of four individuals.}
\label{fig:3-2} 
\end{figure}

Lastly, if baseline factors are related to the cluster membership, the only option is to complete post-cluster analysis with the cluster labels as the outcome variable. One technique is to assume a multinomial logistic regression model for the labels and estimate parameters using observed baseline variables. This analysis automatically assumes that the group labels are known and fixed; it does not take into account the uncertainty in the group memberships and sample variability. A subject could have outcome measures that puts him/her on the boundary of two groups, but a partitioning algorithm has to put them in one group. Therefore, any inference based on calculated standard errors should be done with caution as the standard errors are calculated conditional on cluster labels and do not take into account other sources of uncertainty.

\subsection{Finite Mixture Models}
In contrast to partition methods, finite mixture models provide a probability framework in which to take into account the uncertainty of group memberships and simultaneously estimate the relationship between baseline factors and groups. Additionally, the model is flexible to accommodate irregular sampling which frequently occurs with longitudinal data. Mixture models are useful in modeling distinct subgroups in the population. An overview of finite mixture models is available in many texts \cite{everitt1981,mclachlan1988,mclachlan2000} and earlier in this thesis. A finite mixture model is a weight sum of component distributions. In this thesis, we assume a parametric form for these distributions and that parameters differ between components. Data in each component are assumed to follow a Gaussian distribution.\\

In general, assume there are $K$ latent relationships between the outcome vector, $\B y$, and explanatory variables, $\B x$, that occur in the population with frequencies $\pi_{1},...,\pi_{K}$. If subject $i$ is a member of the $k$th group, the observed data vector for that subject is given by
$$\B y_{i} = \B x_{i}\bs\beta_{k}+\bs\epsilon_{i}$$
such that $\bs\epsilon \sim N(0,\bs\Sigma_{k})$. Maximum likelihood estimation via the EM algorithm is used to estimate the parameters in the Gaussian mixture model. Given the data, each subject has a posterior probability of belonging to each component; therefore, they are `soft' clustered to groups since they belong to all clusters to some degree. Subjects can then be `hard' clustered through assignment based on the maximum posterior probability. \\

If the outcome vector is a product of a longitudinal study, there are some necessary adjustments that need to be made to the mixture model. The outcome vectors $\{\B y_{i}\}$ may not have equal length or be observed at the same times. Therefore, we must impose structure on the covariance matrices, $\bs\Sigma_{k}$. Most software packages restrict the structure to conditional independence, which we showed may cause bias in the estimates in Chapter 2. To model the mean over time, the matrices of explanatory variables, $\{\B x_{i}\}$, must include time components that can model the shape of the curve. \\

If the correlation structure is assumed conditional independence, then the likelihood function is based on the squared Euclidean distance between observations and the mean scaled by the estimated variance. Akin to the K-means algorithm, maximizing the likelihood with the raw data vectors will generally fail to group individuals by shape if shape and level are weakly dependent. There are a few exceptions. If each outcome is a realization of $\B y_{i} = \B x_{i}\bs\beta_{k} + \alpha_{i}+\bs\epsilon_{i}$ where $\alpha_{i}\sim N(0,\tau^{2})$ and $\bs\epsilon\sim N(0,\sigma^{2}_{k})$ for subjects in the $k$th group, then it is possible to model the variability of the intercept through the correlation stricture. In general, if the distribution of the level/intercept is known and can be correctly modeled within each shape group, then including that into the standard model should produce shape groups. In practice, this is not known and the results are highly sensitive to incorrect assumptions about the random intercept.\\

Despite the evidence that the standard finite mixture fails to group individuals by shape, there are some that attempt to lead people to believe that a finite mixture model assuming conditional independence ``lends itself to analyzing questions that are framed in terms of the shape of the developmental course of the outcome of interest''  and  ``focuses on identification of different trajectory shapes and on examining how the prevalence of the shape and the shape itself relate to predictors'' \cite{nagin}.

\section{Extensions for Shape}
Although there has been work done in developing clustering methods for longitudinal data, there has been much less work and discussion on the best methods to utilize when shape or change over time is the feature of interest. Now the term shape has many meanings. It is used geometrically to define objects such as circles and triangles. In this thesis, we are using the term to describe the functional pattern since longitudinal data fits the typical definition of a function where each input has only one output; for each individual, we measure only one outcome at each time point. Therefore, using the language from elementary calculus to describe functions we can determine whether the function is increasing or decreasing, the location of the extreme points, and the concavity. All of these components describe different aspects of the shape of a function. The first derivative of the function indicates whether the function is increasing or decreasing at a point. The location of extreme points where the first derivative is zero indicate the local and global minima and maxima of the function. Lastly, the second derivative indicates the growth of the first derivative and thus whether the function is concave up or down at any point. These are all worthy descriptions of shape and may be applicable in different scientific settings. For example, it may be of interest to cluster genes based on the location of the peaks in expression levels. For others, it may only be important whether the function is increasing, decreasing, or stable, so that only the sign of the first derivative is needed \cite{phang2003}. We are interested in defining shape as the pattern curve after disregarding the vertical level. There are a few ways of removing the level. One way is to calculate the first derivative as it removes the level and uniquely describes on part of the shape of the remaining curve. However, it is difficult to calculate the first derivative of a curve when we observe a noisy version of that curve. Another popular procedure involves calculating the Pearson correlation coefficient distance measure for two vectors in the dissimilarity measure. Within the correlation coefficient, the vectors are standardized and thus the level is removed within the process. SAY MORE The noise has less of an impact on the correlation coefficient if the signal is strong in comparison to the noise.
\subsection{Derivative-based dissimilarity}
 Since we do not directly observe the derivative function, researchers such as M\:{o}ller-Levet et. al. \cite{moller2003} and D'Urso \cite{d2000} suggested estimating the derivative function through differencing, calculating the slope of a linear interpolation between points via a difference quotient estimator, and then calculating the Euclidean distance between those estimates from two trajectories. This leads to calculating the dissimilarity between individuals based only on their estimated derivative curve which is separate from the original level of the curve. However, many issues arise with this procedure. First, this method also requires balanced, evenly sampled data to compare derivative estimates from two vectors. Secondly, if we assume that the observed data is a realization of the model
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma_{i}^{2})$ then the difference quotient estimator of the derivative is $\hat{f}_{i}^{'}(\tau) = (y_i(t_{j+1})-y_i(t_j))/(t_{j+1}-t_j)$ for $\tau= t_{j}$ or$ t_{j+1})$. This estimator is unbiased for the difference quotient, but not for $f_{i}^{'}(\tau)$ with $E(\hat{f}_{i}^{'}(\tau)) = (f_i(t_{j+1})-f_i(t_j))/(t_{j+1}-t_j)$. However, if the variance of the random noise is substantial, this estimator results in highly inefficient,
$$\text{Var}(\hat{f}_{i}^{'}(\tau)) =  2\sigma^{2}_{i}/ (t_{j+1}-t_j)^{2}$$
assuming the sample times are fixed. Therefore, the variance of the estimates are a function of the ratio between the standard deviation of the errors and the length of the time sampling increments. This would suggest that maximizing the time between observations gives the most efficient estimates. Therefore, if you have a fixed observation period, that would mean only taking a baseline and one follow up measurement. This may be fine if the function is linear; however, this prevents the detection of other behavior during the time period. It makes sense that more observation points are required in the middle to accurately calculate the derivative of the function, but it will increase the variability of this estimator. Under the suggestion of M\:{o}ller-Levet et. al. and D'Urso, the difference derivate estimates are compared between individuals by calculating the Euclidean distance between the vectors of estimates. MORE HERE IF LINEARITY IS THE MAIN EXPECTATION. Therefore, there is no way to borrow strength between individuals to estimate the derivative and the noisy can have a large impact on the calculated distances. Of course, this requires the data to balanced with equal number of observations per individual and equal sampling times. \\\\ 
The main concern is how these methods compare in the actual clustering; therefore, it is important to determine the behavior of Euclidean distance based on these estimated slopes and how well it can distinguish between two noisy curves in terms of their shape over time. DECIDE WHETHER TO LEAVE THIS EXAMPLE Let $n=3$, $f_1(t) = f_2(t) =0$, $f_3(t)=s*t$, and $\sigma_1=\sigma_2=\sigma_3=\sigma$. Also, let $\B t_1 = \B t_2=\B t_3 = (\delta_t,2\cdot\delta_t,...,m\cdot\delta_t)$. The $m-1$ length vectors of calculated slopes between the realizations for each $y_1$,$y_2$, and $y_3$ are denoted as $dy_1$, $dy_2$, $dy_3$. Then we are interested in the probability of the event that the distance between $dy_1$ and $dy_2$, which are both generated from horizontal lines, is less than the distance between $dy_2$ and $dy_3$. Given $\sigma^2$, $\delta_t$, $s$, and $m$, we want to calculate
$$P(\|dy_1-dy_2\|^2_2 < \|dy_2-dy_3\|)$$
assuming Gaussian errors. Generating the three realizations, calculating the distance between calculated slopes, and comparing the distances 5000 times provides an approximation to the probability stated above. We completed this simulation for many conditions specified by a combination of $s = 0.25, 0.5, 1,2,3,4,5$, $m = 3,5,10,30$, $\delta_t = 0.1,0.6,1.1,1.6$, and $\sigma = 0.1,0.6,1.1,1.6,2.1,2.6$. TRY MORE REASONABLE VALUES FOR THESE. Since the variance is a function of the ratio, $\sigma/\delta_t$, the probability is also affected by the ratio. The estimated probabilities are plotted against $\log(\sigma/\delta_t)$ for different slopes and lengths of vectors (Figure). FIX THE LENGTH OR FIX THE RANGE. The sinusoidal shape of estimated probabilities for each slope can be modeled with the logistic curve $g(x) = \frac{0.5}{1+ae^{bx}}+0.5$ using nonlinear least squares. We note that as the ratio increases, the probability of the two horizontal realizations being closer than the two observations from different shapes decreases to 0.50. Therefore, if the variance is large in comparison to the sampling increments, it is a toss up as to whether this method can determine that two observations were generated from the same shape. On the other hand, if the variance is small relative to the time between samples, the procedure will find the two horizontal lines more similar than two lines with different slopes. Therefore, if the individuals are not observed frequently and the measurement error is relatively small, this clustering procedure may behave appropriately by grouping individuals with similar patterns over time. 
\begin{center}
Insert slope figures here
\end{center}


\subsection{Correlation-based dissimilarity}
The correlation coefficient has been generally used to calculate dissimilarities based on the shape of the data without much discussion about how well it does to discriminate between shapes \cite{chouakria2007,  eisen1998, chiou2008}. In the context of functional clustering, Chiou and Li \cite{chiou2008} suggested using the functional correlation as a similarity measure to cluster similar functions. However, their approach involves estimating parameters of a mixture of stochastic process in an iterative procedure that seeks to maximizing the functional correlation among all clusters. This clustering approach requires densely collected longitudinal data and therefore, it is not applicable in our circumstance. In the multivariate setting, the Pearson correlation coefficient between two vectors can be the basis of a dissimilarity measure such as $d_{cor}(x,y) = (1-cor(x,y))/2$ where $$cor(x,y) = \frac{\sum^{m}_{j=1}(x_{j}-\bar{x})(y_{j}-\bar{y})}{\sqrt{\sum^{m}_{j=1}(x_{j}-\bar{x})^{2}}\sqrt{\sum^{m}_{j=1}(y_{j}-\bar{y})^{2}}}.$$
The correlation should be close to one for two curves with the same shape, as long as the signal is stronger than the noise. This results in a distance of zero and thus these two curves would be placed in the same cluster. The correlation coefficient involves the product of standardized elements, therefore, the largest distances occur when the two curves are mirrored images of each other over their mean lines. However, if the vector is constant in that its graph is a horizontal line, the correlation coefficient will always be close to zero and thus the distance equals to 1/2. This means that the Pearson correlation coefficient cannot detect two horizontal curves as having the same shape. Thus, this methods fails in many cases where the longitudinal values may be stable over time. This characteristic does not get discussed in the literature, but it has huge ramifications for clustering typical longitudinal data.

\section{Conclusion}
All of the dissimilarity-based methods described require data collected at fixed time points for all of the subjects. Ideally, we want a method that will exploit and work with the properties of longitudinal data, which includes the possibility of unbalanced, irregular data. We propose three extensions of existing method that fit this criteria. Before we discuss the proposed methodology, it is worth discussing the decomposition of a curve into level and shape and how that translates into a model. Additionally,  there are two common models to assume for the data-generating mechanism that suggests that groups are based on shape and individuals could differ in the vertical level within the group. First, like above in a functional data approach, we assume that the $j$th observation for the $i$th individual at time $t_{ij}$ is
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma_{i}^{2})$ and $f_{i}$ is a random function sampled from a Hilbert space of square integrable functions on a real interval. Secondly, in the model-based approach, we assume that the $j$th observation for the $i$th individual at time $t_{ij}$ is
$$y_{ij}=\alpha_i+f_k(t_{ij})+\epsilon_{ij}$$
given the $i$th individual is in group $k$, where $\alpha_i$ is adjusts the vertical level of an individuals' curve, $f_k$ is a continuous mean function for cluster $k$, and $\epsilon_{ij}$ is random error that may be correlated within subject $i$ but independent between subjects. The type of model is used frequently in longitudinal data analysis \cite{diggle2002} and if $\alpha_{i}$ is considered a random variable following some distribution, then it is considered a random intercept model. \\\\
 In the next few chapters, we will review the standard methods, and then present three new methods adapted from related literature that address issues of shape or pattern over time and apply them to growth trajectories data of children. Additionally, we will compare the methods in a simulation study and discuss the advantages and disadvantages of using the methods in practice. 
\bibliographystyle{plain}	
\bibliography{Dissertation}	

\end{document}