\chapter{Shape-based clustering}

Longitudinal data is collected with the intent of studying the change over time. One side effect of collecting this type of data is the inherent dependence between the repeated measure. The last chapter provided evidence that this nuisance cannot be ignored when clustering data using a mixture model. In this chapter, I return to discussing clustering methods with the research goals of longitudinal studies in mind. 

\section{Motivation}
There is no one way to cluster data. There are many characteristics of quantitative data on which to base a grouping. This is not only true for data sets. We constantly group products, people, and projects based on different qualities and characteristics. However, we tend forget this choice when faced with a data set filled with continuous measurements. The most popular method is to use the squared Euclidean distance to determine similarity. This may work adequately when the data are organized into vectors of independent explanatory factors all of which are equally important features of the object or subject. However, when the vector has time-ordered structure, a dissimilarity measure that is based only the element-wise differences between vectors ignores the important structural properties of the data.  

There as been some work in adapting and developing clustering methods for longitudinal data in specific areas of applications such as psychology and genomics \cite{schneiderman1993,genolini2010, jones2001, muthen2010, mcnicholas2010}. Most of these methods involve fitting a finite mixture model with adjustments to the mean or covariance to make it more adaptable to longitudinal data. Others cluster subjects with a partition algorithm such as K-means \cite{macqueen1967,hartigan1979,} or partitioning around mediods (PAM) \cite{kaufman1990} using a dissimilarity measure developed specifically to take the time-ordered structure of longitudinal data into account. Applying cluster methods to longitudinal data has gained popularity in other areas of application. For example, there is a growing literature about clustering distinct childhood body mass index growth patterns over time and which early life factors that contribute to following the patterns \cite{pryor2011,carter2012}. 

The main goal of a longitudinal study is to study individual change over time. However, none of the popular methods explicitly group subjects based on the shape of the pattern over time. Although they claim to group similar patterns, the similarity is not defined in terms of any one specific feature of the data. In the time series literature, \textcite{wang2006} suggested estimating global features of time series processes such as the trend, seasonality, autocorrelation, and kurtosis to use when clustering. Some of these features apply to longitudinal data. The trend over time in particular is of interest. We go a step further and break trend into two components: level and shape. 

In many circumstances, these two characteristics provide distinct information that can elicit different actions. For example, when investing in stocks, the magnitude of the share price provides information about value and practices of a company. While that information is important, the feature that drives a decision to buy or sell shares of a stock is the historical change in price over time. Similarly, a high average body mass index triggers concerns about health, but knowing how that value has changed over time indicates whether the problem is improving or worsening. When studying gender inequality in salaries, it is important to distinguish discrimination in starting salaries  and systematic discrimination over time. I distinguish between these two features of a trajectory when thinking about clustering as they provide different information.  

Despite the interest in the shape of the change over time, too many researchers just apply a standard clustering method without much thought on the implications on their results. Not only will the groups be driven by the level rather than the shape of the trajectories, the means within each cluster may not be representative of any individual's trajectory since it may be an average of trajectories with different shapes at the same level. Most importantly, the shape of the mean curves does not represent the shapes of the individuals. It is common for authors make these generalizations when the subtitles of a method are not clearly understood \cite{windle2004,mulvaney2006,broadbent2008,pryor2011,mccoy2010}. 

Imagine a measuring alcohol consumption over time for a sample of adolescents. This cohort may contain individuals who never drink as well as those that have moderate or heavy drinking habits. Besides different levels of alcohol consumption, there may be patterns of changing behavior such as escalation, reduction, or stabilization. All of these patterns can occur within every level of alcohol consumption. Figure \ref{fig:3-1} illustrates a possible scenario of four individuals, two of which drink heavily and two of which are low to moderate drinkers. Within both level categories, one of the individuals has escalating behavior and the other is reducing alcohol consumption. We assume linear change for simplicity. The shape of the curve, the slope in this case, is independent of the consumption level. If K-means is applied to the observed vectors, two clusters are discovered that are determined by the level of consumption; the two heavy drinkers are grouped together and the two moderate consumers in another group. Consequently, the mean trajectory for each group is a horizontal line, which disguises the fact that the alcohol consumption is not stable for any of the individuals. Although the level of alcohol consumption is important for public health, the knowledge of whether or not behavior is escalating or decreasing in a population informs whether or not and when interventions need to be implemented. This is a trivial, highly simplified example, but it illustrates the type of results that occurs in practice \cite{mccoy2010}.

\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Exp}
\end{center}
\caption{Graph of linear trajectories representing the hypothetical alcohol consumption of four individuals.}
\label{fig:3-1}
\end{figure}

There are factors that influence the level and perhaps other factors that affect the shape of the patterns and it is important to separate them to as to not muddy the results. If the goal is to detect and compare groups based on their temporal change over time, we need methods that answer the following research questions: Are there distinct shape patterns in the data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? In the next section, I discuss two popular standard clustering methods in the context of cluster based on shape and highlight the situations in which these methods fail to address the research questions above. 

\section{Limitations of standard clustering methods}
Two standard methods for clustering multivariate data include partition methods based on a dissimilarity measure such as the Euclidean distance and model-based methods such as finite mixture models. These methods were introduced in the first chapter of this thesis and I now illustrate the limitations of these methods in answering the research questions above. 

I assume that there are $n$ subjects such that for subject $i$, there are $m_{i}$ repeated measures of an outcome of interest. I denote the vector of measured outcomes as $\B y_{i}=(y_{i1},...,y_{im_{i}})$ for $i=1,...,n$ and $\B w_{i}\in \mathbb{R}^{q}$ as the design vector based on baseline variables that may impact group membership. Lastly, the corresponding time of the data collection for subject $i$ is $\B t_{i}=(t_{i1},...,t_{im_{i}})$. 

\subsection{Partitioning algorithms}
If the outcome is measured at the same time points for every subject such that $m_{i}=m$ and $\B t_{i}=\B t$ for all $i=1,...,n$, longitudinal data fit into a typical multivariate data framework where a partitioning method like K-means \cite{macqueen1967, hartigan1979} on the observed vector is typically appropriate to use. For the K-means algorithm, given the number of clusters, $K$, subjects are randomly assigned into $K$ groups. A mean vector, known as a centroid, is calculated for each group of vectors. Subjects are then reassigned to the group with the closest centroid in terms of the squared Euclidean distance. This process of calculating the centroid and then reassigning subjects is repeated until convergence. At convergence, subjects are assigned to one and only one group. This strict partitioning of the data is termed `hard' clustering. In other words, the K-means algorithm searches for $K$ centroid vectors $\B a=\{\B a_{1}, ...,\B a_{K}\}$ that minimize the following objective function
$$\frac{1}{n} \sum^{n}_{i=1}\min_{1\leq k\leq K}\|\B  y_{i}-\B a_{k}\|_{2}^{2}$$
where $\|\B y - \B x\|^{2}_{2}$ is the squared Euclidean distance between two vectors $\B y$ and $\B x$.

I now illustrate how this algorithm fails to address all three of the questions previously posed. Using the simple alcohol consumption example, let $n=4$, and suppose the average number of drinks per week is collected over a ten year period, $\B t=(10,11,...,19,20)$. Let the observed outcome vectors be linear in trend as shown in Figure \ref{fig:3-1}. Note that there are two shape groups: increasing and decreasing. In this circumstance, the level and the shape are not strongly associated since there are different levels within each shape group. The K-means algorithm groups based on the squared Euclidean distance between individuals. The distances put into a symmetric distance matrix for these four subjects is listed in Table \ref{tab:3-1}. 
\begin{table}[h]
\begin{center}
\begin{tabular}{c|cccc}
&Low Decreasing& High Decreasing&Low Increasing&High Increasing\\
\hline
Low Decreasing&0&275.0&17.6&292.6\\
High Decreasing&275.0  &0 &  292.6 &17.6 \\                     
Low Increasing& 17.6 &292.6  &0   &275.0   \\          
High Increasing& 292.6 &17.6 &275.0   &0 
\end{tabular}
\end{center}
\caption{Squared Euclidean distance matrix for the hypothetical alcohol consumption vectors of four individuals: high level but slowly decreasing, high level but slowly increasing, low level but slowly decreasing, and low level but slowly increasing. }
\label{tab:3-1}
\end{table}

In terms of distance, the level dominates such that the subjects with the same level (high or low) are clearly the most similar with this measure. The distance between those with same shape have a distance of about fifteen times that of level. This clustering method detects distinct levels and if the distinct shapes do not coincide with the distinct levels, the K-means algorithm fails at grouping individuals based on shape and detecting the number of patterns. The mean curves over time for the two groups end up being horizontal even though the alcohol consumption is not stable over time for any of the individuals (Figure \ref{fig:3-1}).

Imagine if the shape of the trajectory were correlated with the vertical level where all of the heavy drinkers slowly decrease the number of drinks per week and the light drinkers increase over their teenage years, then K-means algorithm gives clusters based on level and thus on shape in this case (Figure \ref{fig:3-2}). This only works in cases where the trajectories with similar shapes also have similar levels, which may be the case in some applications, but for many data sets, this is not true. We have focused on the K-means algorithm right now, but these conclusions hold for the PAM algorithm when the Euclidean or squared Euclidean distance is used.
\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Exp2}
\end{center}
\caption{Graph of linear trajectories representing the hypothetical alcohol consumption of four individuals.}
\label{fig:3-2} 
\end{figure}

Lastly, if baseline factors are related to the cluster membership, the only option is to complete an analysis after the clustering algorithm is complete. One technique is to fit a multinomial logistic regression model for the group labels with baseline factors as the explanatory variables. This analysis automatically assumes that the group labels are known and fixed; it does not take into account the uncertainty in the group memberships and sample variability. Imagine a subject with an outcome measure that puts him/her on the boundary of two groups. A partitioning algorithm puts them in one and only one group without stating any uncertainty in that classification. Therefore, any inference should be done with caution as the standard errors are calculated conditional on cluster labels and do not take into account other sources of uncertainty.

\subsection{Finite mixture models}
In contrast to partition methods, finite mixture models provide a probability framework in which to take into account the uncertainty of group memberships and simultaneously estimate the relationship between baseline factors and groups. Additionally, the model is flexible to accommodate irregular sampling which frequently occurs in longitudinal studies. Mixture models are useful in modeling distinct subgroups in the population. An overview of finite mixture models is available in many texts \cite{everitt1981,mclachlan1988,mclachlan2000} and earlier in this thesis. A finite mixture model is a weight sum of component distributions. We assume a parametric form for these distributions and that parameters differ between components.

In general, I assume there are $K$ latent groups that occur in the population with frequencies $\pi_{1},...,\pi_{K}$. If subject $i$ is a member of the $k$th group, the observed data vector for that subject is given by
$$\B y_{i} = \BS\mu_{k}+\BS\epsilon_{i}\quad \BS\epsilon \sim N(0,\BS\Sigma_{k}).$$
Maximum likelihood estimation via the EM algorithm is used to estimate the model parameters and the posterior probability a subject belongs to each component. These probabilities provide a way to `soft' cluster subjects to groups as a subject belongs to every cluster to some degree. Subjects can then be `hard' clustered through assignment based on the maximum posterior probability. 

If the outcome vector is repeated measures over time, there are some necessary adjustments that need to be made to the mixture model. The outcome vectors $\{\B y_{i}\}$ may not have equal length and the measurements may not be observed at the same times between individuals. Therefore, structure must be imposed on the mean vector and the covariance matrix. A regression structure can be used by assuming a linear model for the mean, $\BS\mu_k=\B x \BS\beta_k$. The matrices of explanatory variables, $\{\B x_{i}\}$, must include time components that can model the shape of the curve.  In terms of the covariance structure, most software packages restrict the structure to conditional independence, which can be problematic as shown in Chapter 2.

If the correlation structure is assumed conditional independence, then the likelihood function is based on the squared Euclidean distance between observations and the mean scaled by the estimated variance. Akin to the K-means algorithm, maximizing the likelihood with the original data vectors generally fails to group individuals by shape if shape and level are weakly dependent. If the level of the outcome vector is determined by a random intercept such that $\B y_{i} = \B x_{i}\BS\beta_{k} + \alpha_{i}+\BS\epsilon_{i}$ where $\alpha_{i}\sim N(0,\tau_k^{2})$ and $\BS\epsilon\sim N(0,\sigma^{2}_{k})$ for subjects in the $k$th group, then it is possible to model the variability of the level through the correlation stricture. In general, if the distribution of the level/intercept is known and can be correctly modeled within each shape group, then including that into the model should produce shape groups. In practice, the distributions are not known and clustering results are be sensitive to incorrect assumptions.

It only takes a simple simulation to show that the standard finite mixture fails to group individuals by shape, but Nagin's book leads people to believe that a finite mixture model assuming conditional independence ``lends itself to analyzing questions that are framed in terms of the shape of the developmental course of the outcome of interest''  and  ``focuses on identification of different trajectory shapes and on examining how the prevalence of the shape and the shape itself relate to predictors'' \cite{nagin2005}. 


\section{Methods extended for shape}
There has been work done in developing and adapting clustering methods for longitudinal data, but there has been much less discussion in the literature about the best methods to utilize when shape of pattern over time is the feature of interest. In this thesis, the term shape refers to the shape of the underlying functional pattern over time.

The shape of a function can be describe in many ways. The first derivative of the function with respect to time indicates the instantaneous rate of change at a point. The sign of the derivative indicates whether the function is increasing or decreasing at a point.  Local and global minima and maxima of the function are determined by where the first derivative is zero. Lastly, the second derivative provides information about the growth of the first derivative and thus whether the function is concave up or down at any point. 

These are all aspects of the shape and they may be important in different scientific settings. For example, when clustering genes, the location of the peaks in expression levels may be of interest so focusing local maxima may be useful when clustering \cite{luan2003}. In other circumstances, it may only be important whether the function is increasing or decreasing so that only the sign of the first derivative is needed \cite{phang2003}. 

I do not separate these components and define shape as the pattern of the function after disregarding the vertical level. There are a few ways to compare the shape of two function using this definition. One way is to calculate the first derivative as it removes the level and uniquely describes the shape of the remaining curve. However, it is difficult to estimate the derivative of a discretized, noisy version of the function. Another popular approach involves calculating the Pearson correlation coefficient between two vectors of discretized versions of the functions. If the shape is the same, the correlation is equal to 1. These two ideas have been implemented in a clustering framework for longitudinal data.


\subsection{Derivative-based dissimilarity}
One approach to clustering on the basis of shape is to compare derivative functions. With longitudinal data, we do not directly observe the derivative function for each individual. Therefore, \textcite{moller2003} and \textcite{d2000} independently suggested estimating the derivative function via the difference quotient, calculating the slope of a linear interpolation between adjacent repeated measures. The dissimilarity between two individuals is measured as the derivative estimates. As a result, individuals are compared on the shape of their underlying trend over time ignoring the original level of the data. 

We assume that the observed data is a realization of the model
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
for $i=1,...,n$ and $j=1,...,m_{i}$ where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma_{i}^{2})$, $f_{i}$ is a differentiable, continuous function, and $m_{i}$ is relatively small (usually around $5-15$). This clustering method essentially requires balanced data so the observations are fixed across individuals, $m_{j}=m$ and $t_{ij} = t_{j}$ for $j=1,..,m$ and $i=1,...,n$. This ensures the estimated slopes are comparable between individuals. The forward difference quotient equals 
$$\hat{f}_{i}^{'}(t_{j}) = (y_i(t_{j+1})-y_i(t_j))/(t_{j+1}-t_j).$$
It is an unbiased estimate of $f_{i}^{'}(\tau)$ for $\tau\in[t_{j},t_{j+1}]$ with
$$E(\hat{f}_{i}^{'}(t_{j})) = f_i(t_{j+1})-f_i(t_j))/(t_{j+1}-t_j)$$
by the mean value theorem but is highly variable if $\sigma_{i}^{2}$ is large since
$$\text{Var}(\hat{f}_{i}^{'}(\tau)) =  2\sigma^{2}_{i}/ (t_{j+1}-t_j)^{2}.$$

This variability can impact the final clusters if enough estimates have the opposite sign of the true derivative. Now, one way to minimize the variance is to maximize the time between observations. For example, observing only two observations, one at baseline and another at the end of follow up, minimizes the variance but at the expense of observing the rate of change during follow up. If time of observations are densely sampled, a functional approach is to smooth out the noise using splines to estimate the function and then the derivative function. In either circumstance, the derivatives are independently estimated for each individual and there is no direct way to borrow strength between individuals to better estimate the derivative even if subjects are thought to have a common shape. 

The main concern is how well these methods can cluster based on shape; therefore, it is important to determine the behavior of Euclidean distance using these estimated derivatives and how well it can distinguish between two noisy curves in terms of their shape over time. Assume there are three subjects ($n=3$) observed ten times ($m=10$) at uniformly at intervals of $\Delta_{t}$, $\B t_1 = \B t_2=\B t_3 = (\Delta_t,2\cdot\Delta_t,...,10\cdot\Delta_t)$. Two subjects have the same horizontal shape over time and the third has a positive slope, $f_1(t) = f_2(t) =0$, $f_3(t)=a*t$, and the variability of the noise is the same amongst the subjects, $\sigma_1=\sigma_2=\sigma_3=\sigma$. The outcomes for individual $i$ at the $j$th observation time equals
$$y_{ij}=f_{i}(t_{j})+\epsilon_{ij}\quad \epsilon_{ij}\sim N(0,\sigma^{2})$$
for $i=1,2,3$ and $j=1,...,10$. The vectors of calculated slopes for the three subjects are denoted as $\B d\B y_1$, $\B d\B y_2$, $\B d\B y_3$. We are interested in the probability of the event that the distance between $\B d\B y_1$ and $\B d\B y_2$, which are both generated from the same horizontal shape, is less than the distance between $\B d\B y_2$ and $\B d\B y_3$, which are generated from different shapes. Given values for $\sigma$, $\Delta_t$, $a$, I empirically estimate
$$P(\|\B d\B y_1-\B d\B y_2\|^2_2 < \|\B d\B y_2-\B d\B y_3\|^{2}_{2})$$
through generating data for the three subjects 5000 times. We completed this simulation under many conditions specified by a combination of $a = 0.25, 1, 5$, $\Delta_t = 0.5,1,1.5,2,2.5,3$, and $\sigma = 0.5,1,1.5,2,2.5,3$. The estimated probabilities are plotted against the ratio, $\sigma/\Delta_t$, for different slopes of $f_{3}(t)$ (Figure \ref{fig:3-3}). 

\begin{figure}
\begin{center}
\includegraphics[height=4in]{Chp3Deriv}
\end{center}
\caption{Empirical probabilities that the distance between $\B d\B y_{1}$ and $\B d\B y_{2}$, which share the same underlying horizontal function, is smaller than the distance between $\B d\B y_{1}$ and $\B d\B y_{3}$ for different ratios of standard deviation to length of time lags and slopes of the linear function underlying $\B y_{3}$ based on 5000 replications.}
\label{fig:3-3} 
\end{figure}

We note that as the ratio of variance to time interval increases, the probability of the two horizontal observed vectors being closer than the two vectors from different shapes decreases to 0.50. Therefore, if the variance is large in comparison to the sampling increments, it is a toss up as to whether difference quotient method correctly determines which vectors were generated from the same shape. On the other hand, if the variance is small relative to the time between samples, the procedure finds the two horizontal lines more similar than two lines with different slopes most of the time. As the slope increases and thus the difference between shapes increases, the method correctly finds the horizontal lines more similar. There is a trade off of bias and variance so if the individuals are not observed frequently relative to the measurement error and degree to which the shapes drastically differ, this clustering procedure may behave appropriately by grouping individuals with similar patterns over time. 

\subsection{Correlation-based dissimilarity}
The Pearson correlation coefficient has been used to measure dissimilarity between two vectors based on the shape of the data  \cite{chouakria2007,  eisen1998, chiou2008}. In the context of functional clustering, \textcite{chiou2008} suggested using the functional correlation as a similarity measure to cluster similar functions. This clustering approach requires densely collected longitudinal data and therefore, it is not applicable for this thesis since the number of observations is relatively small, less than 15. Despite the wide use, there has been little to no discussion about how well this measure does to discriminate between shapes.

In the multivariate setting, a dissimilarity measure based on the Pearson correlation coefficient between two comparable vectors equals 
$$d_{Cor}(\B x,\B y) = 1-Cor(\B x,\B y)$$ 
where $$Cor(\B x,\B y) = \frac{\sum^{m}_{j=1}(x_{j}-\bar{x})(y_{j}-\bar{y})}{\sqrt{\sum^{m}_{j=1}(x_{j}-\bar{x})^{2}}\sqrt{\sum^{m}_{j=1}(y_{j}-\bar{y})^{2}}}.$$
Assume the data vectors are generated from the same model presented for the derivative-dissimilarity measure. Imagine $\epsilon_{ij}=0$ for $i=1,...,n$ and $j=1,...,m$. A vector with an underlying functional shape of $f(x)$ is perfectly positively correlated with a vector with functional shape $af(x) + b$ where $a\geq0$ and $b\in\mathbb{R}$. As a result, two vectors that have the same shape but at different levels have a dissimilarity of zero since the correlation coefficient equals 1. However, two vectors that are multiples of each other are placed in the same cluster despite drastically different shapes. While this method succeeds in grouping individuals with the same shape at different levels, it fails in the sense that is also groups individuals with different shapes. If the data is observed with error such that $\epsilon_{ij}\sim(0,\sigma_{i}^{2}),$ the dissimilarities are be larger when the noise is large relative to the functional shape. 

Now, if two vectors are noiseless observations of $f(x)=c$ where $c\in\mathbb{R}$,  the correlation coefficient is undefined. When this type of data is observed with noise, the correlation and thus the dissimilarity will be quite variable when the number of observations is small and the magnitude of noise is large. In other words, the Pearson correlation coefficient cannot consistently detect that two horizontal curves have the same shape. This methods fails to group by shape when $\sigma_{i}^{2}$ is large, number of observations is low, and the data set includes horizontal patterns over time. These characteristics do not get discussed in the literature, but they have huge ramifications for clustering longitudinal data.

\section{Discussion}
The standard clustering methods generally fail to answer the three research questions posed at the beginning of this chapter: Are there distinct shape patterns in the data? How many patterns are there? Are there baseline factors that impact the shape of an individual's trajectory? There have been some attempt to adjust the input into partition algorithms to focus on shape, but these methods based on difference quotients and the Pearson correlation coefficient generally fail when data observed equals a smooth function plus moderate variability. Additionally, both methods require data to be collected at fixed time points for all of the subjects. Ideally, we want a method that exploits the properties of longitudinal data, which includes the possibility of unbalanced, irregular data.

We propose three extensions of existing method that fit this criteria while attempting to answer the research questions about shape. Before we discuss the proposed methodology in the next chapter, it is worth discussing the decomposition of a curve into level and shape and how that translates into a model. There are two common data-generating models that suggest groups are based on shape and individuals differ in the vertical level within the group. In a functional data approach \cite{ramsay2002}, assume that the $j$th observation for the $i$th individual at time $t_{ij}$ is
$$y_{ij}= f_i(t_{ij})+\epsilon_{ij}$$
for $i=1,...,n$ and $j=1,...,m_{i}$ where $\epsilon_{ij}\overset{iid}{\sim} (0,\sigma_{i}^{2})$ and $f_{i}$ is a random function sampled from a Hilbert space of square integrable functions on a real interval. 

In a typical longitudinal data analysis approach \cite{diggle2002}, assume that the $j$th observation for the $i$th individual at time $t_{ij}$ in group $k$ is
$$y_{ij}=\lambda_{i}+f_k(t_{ij})+\epsilon_{ij}$$
for $i=1,...,n$ and $j=1,...,m_{i}$ where $\lambda_{i}$ adjusts the vertical level of an individuals' curve, $f_k$ is a continuous mean function for cluster $k$, and $\epsilon_{ij}$ is random error that may be correlated within subject $i$ but independent between subjects. If $\lambda_{i}$ is assumed random, then the model is considered a random intercept model. 

 In the next chapter, I present three new methods adapted from related literature that address issues of shape or pattern over time and then compare them with the methods discussed in this chapter.	
