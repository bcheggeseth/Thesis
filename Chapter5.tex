\chapter{Simulations}

In this chapter, I compare the three proposed methods presented in the last chapter with standard clustering methods through a simulation study. There is not one method that works well in all situations and for all research questions. The goal of this study is to understand the behavior of these methods when applied to data with individuals with the same shape trajectory at a variety of levels. The data are generated under a variety of conditions to test the methods on their robustness to model misspecification, noise, and group overlap. The outcomes of interest are the data-driven number of groups chosen and the methods' ability to detect shape groups despite differences in level. \\

First, I describe the data-generating process used throughout the simulation study. Then, I discuss the specific details of the simulation implementation in terms of methods used, the number of repetitions, and the outcomes of interest. Lastly, I present the results and compare the performance of the new methods to the standard.
\section{Data-Generating Process}
When deciding upon the data-generating process, it is important to choose a simple example that addresses the our goals and could generalize to a more complex situations. I have chosen to have three trajectory shapes for individuals to follow over time. Since many complex functions can be locally approximated with linear functions, the three shapes used in the simulation are straight lines with different slopes---positive, negative, and zero. After establishing the general shapes, it is necessary to include diversity in the level of the trajectories. The average levels can be placed into three groups: low, middle, and high. I translated these groups into five distinct mean functions with three shapes and three levels:
\begin{align*}
g_{1}(t) &= -1 - t\\
g_{2}(t) &= 11 - t\\
g_{3}(t) &= 0\\
g_{4}(t) &= -11 + t\\
g_{5}(t) &= 1 + t\\
\end{align*} 
To induce a relationship between the shape and level groups, I restricted all horizontal lines to be in the middle level and all non-horizontal lines to be in either the high or low group. I started with the simple functions $g_{1}, g_{3}, g_{5}$ to represent the three shapes and then vertically shifted $g_{1}$ and $g_{5}$ by 12 units to get the other two function $g_{2}$ and $g_{4}$. The magnitude of the shift was chosen so that the lines crossed in the time of observation. The group mean functions were chosen so that there was enough overlap to challenge the cluster methods since it is easy to cluster successfully when groups are clearly differentiated.   \\

With the mean function set, I specify the mechanism with which to determine an individual's shape and level group and thus their mean function. I introduce two binary baseline factors to determine function group membership, the first of which impacts the shape and the other which impacts the level. Individuals are randomly assigned values of these two binary factors, $z_{1}$ and $z_{2}$, with the independent simulated tosses of a fair coin such that $P(z_{1}=1) = P(z_{1}=0) = 0.50$ and $P(z_{2}=1) = P(z_{2}=0)=0.50$. \\

Let $S$ be a categorical random variable that indicates the shape group. If $S=1$, then the individual is in the negative slope group. $S=2$ refers to the horizontal group and $S=3$ indicates the positive slope group. Then conditional on the baseline factors, the probability of being in a shape group equals
$$P(S=k |z_{1}) = \frac{\exp(\gamma_{0k}+\gamma_{1k}z_{1})}{\sum^{3}_{l=1} \exp(\gamma_{0l}+\gamma_{1l}z_{1})}$$
for $k=1,2,3$ where $\gamma_{01}=2,\gamma_{11} = -4,\gamma_{02}=1.5,\gamma_{12}=-2,\gamma_{03}=\gamma_{13} = 0$ and $z_{1}\in\{0,1\}$. With these chosen coefficients, the change of being in the horizontal group does not depend on $z_{1}$. We note that the following statement is true and can be shown through cross multiplication.
\begin{align*}\frac{\exp(1.5)}{\sum^{3}_{l=1} \exp(\gamma_{0l})} & =\frac{\exp(-0.5)}{\sum^{3}_{l=1} \exp(\gamma_{0l}+\gamma_{1l})}\\
\exp(1.5)\sum^{3}_{l=1} \exp(\gamma_{0l}+\gamma_{1l}) & =\exp(-0.5)\sum^{3}_{l=1} \exp(\gamma_{0l})\\
\exp(-0.5)+\exp(1)+\exp(1.5)& =\exp(1.5)+\exp(1)+\exp(-0.5)\\
\end{align*}
Therefore, $P(S=2|z_{1}=1) = P(S=2|z_{1}=0) = 0.34$. The chances of being in the positive and negative slope group do depend on $z_{1}$ with $P(S=1|z_{1}=1) = 0.58 = P(S=3|z_{1}=0)$ and $P(S=1|z_{1}=0) = 0.08 = P(S=3|z_{1}=1)$. Since the value of $z_{1}$ is determined by a coin toss, on average, each shape group has about an equal  frequency. \\

The second factor impacts the level, but I placed restrictions when creating the mean functions; therefore, level and shape are not independent of each other. Let $L$ be a categorial random variable that indicates level group. If $L=1$, an individual is in the low group. $L=2$ indicates middle and $L=3$, high. Conditional on the shape group, all of the horizontal and only the horizontal lines can be in the middle level despite the value of the second factor:
\begin{align*}
P(L=2|S=2, z_{2}) & = 1\\
P(L=2|S=1 \text{ or } S=3, z_{2}) & = 0
\end{align*}
The second factor does impact the level for those in either the negative or positive slope groups with
\begin{align*}
P(L=k|S=1 \text{ or } S=3,z_{2}) & = \frac{\exp(\zeta_{0k}+\zeta_{1k} z_{2})}{\sum_{l\in\{1,3\} }\exp(\zeta_{0l}+\zeta_{1l}z_{2})}\\
P(L=k|S=2,z_{2}) & =0
\end{align*}
for $k=1,3$ and $z_{2}\in\{0,1\}$ where $\eta_{01}=0,\zeta_{11}=0,\zeta_{03}=-3,\zeta_{13}=6$. Therefore, $P(L=1|S=1 \text{ or }S=3,z_{2}=0) = 0.95 = P(L=3|S=1 \text{ or }S=3,z_{2}=1)$ and $P(L=1|S=1 \text{ or }S=3,z_{2}=1) = 0.05 = P(L=3|S=1 \text{ or }S=3,z_{2}=0)$.  Due to the contraction of the shape groups and the baseline factors, this results in about equal frequencies within each level on average.\\

To summarize, individual trajectories are generated by first simulating two coin tosses to determine values for the two baseline binary factors. Then conditional on the factors, shape and level groups are randomly assigned by plugging the factors into the generalized logit functions above and drawing from a multinomial distribution. At this point, the mean function is set for a particular individual since the groups map to the functions
\begin{align*}
S = 1,\; L=1 \rightarrow g_{1}\\
S = 1,\; L=3 \rightarrow g_{2}\\
S = 2,\; L=2 \rightarrow g_{3}\\
S = 3,\; L=1 \rightarrow g_{4}\\
S = 3,\; L=3 \rightarrow g_{5}\\
\end{align*}

In order to generate data for an individual, I evaluate the mean function at five equidistant observation times $t=1,3.25,5.5,7.75,10$ that span the period 1 to 10 units. Then,  random noise is added to the mean values to create variability. The random noise is made up of two components: individual-specific level perturbation and time-specific Gaussian measurement error. For individual $i$ ($i=1,...,n$) at the $j$th observation time ($j=1,..,5$) with the $l$th mean function, the observed outcome equals
$$y_{ij} = g_{l}(t_{j})+\tau_{i}+\epsilon_{ij}\quad\text{where}\quad \epsilon_{ij}\sim N(0,\sigma_{\epsilon}^{2}), \tau_{i}\sim F(0,\sigma_{\tau}^{2})$$
where $\sigma_{\epsilon}$  is the standard deviation of the measurement error and $\sigma_{\tau}$ is the standard deviation and $F$ is the probability distribution of the level perturbation.\\
 
The level perturbation can be viewed as a random intercept that induces constant within individual correlation between the random deviations from the mean. The distribution of the level perturbation as well as the standard deviations of the random noise are values that vary in the simulation to create different conditions on which to test the clustering methods. The values are chosen to alter the overlap of the groups and the signal to noise ratio and are discussed in the next section.
\section{Simulation Implementation}
In this section, I describe the random noise values chosen for this simulation study and briefly overview the included clustering methods. 
\subsection{Random Noise}
The first components of the random noise is the level perturbation. Since many methods assume normality directly or indirectly, I use the uniform distribution in addition to the Gaussian distribution to generate the level perturbations to test the sensitivity of the methods to distributional assumptions. To impact the overlap of groups, I choose the standard deviation of the perturbation by dividing the magnitude of the vertical shift so that there are 4 or 6 standard deviations between the mean functions. This results in standard deviations of 2 and 3. Lastly the size of measurement error influences the signal to noise ratio and I use extreme standard deviations of 0.5 and 2. There are eight possible combinations of these three properties, representing the eight conditions of the data-generating process I use in the simulation study. 
\subsection{Clustering Methods}
For each of the eight conditions, I let $n=500$ and draw 500 individuals from the data-generating process previously described and apply each cluster method to partition the data into $K=2,3,4,$ and $5$ groups. However, each method produces different output making comparisons difficult. \\

Partition methods produce group labels while fitting a mixture model results in posterior probability estimates for each individual and group. Therefore, I translate the estimated probabilities into labels by placing individuals into the group that has the highest posterior probability.\\

Besides group labels, the representative `curve' of each group is provided in a different format. The K-means algorithm results in a mean of the vectors in each group. The vector output depends on the input vectors. They could be the raw data vectors or transformations of the data. On the other hand, I model the mean with a function that is linear in spline basis functions in the model-based clustering methods. The output includes the estimated coefficients for the group-specific mean functions. To make the resulting group representatives from the partition and model-based methods comparable, I transform all output into a derivative function which describes the shape of the curves. I project the raw data vectors onto a common spline basis, if not already in that form, differentiate the spline functions, and rearrange terms to calculate the coefficients of the spline function one order lower. \\

The common spline used throughout the simulation study is a quadratic B-spline with no internal knots and boundary knots at 0.5 and 10.5. A quadratic B-spline basis without internal knots is equivalent to a Bernstein polynomial basis (cite) which generalizes the standard polynomial basis; however, for the purposes of this thesis, I advocate using a B-spline basis that provides as much flexibility as possible given the number of observations per individual and is computationally stable (cite).  As mentioned above, this function is then differentiated and the coefficients are transformed in order to get the derivative coefficients of a linear B-spline. See Chapter 4 for more details on derivatives of B-spline functions.\\

Lastly, for every clustering method, I estimate the relationship between the two baseline factors, $z_{1}$ and $z_{2}$, and the group membership. This estimation procedure differs between the methods as it may occur while simultaneously estimating group membership or after the clustering algorithm is complete. \\

\noindent \textbf{K-means}\\
The K-means algorithm is a general clustering procedure for data vectors. $K$ groups are determined through an iterative assignment and update process to minimize the within-group sum of squares distance to the group mean vectors. Note that the objective function, defined as the within-group sum of squares, indirectly imposes a spherical shape to the groups in the vector space.\\

To choose the optimal value of $K$ in this simulation study, I use the silhouette measure, which measures the compactness of clusters, as previously introduced in this thesis. \\

To estimate the relationship between baseline factors and group membership from the K-means algorithm, the groups labels are taken as known and become the outcomes for a multinomial logistic regression with the observed baseline factors as explanatory variables. Let $c_{i}$ be the group label for individual $i$, then the model assumes that
$$\log\frac{P(c_{i}=k)}{P(c_{i}=K)} = \eta_{0k}+\eta_{1k}z_{i1}+\eta_{2k}z_{i2}$$
for $k=1,..,K-1$. Estimates for the coefficients, $\bs\eta = (\eta_{01},...,\eta_{2K-1})$, are produced through maximum likelihood estimation if the size of each group is adequately large. Both procedures to choose $K$ and estimate the relationship with baseline factors are used for all applications of K-means in this study.\\

\textbf{K-means on Raw Data}\\
The K-means algorithm applied to the 500 raw data vectors of length 5 results in groups based on both the level and shape. Both characteristics can play a role since this method is based only on the squared Euclidean distance of the raw data. In order to compare the estimated group representatives to those of the following methods, the $K$ group mean vectors are projected onto the common B-spline basis and the coefficients are then transformed into derivative coefficients.  \\

\textbf{K-means on Spline Coefficients}\\
To focus on the individual trends without noise, the individual raw data vectors can be projected onto the common B-spline basis prior to clustering. This process in effect smoothes each individual trajectory into a quadratic function. The vector of basis coefficients for each individual are then used as the input for the K-means algorithm. This technique originates in the Functional Data Analysis literature (cite Silverman and Ramsay) and has been used as a way to decrease the dimension of the data before clustering. \\

\textbf{K-means on Derivative Spline Coefficients}\\
This is the first proposed method of this thesis. A slight adjustment to the previous method allows the clustering to be based on the derivative, which describes the shape, rather than the original function. Before algorithm is applied, the coefficients from the basis are transformed such that they become the coefficients to the derivative function written as a B-spline basis of degree 1. \\

\textbf{K-means on Raw Derivatives}\\
Another way of clustering on the basis of derivatives is to calculate raw difference quotients by taking the pair-wise differences within the ordered data vector and divide by the time between observations
$$\Delta_{ij} = \frac{y_{ij}-y_{ij-1}}{t_{j}-t_{j-1}}.$$
The vector of difference quotients can be used as the input for the K-means algorithm.\\

\textbf{PAM with Correlation Measure}\\
Partitioning around medoids (PAM) is a variant of the K-means algorithm. Rather than focusing on the distance to the centroid or group mean, this method uses the center data vector or medoid as the foundation of the groups, which results in a more robust procedure less sensitive to outliers. Also, the algorithm is general enough to allow for other measures of dissimilarity beyond the square Euclidean distance, used in K-means. For this study, the chosen measure is a linear transformation of the Pearson's correlation coefficient, which measures the linear association between two vectors and is thought to detect similarity in the shape of longitudinal data (cite Chiou 2008). The dissimilarity between two vectors is defined as
$$d(\B y_{1},\B y_{2}) = (1-Cor(\B y_{1},\B y_{2}))/2$$
such that perfect positive correlation results in a dissimilarity of 0 and perfect negative correlation results in a value of 1. Thus, if a vector is an additive shift of another vector, they are deemed to be similar. This supports one goal of our methods. However, the same can be said for multiplicative transformations of data vectors. Therefore, this measure cannot detect magnitude differences in the slope of trend lines and cannot distinguish between a horizontal line and that with positive or negative slope.\\

To choose the number of groups and estimate the relationship between baseline factors and group membership, the same procedures as the K-means algorithm are used.\\

\noindent \textbf{Finite Mixture Model}\\
In contrast to the partition methods mentioned above, using a probability model allows for uncertainty in the model parameters and group membership. The data are fit to the chosen model using the Expectation Maximization algorithm in order to maximize the likelihood function. Rather than being separated into $K$ groups, fitting mixture model involves allowing individuals contribute to every group to some degrees. For example, if two groups are not well separated, then one individual between the groups could contribute equally to the estimation of both groups' parameters. \\

For this simulation study, I generally assume the $K$ components of the finite mixture model are multivariate Gaussian with a component-specific mean function that can be expressed with a quadratic B-spline with boundary knots at 0.5 and 10.5. The coefficients for the mean parameters are then transformed into the coefficients for the derivative spline function for the final presentation. Additionally, the prior probabilities of the components are parameterized using a generalized logit function such that the logit is equal to a linear combination of the two baseline factors, $z_{1}$ and $z_{2}$. For individual $i$, 
$$\log\frac{P(c_{i}=k)}{P(c_{i}=K)} = \log\frac{\pi_{k}(z_{i1},z_{i2},\bs\eta)}{\pi_{K}(z_{i1},z_{i2},\bs\eta)} =  \eta_{0k}+\eta_{1k}z_{i1}+\eta_{2k}z_{i2}.$$
The parameters, $\bs\eta$ are estimated simultaneously with the mean and covariance parameters. 
The number of components or groups is selected by minimizing the model selection criteria, Bayesian Information Criteria (BIC), over model fits with $K=2,3,4,5$. \\ 

\textbf{Finite Mixture Model with Independence}\\
For the standard model, the covariance structure is assumed to be independent such that the variance is constant over time but differs between components. Therefore, the only covariance parameters needed in the model are the component-specific variances.\\

\textbf{Finite Mixture Model with Exponential Correlation}\\
As discussed in Chapter 2, assuming independence for longitudinal data when there is inherent dependence between repeated measures can result in bias in parameter and inference estimates. Therefore, we also use a more general correlation structure such as exponential that maintains the stationarity of the covariance structure but allows the correlation to decay as the time between observations increases. The number of covariance parameters increases to two per component: constant variance and range of dependence. \\

\textbf{Multilayer Mixture Model}\\
This is the second proposed method of this thesis. In generalizing the model to allow shape groups to have different level groups, I assume that each shape component of the mixture model is a mixture of level subcomponents assumed to be multivariate Gaussian. In order to control of the number of parameters to be estimated, I assume independence within each subcomponent. For this two-tiered model, the prior probabilities for the shape components, rather than those of the level subcomponents, are parameterized to relate baseline factors to group membership. In addition to specifying the number of shape components, the number of level components within each shape component need to be set prior to estimating the parameters. It is difficult to choose both of these values in an automatic way; therefore, we fix the number of shape components to be $K=3$ since they are known a priori. Then we use the BIC to select the number of subcomponents with the shape components (cite the multilayer paper). \\

\textbf{Vertically Shifted Independence Mixture Model}\\
This is the third proposed method of this thesis. Rather than trying to explicitly model the level differences as subgroups, I remove the level prior to fitting the model by subtracting the individual-specific mean. Before the model is fit to the data, the trajectories are vertically shifted so they lie in the same range. The hope is that shape drives the group development now that the level is removed. As mentioned in the last chapter, subtracting the mean does impact the correlation structure and could potentially result in negative correlation between some observations, which is hard to model with a correlation function of only a few parameters. Therefore, I use the independence correlation structure to model the leftover correlation after removing the mean. The same estimation and modeling procedures as the standard finite mixture model apply.

\textbf{Vertically Shifted Exponential Mixture Model}\\
Rather than assuming independence for the transformed vector, I use the exponential correlation structure model the error structure after removing the mean.

\subsection{Outcomes of Interest}
For each method and condition, we generate a data set of $n=500$ individuals and calculate the following outcomes of interest. This is then repeated $B=500$ times such that we get 500 unique data sets under each condition on which we can apply each methods and summarize the results.\\

\textbf{Number of Groups}\\
For each replication, I use the method-specific procedure of choosing the final data-driven number of groups and record the final value of $K$ for each data set of 500 individuals. Note that this $K$ is not chosen for the multilayer mixture as the number of shape groups is assumed to be equal to three.\\

\textbf{Misclassification Rate}\\
To detect whether the method discovers the underlying shape structure, I let $K=3$ and compare the cluster memberships labels to the true shape group membership using a contingency table. To measure how well the method aligned with the truth, I first reordered the cluster label columns of the contingency table such that the trace of the 3 by 3 inner matrix is maximized. Then the number of individuals on the off-diagonal of the newly permuted table represents the number of misclassified individuals. The best possible performance for a method would result in zero misclassifications. To summarize the replications, I present the mean misclassification rate.\\

\textbf{Adjusted Rand Index}\\
Another way to measure the similarity between  the clustering and the true shape clustering is the Adjusted Rand Index (cite Hubert and Arabie from Comparing Parititons and milligan and Cooper recommending it). This measure assesses the agreement of two partitions. In our case, the true shape partition is used as a reference and I compare the partition when $K=3$.\\
 
Given a set of $n$ objects, $S$, suppose $U = \{u_{1},...,u_{K}\}$ and $V=\{v_{1},...,v_{K'}\}$ represent two different partitions of the objects such that $\cup^{K}_{i=1}u_{i} = S = \cup^{K'}_{j=1}v_{j}$ and $u_{i}\cap u_{i'} = \emptyset = v_{j}\cap v_{j'}$ for $1\leq i\not=i'\leq K$ and $1\leq j\not= j'\leq K'$. The information on the group overlap between the two partitions can be written in the form of a contingency table with elements $n_{ij}$ denoting the number of objects that are common to groups $u_{i}$ and $v_{j}$ and $n_{i\cdot}$ and $n_{\cdot j}$ denoting row and column sums, respectively. Let $a$ be the number of pairs of objects that are placed in the same group in $U$ and in the same cluster in $V$, $b$ be the number of pairs of objects in the same group in $U$ but not in the same cluster in $V$, $c$ be the number of pairs of objects in the same cluster in $V$ but not in the same group in $U$, and $d$ be the number of pairs of objects in different groups and different clusters in both partitions. The quantities $a$ and $d$ are interpreted as agreements and $b$ and $c$ as disagreements. The Rand Index (cite) is simply $\frac{a+d}{a+b+c+d}=\frac{a+d}{{n \choose 2}}$.  The index can yield a value between 0 and 1, where 1 indicates that the clusters are exactly the same.\\

The main issue with the Rand index is that the expected value of the index does not take on a constant value and thus it is hard to evaluate raw indices. Hubert and Arabie suggested using the generalized hypergeometric distribution for the null model for randomness such that the partitions are picked at random. This leads to the Adjusted Rand Index, which is of the form $\frac{\text{Index} - \text{Expected Index}}{\text{Max Index}-\text{Expected Index}}$. The index is bounded above by 1 and takes on the value 0 when the index equals its expected value. In the contingency table notation, the Adjusted Rand Index can be simplified to
$$\frac{\sum_{ij}{n_{ij} \choose 2} - \sum_{i} {n_{i\cdot} \choose 2} \sum_{i} {n_{\cdot j} \choose 2} / {n \choose 2}}{\frac{1}{2}\left[\sum_{i}{n_{i\cdot} \choose 2}+\sum_{j}{n_{\cdot j} \choose 2}\right] - \sum_{i} {n_{i\cdot} \choose 2} \sum_{i} {n_{\cdot j} \choose 2} / {n \choose 2}}$$
A method that performs well would have an Adjusted Rand index Value close to 1. To summarize the replications, I present the mean Adjusted Rand Index value.\\

\textbf{Mean Parameters}\\
As mentioned in the brief descriptions of the methods used in the simulation, the output of the mean or medoid differ between each method. Six of the nine methods use the B-spline basis to model the mean as a function and one method explicitly uses the derivative of the B-spline function. It is not possible to map a derivative function back to the original function. The main focus of this thesis is the shape; therefore, I set $K=3$ and map the clustering output to the derivative B-spline function so that we can compare estimates of the shape within each group. The true mean curves are projected onto the B-spline basis and then differentiated. The coefficients of the derivative spline function are used as the true value for reference. \\

Just as with the misclassification rate, it is necessary to match the group labels to those of the true generating distribution as best as possible. This is completed by permuting the columns until the trace of the 3 by 3 contingency table of the true labels and the clustering group labels is maximized. Once the groups are aligned as best as possible to the truth, the mean squared error is estimated for each derivative coefficient using the 500 replications.\\

\textbf{Baseline Factors}\\
Besides detecting group structure in the data, the information of most practical importance is the relationship between baseline factors and the group membership. Once the groups are discovered, the natural question is to ask who are in the groups and how else do they differ? For each condition, method, and replication, the coefficients of the generalized logit, $\{\eta_{k0},\eta_{k1},\eta_{k2}\}$ for $k=1,...,K-1$ are estimated either simultaneously in the model setting or a posterior in the partition methods. To summarize the parameters from all of the replications, we plot the density of the values assuming $K=3$ and compare them to the true generating values. The clustering is unstable if the density plot is multimodal.

\section{Results}
Tables \ref{tab:freq1}, \ref{tab:freq2}, and \ref{tab:freq3} summarize the simulations in terms of the number of groups, average misclassification rate, and average Adjusted Rand Index. It is clear from these tables that three methods---the standard K-means algorithm applied to raw data, K-means on spline coefficients, and the Gaussian mixture model assuming independence---do not select three groups as the optimal number of groups (Table \ref{tab:freq1}). For all the conditions except the one when $\sigma_{\epsilon}=0.5$ and $\sigma_{\tau}=2$, the raw data K-means algorithm obscures any shape differences in the data by vertically splitting the individuals in half: high and low groups. If the noisy from the data is smoothed using a spline basis, K-means applied to the coefficients tends to favor five groups except in the extreme cases of large standard deviations which results in increased overlap between groups and the algorithm selects two groups. The BIC with the independent mixture model consistently chooses five groups, the maximum we allow in the simulation, under all conditions. All three of these methods do not perform well when forced to choose $K=3$. Only about 50\% of the data is correctly specified in terms of the generating shape groups. Small values for the Adjusted Rand Index also indicate that groups from these methods do not match shape partitions.\\

If the correlation structure of the finite mixture model is generalized from conditional independence to exponential correlation, there is more variability in the number of groups chosen. When the time-specific measurement error was small ($\sigma_{\epsilon}=0.5$), the correct number of groups were chosen about 30\% of the time (Table \ref{tab:freq1}). Additionally, under these same circumstances, when forced to have 3 groups, there was perfect alignment with the generating shape groups. However, when the magnitude of the noise is large ($\sigma_{\epsilon}=2$), three groups was never chosen and shape groups are not discovered when $K=3$. So even though this method does not directly cluster based on shape, having a general enough correlation structure allows  the possibility of the method to pick up the correct groups when the number of group is known and the measurement error is small. It is important to note that it does not choose the correct number of groups in the majority of simulations. \\

Of the established methods that are intended to group on shape, K-means on quotient differences can only pick the correct number of groups if the magnitude of the measurement error is small (Table \ref{tab:freq2}). If the variability around the individual mean is large, the method chooses 2 groups and misclassifies about 38\% of the individuals when forced to have 3 groups. Using the correlation dissimilarity measure with the PAM algorithm gives slightly better results with only 24-28\% misclassification, but the it does not consistently choose three groups. The chosen number is quite variable with 2 and 5 groups being chosen the most.\\

Now, we compare the three proposed methods (Table \ref{tab:freq3}). Using K-means with the derivative spline coefficients is a slight improvement over the raw derivatives such that the true number of groups was chosen much more frequently even under conditions of higher measurement error. However, this method did not decrease the misclassification rate under those same conditions. \\

The multilayer mixture model is only successful in correctly classifying individuals into shape groups when there are distinct subgroups that are well separated in terms of vertical level ($\sigma_{epsilon}=0.5, \sigma_{\tau}=2$). Even though the components are assumed Gaussian, the method seems to be robust to some misspecification as none of the components were generated as independent Gaussians. \\

Lastly, the method that prevailed amongst the competition is the vertically shifted mixture model. In this case, either correlation assumption worked well. For every condition, the method chose three groups as the optimal partition and when forced to $K=3$, the method discovered the shape groups with little misclassification. Only when the measurement error is large ($\sigma_{epsilon}=2$) did the method misclassify 5\% (about 25 individuals) in terms of shape. \\ 

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Mon Jan 28 09:13:22 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{ccc|cccccc}
  \hline &&&\multicolumn{4}{c}{Number of Groups}&Misclassification&Adjusted\\ Distribution of $\tau$&$\sigma_{\epsilon}$&$\sigma_{\tau}$&$K=2$&$K=3$&$K=4$&$K=5$&Rate&Rand Index\\ \hline\multicolumn{9}{c}{\textbf{K-means on Raw Data}}\\ Uniform & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 500.00 & 0.42 & 0.26 \\ 
  Uniform & 2.00 & 2.00 & 492.00 & 0.00 & 0.00 & 8.00 & 0.42 & 0.24 \\ 
  Uniform & 0.50 & 3.00 & 500.00 & 0.00 & 0.00 & 0.00 & 0.51 & 0.09 \\ 
  Uniform & 2.00 & 3.00 & 500.00 & 0.00 & 0.00 & 0.00 & 0.51 & 0.09 \\ 
  Gaussian & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 500.00 & 0.38 & 0.33 \\ 
  Gaussian & 2.00 & 2.00 & 273.00 & 0.00 & 0.00 & 227.00 & 0.40 & 0.29 \\ 
  Gaussian & 0.50 & 3.00 & 412.00 & 0.00 & 0.00 & 88.00 & 0.46 & 0.16 \\ 
  Gaussian & 2.00 & 3.00 & 500.00 & 0.00 & 0.00 & 0.00 & 0.47 & 0.14 \\ 
   \\ \multicolumn{9}{c}{\textbf{K-means on Splines Coefficients}}\\Uniform & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 500.00 & 0.46 & 0.19 \\ 
  Uniform & 2.00 & 2.00 & 31.00 & 0.00 & 1.00 & 468.00 & 0.47 & 0.18 \\ 
  Uniform & 0.50 & 3.00 & 189.00 & 0.00 & 10.00 & 301.00 & 0.48 & 0.17 \\ 
  Uniform & 2.00 & 3.00 & 500.00 & 0.00 & 0.00 & 0.00 & 0.48 & 0.15 \\ 
  Gaussian & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 500.00 & 0.43 & 0.22 \\ 
  Gaussian & 2.00 & 2.00 & 0.00 & 0.00 & 0.00 & 500.00 & 0.45 & 0.20 \\ 
  Gaussian & 0.50 & 3.00 & 1.00 & 0.00 & 0.00 & 499.00 & 0.46 & 0.18 \\ 
  Gaussian & 2.00 & 3.00 & 490.00 & 0.00 & 0.00 & 10.00 & 0.47 & 0.16 \\ 
   \\ \multicolumn{9}{c}{\textbf{Independent Mixture}}\\Uniform & 0.50 & 2.00 & 0.00 & 0.00 & 2.00 & 498.00 & 0.44 & 0.22 \\ 
  Uniform & 2.00 & 2.00 & 0.00 & 0.00 & 2.00 & 498.00 & 0.40 & 0.28 \\ 
  Uniform & 0.50 & 3.00 & 0.00 & 0.00 & 1.00 & 499.00 & 0.49 & 0.12 \\ 
  Uniform & 2.00 & 3.00 & 0.00 & 0.00 & 2.00 & 498.00 & 0.49 & 0.12 \\ 
  Gaussian & 0.50 & 2.00 & 0.00 & 0.00 & 10.00 & 490.00 & 0.41 & 0.25 \\ 
  Gaussian & 2.00 & 2.00 & 0.00 & 0.00 & 24.00 & 476.00 & 0.39 & 0.29 \\ 
  Gaussian & 0.50 & 3.00 & 0.00 & 0.00 & 1.00 & 499.00 & 0.45 & 0.17 \\ 
  Gaussian & 2.00 & 3.00 & 0.00 & 0.00 & 5.00 & 495.00 & 0.45 & 0.17 \\ 
   \\ \multicolumn{9}{c}{\textbf{Exponential Mixture}}\\Uniform & 0.50 & 2.00 & 0.00 & 65.00 & 171.00 & 264.00 & 0.00 & 1.00 \\ 
  Uniform & 2.00 & 2.00 & 0.00 & 0.00 & 23.00 & 477.00 & 0.40 & 0.40 \\ 
  Uniform & 0.50 & 3.00 & 0.00 & 113.00 & 157.00 & 230.00 & 0.00 & 1.00 \\ 
  Uniform & 2.00 & 3.00 & 0.00 & 0.00 & 37.00 & 463.00 & 0.41 & 0.35 \\ 
  Gaussian & 0.50 & 2.00 & 0.00 & 114.00 & 136.00 & 250.00 & 0.00 & 1.00 \\ 
  Gaussian & 2.00 & 2.00 & 0.00 & 0.00 & 34.00 & 466.00 & 0.41 & 0.38 \\ 
  Gaussian & 0.50 & 3.00 & 0.00 & 141.00 & 121.00 & 238.00 & 0.00 & 1.00 \\ 
  Gaussian & 2.00 & 3.00 & 0.00 & 1.00 & 40.00 & 459.00 & 0.41 & 0.35 \\ 
   \hline\end{tabular}
\caption{Frequency table of the number of groups chosen, average misclassification rate ($K=3$), and average Adjusted Rand Index ($K=3$) for 500 replications of clustering methods applied to data generated under different conditions for the distribution of $\tau$ and the standard deviation of $\epsilon$ and $\tau$.}
\label{tab:freq1}
\end{center}
\end{table}

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Mon Jan 28 09:13:38 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{ccc|cccccc}
  \hline &&&\multicolumn{4}{c}{Number of Groups}&Misclassification&Adjusted\\ Distribution of $\tau$&$\sigma_{\epsilon}$&$\sigma_{\tau}$&$K=2$&$K=3$&$K=4$&$K=5$&Rate&Rand Index\\ \hline\multicolumn{9}{c}{\textbf{K-means on Quotient Differences}}\\ Uniform & 0.50 & 2.00 & 0.00 & 500.00 & 0.00 & 0.00 & 0.00 & 1.00 \\ 
  Uniform & 2.00 & 2.00 & 480.00 & 20.00 & 0.00 & 0.00 & 0.38 & 0.31 \\ 
  Uniform & 0.50 & 3.00 & 0.00 & 500.00 & 0.00 & 0.00 & 0.00 & 1.00 \\ 
  Uniform & 2.00 & 3.00 & 480.00 & 20.00 & 0.00 & 0.00 & 0.38 & 0.31 \\ 
  Gaussian & 0.50 & 2.00 & 0.00 & 500.00 & 0.00 & 0.00 & 0.00 & 1.00 \\ 
  Gaussian & 2.00 & 2.00 & 483.00 & 17.00 & 0.00 & 0.00 & 0.38 & 0.31 \\ 
  Gaussian & 0.50 & 3.00 & 0.00 & 500.00 & 0.00 & 0.00 & 0.00 & 1.00 \\ 
  Gaussian & 2.00 & 3.00 & 483.00 & 17.00 & 0.00 & 0.00 & 0.38 & 0.31 \\ 
   \\ \multicolumn{9}{c}{\textbf{PAM with Correlation Measure}}\\Uniform & 0.50 & 2.00 & 131.00 & 105.00 & 64.00 & 200.00 & 0.24 & 0.49 \\ 
  Uniform & 2.00 & 2.00 & 219.00 & 72.00 & 99.00 & 110.00 & 0.27 & 0.46 \\ 
  Uniform & 0.50 & 3.00 & 131.00 & 105.00 & 64.00 & 200.00 & 0.24 & 0.49 \\ 
  Uniform & 2.00 & 3.00 & 219.00 & 72.00 & 99.00 & 110.00 & 0.27 & 0.46 \\ 
  Gaussian & 0.50 & 2.00 & 160.00 & 91.00 & 72.00 & 177.00 & 0.24 & 0.49 \\ 
  Gaussian & 2.00 & 2.00 & 214.00 & 95.00 & 106.00 & 85.00 & 0.28 & 0.45 \\ 
  Gaussian & 0.50 & 3.00 & 160.00 & 91.00 & 72.00 & 177.00 & 0.24 & 0.49 \\ 
  Gaussian & 2.00 & 3.00 & 214.00 & 95.00 & 106.00 & 85.00 & 0.28 & 0.45 \\ 
   \hline\end{tabular}
\caption{Frequency table of the number of groups chosen, average misclassification rate ($K=3$), and average adjusted rand index ($K=3$) for 500 replications of clustering methods applied to data generated under different conditions for the distribution of $\tau$ and the standard deviation of $\epsilon$ and $\tau$.}
\label{tab:freq2}
\end{center}
\end{table}

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Mon Jan 28 09:15:25 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{ccc|cccccc}
  \hline &&&\multicolumn{4}{c}{Number of Groups}&Misclassification&Adjusted\\ Distribution of $\tau$&$\sigma_{\epsilon}$&$\sigma_{\tau}$&$K=2$&$K=3$&$K=4$&$K=5$&Rate&Rand Index\\ \hline\multicolumn{9}{c}{\textbf{K-means on Derivative Splines Coefficients}}\\ Uniform & 0.50 & 2.00 & 0 & 500 & 0 & 0 & 0 & 1 \\ 
  Uniform & 2.00 & 2.00 & 15 & 189 & 164 & 132 & 0.392 & 0.271 \\ 
  Uniform & 0.50 & 3.00 & 0 & 500 & 0 & 0 & 0 & 1 \\ 
  Uniform & 2.00 & 3.00 & 15 & 189 & 164 & 132 & 0.392 & 0.271 \\ 
  Gaussian & 0.50 & 2.00 & 0 & 500 & 0 & 0 & 0 & 1 \\ 
  Gaussian & 2.00 & 2.00 & 18 & 191 & 171 & 120 & 0.393 & 0.268 \\ 
  Gaussian & 0.50 & 3.00 & 0 & 500 & 0 & 0 & 0 & 1 \\ 
  Gaussian & 2.00 & 3.00 & 18 & 191 & 171 & 120 & 0.393 & 0.268 \\ 
   \\ \multicolumn{9}{c}{\textbf{Multilayer Mixture}}\\Uniform & 0.50 & 2.00 &  &  &  &  & 0.07 & 0.82 \\ 
  Uniform & 2.00 & 2.00 &  &  &  &  & 0.29 & 0.35 \\ 
  Uniform & 0.50 & 3.00 &  &  &  &  & 0.33 & 0.32 \\ 
  Uniform & 2.00 & 3.00 &  &  &  &  & 0.46 & 0.12 \\ 
  Gaussian & 0.50 & 2.00 &  &  &  &  & 0.09 & 0.75 \\ 
  Gaussian & 2.00 & 2.00 &  &  &  &  & 0.3 & 0.34 \\ 
  Gaussian & 0.50 & 3.00 &  &  &  &  & 0.31 & 0.34 \\ 
  Gaussian & 2.00 & 3.00 &  &  &  &  & 0.41 & 0.17 \\ 
     \\ \multicolumn{9}{c}{\textbf{Vertically Shifted Independent Mixture}}\\Uniform & 0.50 & 2.00 & 0 & 499 & 1 & 0 & 0 & 1 \\ 
  Uniform & 2.00 & 2.00 & 0 & 499 & 1 & 0 & 0.05 & 0.87 \\ 
  Uniform & 0.50 & 3.00 & 0 & 499 & 1 & 0 & 0 & 1 \\ 
  Uniform & 2.00 & 3.00 & 0 & 499 & 1 & 0 & 0.05 & 0.87 \\ 
  Gaussian & 0.50 & 2.00 & 0 & 499 & 0 & 1 & 0 & 1 \\ 
  Gaussian & 2.00 & 2.00 & 0 & 498 & 2 & 0 & 0.05 & 0.87 \\ 
  Gaussian & 0.50 & 3.00 & 0 & 499 & 0 & 1 & 0 & 1 \\ 
  Gaussian & 2.00 & 3.00 & 0 & 498 & 2 & 0 & 0.05 & 0.87 \\ 
   \\ \multicolumn{9}{c}{\textbf{Vertically Shifted Exponential Mixture}}\\Uniform & 0.50 & 2.00 & 0 & 500 & 0 & 0 & 0 & 1 \\ 
  Uniform & 2.00 & 2.00 & 0 & 500 & 0 & 0 & 0.05 & 0.87 \\ 
  Uniform & 0.50 & 3.00 & 0 & 500 & 0 & 0 & 0 & 1 \\ 
  Uniform & 2.00 & 3.00 & 0 & 500 & 0 & 0 & 0.05 & 0.87 \\ 
  Gaussian & 0.50 & 2.00 & 0 & 500 & 0 & 0 & 0 & 1 \\ 
  Gaussian & 2.00 & 2.00 & 0 & 499 & 1 & 0 & 0.05 & 0.87 \\ 
  Gaussian & 0.50 & 3.00 & 0 & 500 & 0 & 0 & 0 & 1 \\ 
  Gaussian & 2.00 & 3.00 & 0 & 499 & 1 & 0 & 0.05 & 0.87 \\ 
   \hline\end{tabular}
\caption{Frequency table of the number of groups chosen, average misclassification rate ($K=3$), and average adjusted rand index ($K=3$) for 500 replications of clustering methods applied to data generated under different conditions for the distribution of $\tau$ and the standard deviation of $\epsilon$ and $\tau$.}
\label{tab:freq3}
\end{center}
\end{table}

To compare the group means from all methods with the true mean shapes used to generate the data, we transformed the estimates when $K=3$ to coefficients of the derivative spline previously mentioned. There are two coefficients per group and we denoted them as $\alpha_{11},\alpha_{21}$ for group 1, $\alpha_{12},\alpha_{22}$ for group 2, and $\alpha_{13},\alpha_{23}$ for group 3. Tables \ref{tab:mse1}, \ref{tab:mse2}, and \ref{tab:mse3} present the mean squared errors for the derivative spline coefficients under the simulation conditions for all clustering methods. When the clustering algorithm finds the shape groups correctly, the MSE is very close to zero for all coefficients. This occurs for the vertically shifted mixture under all conditions and both derivative-based methods and the exponential mixture model when the measurement error is small. The multilayer mixture model has very small MSE only when the groups are very well separated.\\

 In general, the standard methods find one group with a horizontal shape on average indicated by smaller values for $\alpha_{21}$ and $\alpha_{22}$. If the MSE is large (close to or greater than 1) for a method, this indicates clustering instability such that the method finds different groups for every randomly generated data set resulting in great variation in the estimates (Table \ref{tab:mse2} and \ref{tab:mse3}). This may occur for a variety of reasons. When smoothing individual trajectories, for example, it is easy to occasionally over fit the five observations such that the shape is largely determined by the errors since the splines are chosen for the entire data set.\\
 
 Of the three proposed models, only the vertically shifting mixture model with independence or exponential correlation consistently estimates the mean curves well (Table \ref{tab:mse3}). The K-means on the derivative spline coefficients estimate the shapes well only when the measurement error is small and the multilayer mixture model performs well when the measurement error is small and the vertical perturbation is small. \\

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Mon Jan 28 09:51:17 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{ccc|cccccc}
  \hline Distribution of $\tau$&$\sigma_{\epsilon}$&$\sigma_{\tau}$&$\alpha_{11}$&$\alpha_{21}$&$\alpha_{12}$&$\alpha_{22}$&$\alpha_{13}$&$\alpha_{23}$\\ \hline\multicolumn{9}{c}{\textbf{K-means on Raw Data}}\\ Uniform & 0.50 & 2.00 & 0.71 & 0.71 & 0.09 & 0.09 & 0.72 & 0.72 \\ 
  Uniform & 2.00 & 2.00 & 0.78 & 0.77 & 0.06 & 0.06 & 0.80 & 0.80 \\ 
  Uniform & 0.50 & 3.00 & 0.87 & 0.88 & 0.01 & 0.01 & 0.88 & 0.88 \\ 
  Uniform & 2.00 & 3.00 & 0.88 & 0.89 & 0.01 & 0.01 & 0.89 & 0.88 \\ 
  Gaussian & 0.50 & 2.00 & 0.92 & 0.92 & 0.01 & 0.01 & 0.91 & 0.92 \\ 
  Gaussian & 2.00 & 2.00 & 0.90 & 0.91 & 0.02 & 0.02 & 0.89 & 0.90 \\ 
  Gaussian & 0.50 & 3.00 & 0.89 & 0.89 & 0.01 & 0.01 & 0.89 & 0.90 \\ 
  Gaussian & 2.00 & 3.00 & 0.89 & 0.89 & 0.01 & 0.01 & 0.88 & 0.91 \\ 
   \\ \multicolumn{9}{c}{\textbf{K-means on Splines Coefficients}}\\Uniform & 0.50 & 2.00 & 0.50 & 0.50 & 0.21 & 0.22 & 0.53 & 0.53 \\ 
  Uniform & 2.00 & 2.00 & 0.49 & 0.55 & 0.23 & 0.29 & 0.53 & 0.58 \\ 
  Uniform & 0.50 & 3.00 & 0.52 & 0.53 & 0.23 & 0.24 & 0.59 & 0.59 \\ 
  Uniform & 2.00 & 3.00 & 0.56 & 0.63 & 0.26 & 0.29 & 0.60 & 0.69 \\ 
  Gaussian & 0.50 & 2.00 & 0.49 & 0.49 & 0.19 & 0.20 & 0.56 & 0.56 \\ 
  Gaussian & 2.00 & 2.00 & 0.47 & 0.55 & 0.21 & 0.29 & 0.54 & 0.62 \\ 
  Gaussian & 0.50 & 3.00 & 0.68 & 0.69 & 0.14 & 0.14 & 0.69 & 0.70 \\ 
  Gaussian & 2.00 & 3.00 & 0.64 & 0.76 & 0.16 & 0.20 & 0.64 & 0.76 \\ 
   \\ \multicolumn{9}{c}{\textbf{Independent Mixture}}\\Uniform & 0.50 & 2.00 & 0.87 & 0.87 & 0.01 & 0.01 & 0.87 & 0.87 \\ 
  Uniform & 2.00 & 2.00 & 0.85 & 0.86 & 0.02 & 0.02 & 0.87 & 0.86 \\ 
  Uniform & 0.50 & 3.00 & 0.85 & 0.85 & 0.02 & 0.02 & 0.86 & 0.86 \\ 
  Uniform & 2.00 & 3.00 & 0.84 & 0.83 & 0.02 & 0.02 & 0.86 & 0.85 \\ 
  Gaussian & 0.50 & 2.00 & 0.88 & 0.88 & 0.01 & 0.01 & 0.88 & 0.88 \\ 
  Gaussian & 2.00 & 2.00 & 0.86 & 0.86 & 0.02 & 0.02 & 0.86 & 0.86 \\ 
  Gaussian & 0.50 & 3.00 & 0.91 & 0.91 & 0.01 & 0.01 & 0.90 & 0.90 \\ 
  Gaussian & 2.00 & 3.00 & 0.90 & 0.88 & 0.01 & 0.01 & 0.89 & 0.90 \\ 
   \\ \multicolumn{9}{c}{\textbf{Exponential Mixture}}\\Uniform & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Uniform & 2.00 & 2.00 & 0.28 & 0.30 & 0.38 & 0.37 & 0.26 & 0.27 \\ 
  Uniform & 0.50 & 3.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Uniform & 2.00 & 3.00 & 0.12 & 0.13 & 0.39 & 0.37 & 0.10 & 0.09 \\ 
  Gaussian & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Gaussian & 2.00 & 2.00 & 0.26 & 0.25 & 0.41 & 0.40 & 0.24 & 0.24 \\ 
  Gaussian & 0.50 & 3.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Gaussian & 2.00 & 3.00 & 0.12 & 0.12 & 0.39 & 0.37 & 0.11 & 0.11 \\ 
   \hline\end{tabular}
\caption{Mean squared error for derivative spline coefficients ($K=3$) for 500 replications of clustering methods applied to data generated under different conditions for the distribution of $\tau$ and the standard deviation of $\epsilon$ and $\tau$.}
\label{tab:mse1}
\end{center}
\end{table}
% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Mon Jan 28 09:51:02 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{ccc|cccccc}
  \hline Distribution of $\tau$&$\sigma_{\epsilon}$&$\sigma_{\tau}$&$\alpha_{11}$&$\alpha_{21}$&$\alpha_{12}$&$\alpha_{22}$&$\alpha_{13}$&$\alpha_{23}$\\  \hline\multicolumn{9}{c}{\textbf{K-means on Quotient Differences}}\\ Uniform & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Uniform & 2.00 & 2.00 & 0.10 & 0.09 & 0.20 & 0.17 & 0.10 & 0.10 \\ 
  Uniform & 0.50 & 3.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Uniform & 2.00 & 3.00 & 0.10 & 0.09 & 0.20 & 0.17 & 0.10 & 0.10 \\ 
  Gaussian & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Gaussian & 2.00 & 2.00 & 0.10 & 0.09 & 0.20 & 0.18 & 0.10 & 0.10 \\ 
  Gaussian & 0.50 & 3.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Gaussian & 2.00 & 3.00 & 0.10 & 0.09 & 0.20 & 0.18 & 0.10 & 0.10 \\ 
   \\ \multicolumn{9}{c}{\textbf{PAM with Correlation Measure}}\\Uniform & 0.50 & 2.00 & 0.91 & 0.91 & 0.46 & 0.46 & 1.87 & 1.85 \\ 
  Uniform & 2.00 & 2.00 & 1.21 & 1.23 & 0.62 & 0.67 & 2.11 & 2.01 \\ 
  Uniform & 0.50 & 3.00 & 0.91 & 0.91 & 0.46 & 0.46 & 1.87 & 1.85 \\ 
  Uniform & 2.00 & 3.00 & 1.21 & 1.23 & 0.62 & 0.67 & 2.11 & 2.01 \\ 
  Gaussian & 0.50 & 2.00 & 0.83 & 0.87 & 0.47 & 0.46 & 1.77 & 1.77 \\ 
  Gaussian & 2.00 & 2.00 & 1.19 & 1.29 & 0.67 & 0.64 & 1.92 & 2.08 \\ 
  Gaussian & 0.50 & 3.00 & 0.83 & 0.87 & 0.47 & 0.46 & 1.77 & 1.77 \\ 
  Gaussian & 2.00 & 3.00 & 1.19 & 1.29 & 0.67 & 0.64 & 1.92 & 2.08 \\ 
   \hline\end{tabular}
\caption{Mean squared error for derivative spline coefficients ($K=3$) for 500 replications of clustering methods applied to data generated under different conditions for the distribution of $\tau$ and the standard deviation of $\epsilon$ and $\tau$.}
\label{tab:mse2}
\end{center}
\end{table}

% latex table generated in R 2.15.2 by xtable 1.7-0 package
% Mon Jan 28 09:50:45 2013
\begin{table}[ht]
\begin{center}
\begin{tabular}{ccc|cccccc}
  \hline Distribution of $\tau$&$\sigma_{\epsilon}$&$\sigma_{\tau}$&$\alpha_{11}$&$\alpha_{21}$&$\alpha_{12}$&$\alpha_{22}$&$\alpha_{13}$&$\alpha_{23}$\\  \hline\multicolumn{9}{c}{\textbf{K-means on Derivative Splines Coefficients}}\\ Uniform & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Uniform & 2.00 & 2.00 & 0.50 & 0.49 & 1.21 & 1.29 & 0.41 & 0.51 \\ 
  Uniform & 0.50 & 3.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Uniform & 2.00 & 3.00 & 0.50 & 0.49 & 1.21 & 1.29 & 0.41 & 0.51 \\ 
  Gaussian & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Gaussian & 2.00 & 2.00 & 0.46 & 0.45 & 1.26 & 1.27 & 0.48 & 0.52 \\ 
  Gaussian & 0.50 & 3.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Gaussian & 2.00 & 3.00 & 0.46 & 0.45 & 1.26 & 1.27 & 0.48 & 0.52 \\ 
   \\ \multicolumn{9}{c}{\textbf{Multilayer Mixture}}\\Uniform & 0.50 & 2.00 & 0.02 & 0.02 & 0.01 & 0.01 & 0.03 & 0.03 \\ 
  Uniform & 2.00 & 2.00 & 0.12 & 0.12 & 0.01 & 0.01 & 0.12 & 0.11 \\ 
  Uniform & 0.50 & 3.00 & 0.33 & 0.33 & 0.03 & 0.03 & 0.33 & 0.32 \\ 
  Uniform & 2.00 & 3.00 & 0.40 & 0.40 & 0.02 & 0.02 & 0.42 & 0.40 \\ 
  Gaussian & 0.50 & 2.00 & 0.01 & 0.01 & 0.00 & 0.00 & 0.01 & 0.01 \\ 
  Gaussian & 2.00 & 2.00 & 0.13 & 0.13 & 0.01 & 0.01 & 0.13 & 0.13 \\ 
  Gaussian & 0.50 & 3.00 & 0.31 & 0.32 & 0.02 & 0.02 & 0.29 & 0.29 \\ 
  Gaussian & 2.00 & 3.00 & 0.35 & 0.34 & 0.01 & 0.01 & 0.34 & 0.35 \\ 
     \\ \multicolumn{9}{c}{\textbf{Vertically Shifted Independent Mixture}}\\Uniform & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Uniform & 2.00 & 2.00 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\ 
  Uniform & 0.50 & 3.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Uniform & 2.00 & 3.00 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\ 
  Gaussian & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Gaussian & 2.00 & 2.00 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\ 
  Gaussian & 0.50 & 3.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Gaussian & 2.00 & 3.00 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\ 
   \\ \multicolumn{9}{c}{\textbf{Vertically Shifted Exponential Mixture}}\\Uniform & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Uniform & 2.00 & 2.00 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\ 
  Uniform & 0.50 & 3.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Uniform & 2.00 & 3.00 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\ 
  Gaussian & 0.50 & 2.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Gaussian & 2.00 & 2.00 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\ 
  Gaussian & 0.50 & 3.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  Gaussian & 2.00 & 3.00 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\ 
   \hline\end{tabular}
\caption{Mean squared error for derivative spline coefficients ($K=3$) for 500 replications of clustering methods applied to data generated under different conditions for the distribution of $\tau$ and the standard deviation of $\epsilon$ and $\tau$.}
\label{tab:mse3}
\end{center}
\end{table}

We have determined that some of the methods discover the shape groups fairly well under conditions of small measurement error. We have measured this by comparing the group labels to the true generating labels comparing the estimates of the mean shapes with known generating values. Lastly, we want to know how this impacts the estimation of the relationship between baseline factors and group membership. For simplification, we present density plots of the estimated coefficients for group 1 and 2 for the factors $z_{1}$ and $z_{2}$ for three methods---K-means on the raw data, K-means on the raw derivatives, and vertically shifted exponential mixture models---after setting $K=3$. As a reminder, the first baseline factor, $z_{1}$, was used to generate shape groups and the second, $z_{2}$, was used to generate level groups. \\

It is clear that the K-means on the raw data picks up the relationship between the group membership and the second baseline factor, $z_{2}$ (Figure \ref{fig:gamma1}). The density plots in dashed lines are concentrated at non-zero values while the solid line densities for $z_{1}$ are mostly centered around zero. For two Uniform conditions with $\sigma_{\tau}=2$, the density plot for $z_{1}$ is bimodal, which indicates some variability in the groups chosen. Overall, K-means picks up the level groups since $z_{2}$ is significantly non-zero.\\

On the other hand, the K-means on the quotient differences picks out the shape groups. The coefficients for $z_{1}$ are now mostly significantly non-zero while the coefficients for $z_{2}$ are centered around zero. However, when $\sigma_{epsilon}=2$, one of the density plots is bimodal indicating variability. We saw this earlier with the higher the misclassification rate.\\

Lastly, with the vertically shifting exponential mixture model, there is a significant relationship between group membership and $z_{1}$, the factor that was used to generate shape groups. The second factor is estimated to have zero relationship with the grouping.

\begin{figure}[h]
\begin{center}
\includegraphics[width=6in]{Chp5Gamma1}
\end{center}
\label{fig:gamma1}
\caption{Density plots of estimated multinomial coefficients from analysis after running K-means on raw data under all simulation conditions. Within each plot, the solid line represents the coefficients for the factor, $z_{1}$, and dashed line represents the coefficients for the factor, $z_{2}$, with line thickness indicating group 1 vs. group 2 (group 3 is the reference group with coefficients fixed to zero).}
\end{figure}
\begin{figure}[h]
\begin{center}
\includegraphics[width=6in]{Chp5Gamma2}
\end{center}
\label{fig:gamma1}
\caption{Density plots of estimated multinomial coefficients from analysis after running K-means on quotient differences under all simulation conditions. Within each plot, the solid line represents the coefficients for the factor, $z_{1}$, and dashed line represents the coefficients for the factor, $z_{2}$, with line thickness indicating group 1 vs. group 2 (group 3 is the reference group with coefficients fixed to zero).}
\end{figure}
\begin{figure}[h]
\begin{center}
\includegraphics[width=6in]{Chp5Gamma3}
\end{center}
\label{fig:gamma1}
\caption{Density plots of simultaneously estimated multinomial coefficients from the vertically shifted exponential mixture model under all simulation conditions. Within each plot, the solid line represents the coefficients for the factor, $z_{1}$, and dashed line represents the coefficients for the factor, $z_{2}$, with line thickness indicating group 1 vs. group 2 (group 3 is the reference group with coefficients fixed to zero).}
\end{figure}


\section{Conclusion}
These results are limited as we focused on one generating framework, but the ideas should generalize to similar situations.
\begin{itemize}
\item K-means on raw data does not find shape groups; rather it finds groups based mainly on level.
\item K-means on spline coefficients, which is one way to generalize the original K-means to work with irregularly sampled data, does not improve upon and may be slightly worse than K-means.
\item Finite mixture with independence failed just like K-means.
\item By generalizing the mixture model to allow exponential correlation, the model was able to detect three shape groups when measurement error was small. Its success may be a function of the data generation process, but either way, it won't give you the correct number of groups.
\item Of the methods thought to detect shape, PAM with the correlation measure failed miserably on all accounts. Not only could it not detect the right number of shape groups, it could find the shape groups when forced to the correct number of groups. 
\item Quotient differences on the other hand worked well when measurement error was small. It was able to choose the correct number of groups and and find the shape groups.
\item K-means on the derivative spline coefficients could be seen as a generalization of the quotient differences to accommodate irregularly sampled data. Under the same conditions of small measurement error, this method performed well. However, under larger measurement error, it performed worse than quotient differences. This may change when working with longer data sets with more observations.
\item The multilayer mixture model only works well if the data fit into the shape group/level subgroup framework such that the subgroups are well separated. Unfortunately, this is not a realistic model for most data sets. 
\item Vertically shifting the data prior to fitting a mixture model outperformed the rest of the methods. With larger measurement errors, there was about 5\% misclassification, but it did not impact the mean shape or baseline factor coefficient estimation.
\end{itemize}

